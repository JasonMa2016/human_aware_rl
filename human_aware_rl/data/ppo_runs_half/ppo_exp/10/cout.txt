INFO - PPO - Running command 'ppo_run'
INFO - PPO - Started run with ID "10"
Creating env with params {'RUN_TYPE': 'ppo', 'SEEDS': [9456, 1887, 5578, 5987, 516], 'LOCAL_TESTING': False, 'EX_NAME': 'ppo_bc_train_random1', 'SAVE_DIR': 'data/ppo_poor_runs/ppo_bc_train_random1/', 'GPU_ID': 0, 'PPO_RUN_TOT_TIMESTEPS': 16000000.0, 'mdp_params': {'layout_name': 'random1', 'start_order_list': None, 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}}, 'env_params': {'horizon': 400}, 'mdp_generation_params': {'padded_mdp_shape': [11, 7], 'mdp_shape_fn': [[5, 11], [5, 7]], 'prop_empty_fn': [0.6, 1], 'prop_feats_fn': [0, 0.6]}, 'ENTROPY': 0.1, 'GAMMA': 0.99, 'sim_threads': 30, 'TOTAL_BATCH_SIZE': 12000, 'BATCH_SIZE': 400, 'MAX_GRAD_NORM': 0.1, 'LR': 0.001, 'LR_ANNEALING': 1.5, 'VF_COEF': 0.5, 'STEPS_PER_UPDATE': 8, 'MINIBATCHES': 15, 'CLIPPING': 0.05, 'LAM': 0.98, 'SELF_PLAY_HORIZON': [2000000.0, 6000000.0], 'REW_SHAPING_HORIZON': 5000000.0, 'OTHER_AGENT_TYPE': 'bc_train', 'HM_PARAMS': [True, 0.3], 'NUM_HIDDEN_LAYERS': 3, 'SIZE_HIDDEN_LAYERS': 64, 'NUM_FILTERS': 25, 'NUM_CONV_LAYERS': 3, 'NETWORK_TYPE': 'conv_and_mlp', 'SAVE_BEST_THRESH': 50, 'TRAJECTORY_SELF_PLAY': True, 'VIZ_FREQUENCY': 50, 'grad_updates_per_agent': 159960.0}
Computing MediumLevelPlanner to be saved in /home/ubuntu/human_aware_rl/overcooked_ai/overcooked_ai_py/data/planners/random1_am.pkl
It took 0.6049940586090088 seconds to create mlp
LOADING BC MODEL FROM: random1_bc_train_seed4
Loading a model without an environment, this model cannot be trained until it has a valid environment.
WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/ubuntu/human_aware_rl/stable-baselines/stable_baselines/common/policies.py:436: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING - tensorflow - From /home/ubuntu/human_aware_rl/stable-baselines/stable_baselines/common/policies.py:436: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
Mlp with different params or mdp found, computing from scratch
Computing MediumLevelPlanner to be saved in /home/ubuntu/human_aware_rl/overcooked_ai/overcooked_ai_py/data/planners/random1_am.pkl
It took 0.5936915874481201 seconds to create mlp
WARNING:tensorflow:From /home/ubuntu/human_aware_rl/baselines/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING - tensorflow - From /home/ubuntu/human_aware_rl/baselines/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
(30, 5, 5, 20)
WARNING:tensorflow:From /home/ubuntu/human_aware_rl/human_aware_rl/baselines_utils.py:127: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.conv2d instead.
WARNING - tensorflow - From /home/ubuntu/human_aware_rl/human_aware_rl/baselines_utils.py:127: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.conv2d instead.
WARNING:tensorflow:From /home/ubuntu/human_aware_rl/human_aware_rl/baselines_utils.py:143: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING - tensorflow - From /home/ubuntu/human_aware_rl/human_aware_rl/baselines_utils.py:143: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
Last layer conv network output shape (30, 64)
(800, 5, 5, 20)
Last layer conv network output shape (800, 64)
TOT NUM UPDATES 0
TOT NUM UPDATES 1333
SP envs: 30/30
Other agent actions took 0.6189634799957275 seconds
Total simulation time for 400 steps: 5.277942180633545 	 Other agent action time: 0 	 75.78711291452333 steps/s
Curr learning rate 0.001 	 Curr reward per step 0.014750000000000001

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8:   7%|▋         | 1/15 [00:00<00:01,  9.50it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 84.34it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 196.56it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 189.08it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 200.10it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 199.02it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 200.12it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 201.65it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 198.71it/s]
Logging to /tmp/openai-2019-12-08-08-02-58-063157
--------------------------------------
| approxkl           | 0.00041323344 |
| clipfrac           | 0.024062501   |
| eplenmean          | 400           |
| eprewmean          | 5.9           |
| explained_variance | -0.00179      |
| fps                | 1984          |
| nupdates           | 1             |
| policy_entropy     | 1.7913449     |
| policy_loss        | -0.0007262444 |
| serial_timesteps   | 400           |
| time_elapsed       | 6.05          |
| time_remaining     | 134           |
| total_timesteps    | 12000         |
| true_eprew         | 0             |
| value_loss         | 0.42949343    |
--------------------------------------
Current reward shaping 0.9976
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6025667190551758 seconds
Total simulation time for 400 steps: 3.5413928031921387 	 Other agent action time: 0 	 112.9499104531551 steps/s
Curr learning rate 0.0009997499374843712 	 Curr reward per step 0.011888066666666666

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.47it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 205.55it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 205.44it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 204.41it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 203.70it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.28it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 203.15it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 203.48it/s]
--------------------------------------
| approxkl           | 0.00037645217 |
| clipfrac           | 0.040937502   |
| eplenmean          | 400           |
| eprewmean          | 5.33          |
| explained_variance | 0.0047        |
| fps                | 2873          |
| nupdates           | 2             |
| policy_entropy     | 1.7907346     |
| policy_loss        | -0.001611076  |
| serial_timesteps   | 800           |
| time_elapsed       | 10.2          |
| time_remaining     | 113           |
| total_timesteps    | 24000         |
| true_eprew         | 0             |
| value_loss         | 0.33720955    |
--------------------------------------
Current reward shaping 0.9952
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6047847270965576 seconds
Total simulation time for 400 steps: 3.6994402408599854 	 Other agent action time: 0 	 108.12446585351911 steps/s
Curr learning rate 0.0009994998749687423 	 Curr reward per step 0.016172

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 201.99it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 204.71it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.38it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 202.97it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 203.93it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 204.29it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 205.79it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 205.69it/s]
--------------------------------------
| approxkl           | 0.0005043483  |
| clipfrac           | 0.11660418    |
| eplenmean          | 400           |
| eprewmean          | 5.71          |
| explained_variance | 0.0026        |
| fps                | 2781          |
| nupdates           | 3             |
| policy_entropy     | 1.7904041     |
| policy_loss        | -0.0038339703 |
| serial_timesteps   | 1200          |
| time_elapsed       | 14.5          |
| time_remaining     | 107           |
| total_timesteps    | 36000         |
| true_eprew         | 0             |
| value_loss         | 0.48850706    |
--------------------------------------
Current reward shaping 0.9928
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.598515510559082 seconds
Total simulation time for 400 steps: 3.5901975631713867 	 Other agent action time: 0 	 111.41448150465057 steps/s
Curr learning rate 0.0009992498124531133 	 Curr reward per step 0.0176222

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 189.45it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 193.07it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 193.27it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 192.95it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 193.69it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 191.12it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 190.36it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 192.42it/s]
-------------------------------------
| approxkl           | 0.0006327053 |
| clipfrac           | 0.15676038   |
| eplenmean          | 400          |
| eprewmean          | 5.84         |
| explained_variance | 0.00422      |
| fps                | 2825         |
| nupdates           | 4            |
| policy_entropy     | 1.7891386    |
| policy_loss        | -0.00617287  |
| serial_timesteps   | 1600         |
| time_elapsed       | 18.8         |
| time_remaining     | 104          |
| total_timesteps    | 48000        |
| true_eprew         | 0            |
| value_loss         | 0.49746662   |
-------------------------------------
Current reward shaping 0.9904
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6033849716186523 seconds
Total simulation time for 400 steps: 3.6158082485198975 	 Other agent action time: 0 	 110.62533533511818 steps/s
Curr learning rate 0.0009989997499374845 	 Curr reward per step 0.0178272

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.54it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 204.08it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.90it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 204.62it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 205.11it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 205.02it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 203.38it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 206.74it/s]
-------------------------------------
| approxkl           | 0.0008975529 |
| clipfrac           | 0.22684374   |
| eplenmean          | 400          |
| eprewmean          | 6.55         |
| explained_variance | 0.00563      |
| fps                | 2836         |
| nupdates           | 5            |
| policy_entropy     | 1.7879514    |
| policy_loss        | -0.007583585 |
| serial_timesteps   | 2000         |
| time_elapsed       | 23           |
| time_remaining     | 102          |
| total_timesteps    | 60000        |
| true_eprew         | 0            |
| value_loss         | 0.55839175   |
-------------------------------------
Current reward shaping 0.988
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5956630706787109 seconds
Total simulation time for 400 steps: 3.721712827682495 	 Other agent action time: 0 	 107.4773950920548 steps/s
Curr learning rate 0.0009987496874218555 	 Curr reward per step 0.021241999999999997

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 192.88it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 196.29it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 195.62it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 191.61it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 197.10it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 192.05it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 198.03it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 198.58it/s]
--------------------------------------
| approxkl           | 0.0010009316  |
| clipfrac           | 0.25506252    |
| eplenmean          | 400           |
| eprewmean          | 7.43          |
| explained_variance | 0.0253        |
| fps                | 2748          |
| nupdates           | 6             |
| policy_entropy     | 1.7865576     |
| policy_loss        | -0.0089832265 |
| serial_timesteps   | 2400          |
| time_elapsed       | 27.4          |
| time_remaining     | 101           |
| total_timesteps    | 72000         |
| true_eprew         | 0             |
| value_loss         | 0.60212666    |
--------------------------------------
Current reward shaping 0.9856
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.601301908493042 seconds
Total simulation time for 400 steps: 3.6449596881866455 	 Other agent action time: 0 	 109.74058267267108 steps/s
Curr learning rate 0.0009984996249062265 	 Curr reward per step 0.022915199999999997

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 205.68it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 206.96it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 208.80it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 208.95it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 207.05it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 200.70it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 203.70it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.95it/s]
-------------------------------------
| approxkl           | 0.0012513663 |
| clipfrac           | 0.29843745   |
| eplenmean          | 400          |
| eprewmean          | 8.21         |
| explained_variance | 0.0293       |
| fps                | 2819         |
| nupdates           | 7            |
| policy_entropy     | 1.7841297    |
| policy_loss        | -0.009262741 |
| serial_timesteps   | 2800         |
| time_elapsed       | 31.6         |
| time_remaining     | 99.9         |
| total_timesteps    | 84000        |
| true_eprew         | 0            |
| value_loss         | 0.7039489    |
-------------------------------------
Current reward shaping 0.9832
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6004478931427002 seconds
Total simulation time for 400 steps: 3.592813730239868 	 Other agent action time: 0 	 111.33335319704834 steps/s
Curr learning rate 0.0009982495623905977 	 Curr reward per step 0.02285939999999999

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.68it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.79it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.04it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 205.11it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 203.90it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 202.91it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 201.58it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 201.44it/s]
-------------------------------------
| approxkl           | 0.0013487234 |
| clipfrac           | 0.3207188    |
| eplenmean          | 400          |
| eprewmean          | 8.96         |
| explained_variance | 0.0553       |
| fps                | 2848         |
| nupdates           | 8            |
| policy_entropy     | 1.781512     |
| policy_loss        | -0.01046147  |
| serial_timesteps   | 3200         |
| time_elapsed       | 35.9         |
| time_remaining     | 99           |
| total_timesteps    | 96000        |
| true_eprew         | 0            |
| value_loss         | 0.6163912    |
-------------------------------------
Current reward shaping 0.9808
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6035802364349365 seconds
Total simulation time for 400 steps: 3.6085031032562256 	 Other agent action time: 0 	 110.84928807156899 steps/s
Curr learning rate 0.0009979994998749687 	 Curr reward per step 0.020760266666666666

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 194.21it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 195.10it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 193.10it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 194.49it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 193.49it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 189.24it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 190.59it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 187.15it/s]
-------------------------------------
| approxkl           | 0.0014717885 |
| clipfrac           | 0.3309167    |
| eplenmean          | 400          |
| eprewmean          | 8.84         |
| explained_variance | 0.0717       |
| fps                | 2816         |
| nupdates           | 9            |
| policy_entropy     | 1.7787374    |
| policy_loss        | -0.01051704  |
| serial_timesteps   | 3600         |
| time_elapsed       | 40.1         |
| time_remaining     | 98.4         |
| total_timesteps    | 108000       |
| true_eprew         | 0            |
| value_loss         | 0.558034     |
-------------------------------------
Current reward shaping 0.9784
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.597916841506958 seconds
Total simulation time for 400 steps: 3.8077008724212646 	 Other agent action time: 0 	 105.05026875854497 steps/s
Curr learning rate 0.0009977494373593397 	 Curr reward per step 0.02821053333333333

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 201.59it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.55it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 203.29it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 203.77it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 203.97it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 204.59it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 203.49it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 203.81it/s]
-------------------------------------
| approxkl           | 0.0014755919 |
| clipfrac           | 0.33655205   |
| eplenmean          | 400          |
| eprewmean          | 9.62         |
| explained_variance | 0.0926       |
| fps                | 2710         |
| nupdates           | 10           |
| policy_entropy     | 1.7782291    |
| policy_loss        | -0.011918102 |
| serial_timesteps   | 4000         |
| time_elapsed       | 44.5         |
| time_remaining     | 98.2         |
| total_timesteps    | 120000       |
| true_eprew         | 0            |
| value_loss         | 0.85415864   |
-------------------------------------
Current reward shaping 0.976
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5982415676116943 seconds
Total simulation time for 400 steps: 3.587388038635254 	 Other agent action time: 0 	 111.50173766877239 steps/s
Curr learning rate 0.000997499374843711 	 Curr reward per step 0.03228933333333332

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.29it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 204.53it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.52it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 204.98it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 205.27it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 204.18it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 204.37it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 202.84it/s]
--------------------------------------
| approxkl           | 0.001793075   |
| clipfrac           | 0.37034377    |
| eplenmean          | 400           |
| eprewmean          | 10.7          |
| explained_variance | 0.104         |
| fps                | 2855          |
| nupdates           | 11            |
| policy_entropy     | 1.7732311     |
| policy_loss        | -0.0115967095 |
| serial_timesteps   | 4400          |
| time_elapsed       | 48.8          |
| time_remaining     | 97.7          |
| total_timesteps    | 132000        |
| true_eprew         | 0             |
| value_loss         | 0.9983079     |
--------------------------------------
Current reward shaping 0.9736
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.599351167678833 seconds
Total simulation time for 400 steps: 3.5859789848327637 	 Other agent action time: 0 	 111.54555051544857 steps/s
Curr learning rate 0.000997249312328082 	 Curr reward per step 0.024907933333333333

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 203.76it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 206.72it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 206.00it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 206.42it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 206.63it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 205.24it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 205.16it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.46it/s]
-------------------------------------
| approxkl           | 0.0017746846 |
| clipfrac           | 0.37330207   |
| eplenmean          | 400          |
| eprewmean          | 11.1         |
| explained_variance | 0.151        |
| fps                | 2858         |
| nupdates           | 12           |
| policy_entropy     | 1.7721225    |
| policy_loss        | -0.012390137 |
| serial_timesteps   | 4800         |
| time_elapsed       | 53           |
| time_remaining     | 97.1         |
| total_timesteps    | 144000       |
| true_eprew         | 0            |
| value_loss         | 0.69318223   |
-------------------------------------
Current reward shaping 0.9712
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6033763885498047 seconds
Total simulation time for 400 steps: 3.606354236602783 	 Other agent action time: 0 	 110.91533824941264 steps/s
Curr learning rate 0.000996999249812453 	 Curr reward per step 0.027841066666666664

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 195.27it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 198.26it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 196.71it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 197.01it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 195.95it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 196.17it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 196.19it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 197.41it/s]
-------------------------------------
| approxkl           | 0.0020755427 |
| clipfrac           | 0.39699998   |
| eplenmean          | 400          |
| eprewmean          | 11.3         |
| explained_variance | 0.194        |
| fps                | 2827         |
| nupdates           | 13           |
| policy_entropy     | 1.7689971    |
| policy_loss        | -0.012375402 |
| serial_timesteps   | 5200         |
| time_elapsed       | 57.2         |
| time_remaining     | 96.8         |
| total_timesteps    | 156000       |
| true_eprew         | 0            |
| value_loss         | 0.71748394   |
-------------------------------------
Current reward shaping 0.9688
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.600985050201416 seconds
Total simulation time for 400 steps: 3.8325915336608887 	 Other agent action time: 0 	 104.3680226517435 steps/s
Curr learning rate 0.0009967491872968242 	 Curr reward per step 0.02970986666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 196.71it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 201.27it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 200.60it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 199.64it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 199.75it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 201.90it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 199.62it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 199.46it/s]
-------------------------------------
| approxkl           | 0.0021291585 |
| clipfrac           | 0.40628117   |
| eplenmean          | 400          |
| eprewmean          | 11.3         |
| explained_variance | 0.214        |
| fps                | 2689         |
| nupdates           | 14           |
| policy_entropy     | 1.7686812    |
| policy_loss        | -0.013274666 |
| serial_timesteps   | 5600         |
| time_elapsed       | 61.7         |
| time_remaining     | 96.8         |
| total_timesteps    | 168000       |
| true_eprew         | 0            |
| value_loss         | 0.8047287    |
-------------------------------------
Current reward shaping 0.9664
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6064696311950684 seconds
Total simulation time for 400 steps: 3.66757869720459 	 Other agent action time: 0 	 109.06378104575589 steps/s
Curr learning rate 0.0009964991247811954 	 Curr reward per step 0.03444373333333332

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.04it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 204.25it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 205.28it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 205.37it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 206.25it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 205.15it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 206.78it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 206.99it/s]
-------------------------------------
| approxkl           | 0.002262404  |
| clipfrac           | 0.4090104    |
| eplenmean          | 400          |
| eprewmean          | 11.9         |
| explained_variance | 0.199        |
| fps                | 2804         |
| nupdates           | 15           |
| policy_entropy     | 1.7636802    |
| policy_loss        | -0.011018044 |
| serial_timesteps   | 6000         |
| time_elapsed       | 65.9         |
| time_remaining     | 96.6         |
| total_timesteps    | 180000       |
| true_eprew         | 0.2          |
| value_loss         | 1.2832023    |
-------------------------------------
Current reward shaping 0.964
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.597620964050293 seconds
Total simulation time for 400 steps: 3.6388626098632812 	 Other agent action time: 0 	 109.92445796545991 steps/s
Curr learning rate 0.0009962490622655665 	 Curr reward per step 0.03655166666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.98it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 204.01it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 203.12it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 202.06it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 200.74it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 195.92it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 198.39it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 200.86it/s]
-------------------------------------
| approxkl           | 0.0025699975 |
| clipfrac           | 0.43253124   |
| eplenmean          | 400          |
| eprewmean          | 13.3         |
| explained_variance | 0.255        |
| fps                | 2814         |
| nupdates           | 16           |
| policy_entropy     | 1.757234     |
| policy_loss        | -0.01281785  |
| serial_timesteps   | 6400         |
| time_elapsed       | 70.2         |
| time_remaining     | 96.3         |
| total_timesteps    | 192000       |
| true_eprew         | 0.2          |
| value_loss         | 0.98828125   |
-------------------------------------
Current reward shaping 0.9616
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6100475788116455 seconds
Total simulation time for 400 steps: 3.6434638500213623 	 Other agent action time: 0 	 109.78563709302475 steps/s
Curr learning rate 0.0009959989997499375 	 Curr reward per step 0.040050533333333346

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 181.88it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 185.93it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 186.38it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 186.43it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 185.52it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 185.79it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 187.27it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 188.16it/s]
-------------------------------------
| approxkl           | 0.0023710958 |
| clipfrac           | 0.41875002   |
| eplenmean          | 400          |
| eprewmean          | 14.6         |
| explained_variance | 0.208        |
| fps                | 2779         |
| nupdates           | 17           |
| policy_entropy     | 1.757223     |
| policy_loss        | -0.011574474 |
| serial_timesteps   | 6800         |
| time_elapsed       | 74.5         |
| time_remaining     | 96.1         |
| total_timesteps    | 204000       |
| true_eprew         | 0.4          |
| value_loss         | 1.363942     |
-------------------------------------
Current reward shaping 0.9592
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6041512489318848 seconds
Total simulation time for 400 steps: 3.655839204788208 	 Other agent action time: 0 	 109.41400252946109 steps/s
Curr learning rate 0.0009957489372343087 	 Curr reward per step 0.03883566666666666

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 203.18it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.60it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.55it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 206.27it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 205.63it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 205.75it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 203.88it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 205.30it/s]
-------------------------------------
| approxkl           | 0.0027982453 |
| clipfrac           | 0.44042712   |
| eplenmean          | 400          |
| eprewmean          | 15.5         |
| explained_variance | 0.23         |
| fps                | 2811         |
| nupdates           | 18           |
| policy_entropy     | 1.749678     |
| policy_loss        | -0.010056651 |
| serial_timesteps   | 7200         |
| time_elapsed       | 78.8         |
| time_remaining     | 95.9         |
| total_timesteps    | 216000       |
| true_eprew         | 0.6          |
| value_loss         | 1.2625691    |
-------------------------------------
Current reward shaping 0.9568
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6032614707946777 seconds
Total simulation time for 400 steps: 3.676661252975464 	 Other agent action time: 0 	 108.79435783655248 steps/s
Curr learning rate 0.0009954988747186797 	 Curr reward per step 0.03938826666666666

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 192.40it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 195.61it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 196.09it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 197.12it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 196.81it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 196.92it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 196.79it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 197.35it/s]
-------------------------------------
| approxkl           | 0.0028640328 |
| clipfrac           | 0.44105205   |
| eplenmean          | 400          |
| eprewmean          | 15.7         |
| explained_variance | 0.367        |
| fps                | 2774         |
| nupdates           | 19           |
| policy_entropy     | 1.7482624    |
| policy_loss        | -0.011461226 |
| serial_timesteps   | 7600         |
| time_elapsed       | 83.1         |
| time_remaining     | 95.8         |
| total_timesteps    | 228000       |
| true_eprew         | 0.4          |
| value_loss         | 1.0016637    |
-------------------------------------
Current reward shaping 0.9544
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6017239093780518 seconds
Total simulation time for 400 steps: 3.894440174102783 	 Other agent action time: 0 	 102.71052631901159 steps/s
Curr learning rate 0.0009952488122030507 	 Curr reward per step 0.04294800000000001

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 197.57it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 200.09it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 198.39it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 197.12it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 198.06it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 198.53it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 199.04it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 197.36it/s]
-------------------------------------
| approxkl           | 0.0030925372 |
| clipfrac           | 0.47768748   |
| eplenmean          | 400          |
| eprewmean          | 16.1         |
| explained_variance | 0.326        |
| fps                | 2650         |
| nupdates           | 20           |
| policy_entropy     | 1.7362796    |
| policy_loss        | -0.012509593 |
| serial_timesteps   | 8000         |
| time_elapsed       | 87.6         |
| time_remaining     | 95.9         |
| total_timesteps    | 240000       |
| true_eprew         | 0.2          |
| value_loss         | 1.1059767    |
-------------------------------------
Current reward shaping 0.952
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6087174415588379 seconds
Total simulation time for 400 steps: 3.6799237728118896 	 Other agent action time: 0 	 108.69790373248777 steps/s
Curr learning rate 0.000994998749687422 	 Curr reward per step 0.04775999999999999

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 196.89it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 200.94it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 201.33it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 201.07it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 200.49it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 199.01it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 198.92it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 199.61it/s]
-------------------------------------
| approxkl           | 0.0028100042 |
| clipfrac           | 0.43967694   |
| eplenmean          | 400          |
| eprewmean          | 17.1         |
| explained_variance | 0.25         |
| fps                | 2784         |
| nupdates           | 21           |
| policy_entropy     | 1.7338696    |
| policy_loss        | -0.010417414 |
| serial_timesteps   | 8400         |
| time_elapsed       | 92           |
| time_remaining     | 95.8         |
| total_timesteps    | 252000       |
| true_eprew         | 0.4          |
| value_loss         | 1.7826711    |
-------------------------------------
Current reward shaping 0.9496
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6069486141204834 seconds
Total simulation time for 400 steps: 3.6665353775024414 	 Other agent action time: 0 	 109.09481535467161 steps/s
Curr learning rate 0.000994748687171793 	 Curr reward per step 0.05350386666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.19it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 204.53it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.33it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 205.84it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 205.27it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 204.67it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 204.33it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.69it/s]
-------------------------------------
| approxkl           | 0.003105022  |
| clipfrac           | 0.45325005   |
| eplenmean          | 400          |
| eprewmean          | 18.9         |
| explained_variance | 0.222        |
| fps                | 2803         |
| nupdates           | 22           |
| policy_entropy     | 1.7356229    |
| policy_loss        | -0.009040627 |
| serial_timesteps   | 8800         |
| time_elapsed       | 96.2         |
| time_remaining     | 95.6         |
| total_timesteps    | 264000       |
| true_eprew         | 0.8          |
| value_loss         | 2.2972062    |
-------------------------------------
Current reward shaping 0.9472
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6004078388214111 seconds
Total simulation time for 400 steps: 3.662219762802124 	 Other agent action time: 0 	 109.22337432146414 steps/s
Curr learning rate 0.000994498624656164 	 Curr reward per step 0.05148266666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.07it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 201.96it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 203.91it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 204.33it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 204.68it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.83it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 205.05it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.42it/s]
-------------------------------------
| approxkl           | 0.0032702505 |
| clipfrac           | 0.4493645    |
| eplenmean          | 400          |
| eprewmean          | 20           |
| explained_variance | 0.312        |
| fps                | 2803         |
| nupdates           | 23           |
| policy_entropy     | 1.7288944    |
| policy_loss        | -0.00951561  |
| serial_timesteps   | 9200         |
| time_elapsed       | 101          |
| time_remaining     | 95.4         |
| total_timesteps    | 276000       |
| true_eprew         | 1.2          |
| value_loss         | 1.8397518    |
-------------------------------------
Current reward shaping 0.9448
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6004636287689209 seconds
Total simulation time for 400 steps: 3.640730619430542 	 Other agent action time: 0 	 109.86805721500077 steps/s
Curr learning rate 0.0009942485621405352 	 Curr reward per step 0.05388013333333333

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.49it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 202.55it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.48it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 205.11it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 200.43it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.92it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 205.24it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.46it/s]
-------------------------------------
| approxkl           | 0.003072379  |
| clipfrac           | 0.43228117   |
| eplenmean          | 400          |
| eprewmean          | 20.7         |
| explained_variance | 0.294        |
| fps                | 2818         |
| nupdates           | 24           |
| policy_entropy     | 1.7256943    |
| policy_loss        | -0.008641676 |
| serial_timesteps   | 9600         |
| time_elapsed       | 105          |
| time_remaining     | 95.2         |
| total_timesteps    | 288000       |
| true_eprew         | 1.2          |
| value_loss         | 2.2054262    |
-------------------------------------
Current reward shaping 0.9424
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.608165979385376 seconds
Total simulation time for 400 steps: 3.6599671840667725 	 Other agent action time: 0 	 109.29059739697993 steps/s
Curr learning rate 0.0009939984996249062 	 Curr reward per step 0.048612133333333335

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 195.77it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 196.03it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 197.40it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 197.56it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 199.48it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 200.98it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 201.17it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 200.94it/s]
-------------------------------------
| approxkl           | 0.00369128   |
| clipfrac           | 0.4761249    |
| eplenmean          | 400          |
| eprewmean          | 20.9         |
| explained_variance | 0.48         |
| fps                | 2795         |
| nupdates           | 25           |
| policy_entropy     | 1.714202     |
| policy_loss        | -0.010692421 |
| serial_timesteps   | 10000        |
| time_elapsed       | 109          |
| time_remaining     | 95.1         |
| total_timesteps    | 300000       |
| true_eprew         | 1            |
| value_loss         | 1.1812377    |
-------------------------------------
Current reward shaping 0.94
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6033577919006348 seconds
Total simulation time for 400 steps: 3.9541704654693604 	 Other agent action time: 0 	 101.15901767338701 steps/s
Curr learning rate 0.0009937484371092774 	 Curr reward per step 0.05830166666666666

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.77it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 206.40it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 207.57it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 206.51it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 207.31it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 207.89it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 207.60it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 206.37it/s]
-------------------------------------
| approxkl           | 0.0034687228 |
| clipfrac           | 0.46264574   |
| eplenmean          | 400          |
| eprewmean          | 21.3         |
| explained_variance | 0.328        |
| fps                | 2630         |
| nupdates           | 26           |
| policy_entropy     | 1.7057192    |
| policy_loss        | -0.009055957 |
| serial_timesteps   | 10400        |
| time_elapsed       | 114          |
| time_remaining     | 95.2         |
| total_timesteps    | 312000       |
| true_eprew         | 0.8          |
| value_loss         | 2.0322137    |
-------------------------------------
Current reward shaping 0.9376
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6089978218078613 seconds
Total simulation time for 400 steps: 3.6478476524353027 	 Other agent action time: 0 	 109.65370215857563 steps/s
Curr learning rate 0.0009934983745936484 	 Curr reward per step 0.06831333333333334

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.51it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 205.70it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.54it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 205.46it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 204.22it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 202.83it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.84it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 199.91it/s]
-------------------------------------
| approxkl           | 0.0034672106 |
| clipfrac           | 0.45849994   |
| eplenmean          | 400          |
| eprewmean          | 22.9         |
| explained_variance | 0.236        |
| fps                | 2812         |
| nupdates           | 27           |
| policy_entropy     | 1.6983546    |
| policy_loss        | -0.008541415 |
| serial_timesteps   | 10800        |
| time_elapsed       | 118          |
| time_remaining     | 95.1         |
| total_timesteps    | 324000       |
| true_eprew         | 1.6          |
| value_loss         | 3.8868325    |
-------------------------------------
Current reward shaping 0.9352
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6040358543395996 seconds
Total simulation time for 400 steps: 3.660552501678467 	 Other agent action time: 0 	 109.27312197177577 steps/s
Curr learning rate 0.0009932483120780196 	 Curr reward per step 0.06815606666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.78it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.12it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 202.51it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 203.05it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 202.42it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 204.16it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.99it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 202.47it/s]
-------------------------------------
| approxkl           | 0.0037699002 |
| clipfrac           | 0.48033336   |
| eplenmean          | 400          |
| eprewmean          | 25.2         |
| explained_variance | 0.288        |
| fps                | 2803         |
| nupdates           | 28           |
| policy_entropy     | 1.6912239    |
| policy_loss        | -0.009542938 |
| serial_timesteps   | 11200        |
| time_elapsed       | 122          |
| time_remaining     | 94.9         |
| total_timesteps    | 336000       |
| true_eprew         | 2.4          |
| value_loss         | 3.1476762    |
-------------------------------------
Current reward shaping 0.9328
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6068766117095947 seconds
Total simulation time for 400 steps: 3.664951801300049 	 Other agent action time: 0 	 109.14195375178198 steps/s
Curr learning rate 0.0009929982495623906 	 Curr reward per step 0.06197853333333332

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 199.36it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 202.23it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 202.65it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 201.92it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 201.95it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 201.47it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.26it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 202.44it/s]
-------------------------------------
| approxkl           | 0.0037566693 |
| clipfrac           | 0.4597188    |
| eplenmean          | 400          |
| eprewmean          | 25.9         |
| explained_variance | 0.385        |
| fps                | 2797         |
| nupdates           | 29           |
| policy_entropy     | 1.693089     |
| policy_loss        | -0.008508867 |
| serial_timesteps   | 11600        |
| time_elapsed       | 126          |
| time_remaining     | 94.8         |
| total_timesteps    | 348000       |
| true_eprew         | 2.8          |
| value_loss         | 2.5485651    |
-------------------------------------
Current reward shaping 0.9304
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6058096885681152 seconds
Total simulation time for 400 steps: 3.6724965572357178 	 Other agent action time: 0 	 108.91773314583564 steps/s
Curr learning rate 0.0009927481870467617 	 Curr reward per step 0.07559319999999999

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 198.34it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 200.65it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 200.08it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 199.20it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 199.57it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 198.94it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 199.29it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 198.95it/s]
--------------------------------------
| approxkl           | 0.0040490525  |
| clipfrac           | 0.47790638    |
| eplenmean          | 400           |
| eprewmean          | 27.1          |
| explained_variance | 0.303         |
| fps                | 2782          |
| nupdates           | 30            |
| policy_entropy     | 1.6851658     |
| policy_loss        | -0.0075727478 |
| serial_timesteps   | 12000         |
| time_elapsed       | 131           |
| time_remaining     | 94.7          |
| total_timesteps    | 360000        |
| true_eprew         | 2.8           |
| value_loss         | 3.9433606     |
--------------------------------------
Current reward shaping 0.928
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.605034589767456 seconds
Total simulation time for 400 steps: 3.6499416828155518 	 Other agent action time: 0 	 109.59079206203685 steps/s
Curr learning rate 0.0009924981245311329 	 Curr reward per step 0.07693600000000002

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 198.31it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 197.78it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 201.41it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 204.19it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 202.31it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.03it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 201.71it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 202.34it/s]
--------------------------------------
| approxkl           | 0.003947545   |
| clipfrac           | 0.47985417    |
| eplenmean          | 400           |
| eprewmean          | 28.3          |
| explained_variance | 0.228         |
| fps                | 2807          |
| nupdates           | 31            |
| policy_entropy     | 1.670726      |
| policy_loss        | -0.0070980135 |
| serial_timesteps   | 12400         |
| time_elapsed       | 135           |
| time_remaining     | 94.5          |
| total_timesteps    | 372000        |
| true_eprew         | 3.4           |
| value_loss         | 4.563117      |
--------------------------------------
Current reward shaping 0.9256
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5993056297302246 seconds
Total simulation time for 400 steps: 3.6584115028381348 	 Other agent action time: 0 	 109.33707148298835 steps/s
Curr learning rate 0.0009922480620155039 	 Curr reward per step 0.07635126666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 199.18it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 201.22it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 202.19it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 202.90it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 200.48it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 204.64it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 203.92it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 203.47it/s]
-------------------------------------
| approxkl           | 0.003693293  |
| clipfrac           | 0.47823966   |
| eplenmean          | 400          |
| eprewmean          | 30.1         |
| explained_variance | 0.289        |
| fps                | 2804         |
| nupdates           | 32           |
| policy_entropy     | 1.6773137    |
| policy_loss        | -0.008229169 |
| serial_timesteps   | 12800        |
| time_elapsed       | 139          |
| time_remaining     | 94.4         |
| total_timesteps    | 384000       |
| true_eprew         | 4.6          |
| value_loss         | 4.314852     |
-------------------------------------
Current reward shaping 0.9232
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6108267307281494 seconds
Total simulation time for 400 steps: 4.007561683654785 	 Other agent action time: 0 	 99.81131460345011 steps/s
Curr learning rate 0.000991997999499875 	 Curr reward per step 0.08529120000000001

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 195.49it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 196.05it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 197.54it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 197.86it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 195.57it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 198.03it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 201.88it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 202.96it/s]
--------------------------------------
| approxkl           | 0.0037454462  |
| clipfrac           | 0.46762493    |
| eplenmean          | 400           |
| eprewmean          | 32.1          |
| explained_variance | 0.201         |
| fps                | 2585          |
| nupdates           | 33            |
| policy_entropy     | 1.6659149     |
| policy_loss        | -0.0077356934 |
| serial_timesteps   | 13200         |
| time_elapsed       | 144           |
| time_remaining     | 94.5          |
| total_timesteps    | 396000        |
| true_eprew         | 5.6           |
| value_loss         | 6.1944103     |
--------------------------------------
Current reward shaping 0.9208
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6046602725982666 seconds
Total simulation time for 400 steps: 3.6310839653015137 	 Other agent action time: 0 	 110.15994227133915 steps/s
Curr learning rate 0.0009917479369842461 	 Curr reward per step 0.0927432

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 201.49it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 204.81it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 205.23it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 205.33it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 201.23it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 200.13it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 200.93it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 199.65it/s]
--------------------------------------
| approxkl           | 0.0041837185  |
| clipfrac           | 0.48421878    |
| eplenmean          | 400           |
| eprewmean          | 33.4          |
| explained_variance | 0.212         |
| fps                | 2822          |
| nupdates           | 34            |
| policy_entropy     | 1.6503036     |
| policy_loss        | -0.0076108626 |
| serial_timesteps   | 13600         |
| time_elapsed       | 148           |
| time_remaining     | 94.4          |
| total_timesteps    | 408000        |
| true_eprew         | 6.4           |
| value_loss         | 6.406706      |
--------------------------------------
Current reward shaping 0.9184
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.606097936630249 seconds
Total simulation time for 400 steps: 3.6950976848602295 	 Other agent action time: 0 	 108.25153598480046 steps/s
Curr learning rate 0.0009914978744686171 	 Curr reward per step 0.09816586666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.78it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.54it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.66it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 204.92it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 203.40it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 204.01it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 203.22it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 203.47it/s]
-------------------------------------
| approxkl           | 0.004120584  |
| clipfrac           | 0.4775937    |
| eplenmean          | 400          |
| eprewmean          | 36.2         |
| explained_variance | 0.199        |
| fps                | 2782         |
| nupdates           | 35           |
| policy_entropy     | 1.6548682    |
| policy_loss        | -0.008198632 |
| serial_timesteps   | 14000        |
| time_elapsed       | 153          |
| time_remaining     | 94.3         |
| total_timesteps    | 420000       |
| true_eprew         | 8            |
| value_loss         | 7.807662     |
-------------------------------------
Current reward shaping 0.916
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6106564998626709 seconds
Total simulation time for 400 steps: 3.6808931827545166 	 Other agent action time: 0 	 108.66927675979683 steps/s
Curr learning rate 0.0009912478119529881 	 Curr reward per step 0.10139700000000003

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 188.77it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 191.24it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 190.83it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 190.90it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 195.15it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 204.31it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 204.77it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.26it/s]
--------------------------------------
| approxkl           | 0.004169737   |
| clipfrac           | 0.47567704    |
| eplenmean          | 400           |
| eprewmean          | 38.9          |
| explained_variance | 0.201         |
| fps                | 2777          |
| nupdates           | 36            |
| policy_entropy     | 1.6446893     |
| policy_loss        | -0.0074091493 |
| serial_timesteps   | 14400         |
| time_elapsed       | 157           |
| time_remaining     | 94.2          |
| total_timesteps    | 432000        |
| true_eprew         | 9.6           |
| value_loss         | 8.477064      |
--------------------------------------
Current reward shaping 0.9136
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6063597202301025 seconds
Total simulation time for 400 steps: 3.683337450027466 	 Other agent action time: 0 	 108.59716369376292 steps/s
Curr learning rate 0.0009909977494373594 	 Curr reward per step 0.11181853333333333

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 199.49it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 202.50it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 200.89it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 202.51it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 204.02it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.54it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 201.16it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 201.81it/s]
-------------------------------------
| approxkl           | 0.0041726627 |
| clipfrac           | 0.4720938    |
| eplenmean          | 400          |
| eprewmean          | 41.2         |
| explained_variance | 0.165        |
| fps                | 2787         |
| nupdates           | 37           |
| policy_entropy     | 1.6390992    |
| policy_loss        | -0.006842671 |
| serial_timesteps   | 14800        |
| time_elapsed       | 161          |
| time_remaining     | 94.1         |
| total_timesteps    | 444000       |
| true_eprew         | 11.4         |
| value_loss         | 10.277379    |
-------------------------------------
Current reward shaping 0.9112
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5963578224182129 seconds
Total simulation time for 400 steps: 3.654576063156128 	 Other agent action time: 0 	 109.45181960573453 steps/s
Curr learning rate 0.0009907476869217304 	 Curr reward per step 0.10942626666666666

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 199.49it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 201.87it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 202.91it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 201.98it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 203.42it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 204.00it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 199.32it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 203.54it/s]
-------------------------------------
| approxkl           | 0.0036142515 |
| clipfrac           | 0.4496146    |
| eplenmean          | 400          |
| eprewmean          | 42.3         |
| explained_variance | 0.173        |
| fps                | 2806         |
| nupdates           | 38           |
| policy_entropy     | 1.6419054    |
| policy_loss        | -0.006417371 |
| serial_timesteps   | 15200        |
| time_elapsed       | 165          |
| time_remaining     | 94           |
| total_timesteps    | 456000       |
| true_eprew         | 11.8         |
| value_loss         | 9.624851     |
-------------------------------------
Current reward shaping 0.9088
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5984737873077393 seconds
Total simulation time for 400 steps: 3.6706485748291016 	 Other agent action time: 0 	 108.97256761187585 steps/s
Curr learning rate 0.0009904976244061016 	 Curr reward per step 0.11315626666666669

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 192.13it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 195.88it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 186.44it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 194.84it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 195.81it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 195.96it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 195.36it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 195.45it/s]
-------------------------------------
| approxkl           | 0.0036628474 |
| clipfrac           | 0.4582189    |
| eplenmean          | 400          |
| eprewmean          | 44.1         |
| explained_variance | 0.169        |
| fps                | 2779         |
| nupdates           | 39           |
| policy_entropy     | 1.6243986    |
| policy_loss        | -0.008141904 |
| serial_timesteps   | 15600        |
| time_elapsed       | 170          |
| time_remaining     | 93.9         |
| total_timesteps    | 468000       |
| true_eprew         | 13           |
| value_loss         | 9.594717     |
-------------------------------------
Current reward shaping 0.9064
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.597386360168457 seconds
Total simulation time for 400 steps: 3.6491503715515137 	 Other agent action time: 0 	 109.61455661525166 steps/s
Curr learning rate 0.0009902475618904726 	 Curr reward per step 0.14033786666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.23it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 207.61it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 208.07it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 206.19it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 206.09it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 205.82it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 207.48it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 207.83it/s]
-------------------------------------
| approxkl           | 0.004320449  |
| clipfrac           | 0.4895938    |
| eplenmean          | 400          |
| eprewmean          | 48.4         |
| explained_variance | 0.133        |
| fps                | 2812         |
| nupdates           | 40           |
| policy_entropy     | 1.6193317    |
| policy_loss        | -0.008380983 |
| serial_timesteps   | 16000        |
| time_elapsed       | 174          |
| time_remaining     | 93.8         |
| total_timesteps    | 480000       |
| true_eprew         | 15.2         |
| value_loss         | 15.5838785   |
-------------------------------------
Current reward shaping 0.904
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5998754501342773 seconds
Total simulation time for 400 steps: 3.6492905616760254 	 Other agent action time: 0 	 109.61034569313392 steps/s
Curr learning rate 0.0009899974993748438 	 Curr reward per step 0.13937333333333332

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 197.31it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 198.01it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 197.04it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 199.20it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 200.07it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 195.97it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 197.19it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 194.63it/s]
-------------------------------------
| approxkl           | 0.0032032127 |
| clipfrac           | 0.4360522    |
| eplenmean          | 400          |
| eprewmean          | 51.6         |
| explained_variance | 0.119        |
| fps                | 2800         |
| nupdates           | 41           |
| policy_entropy     | 1.6116985    |
| policy_loss        | -0.008356153 |
| serial_timesteps   | 16400        |
| time_elapsed       | 178          |
| time_remaining     | 93.7         |
| total_timesteps    | 492000       |
| true_eprew         | 18           |
| value_loss         | 15.5153      |
-------------------------------------
Current reward shaping 0.9016
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6016778945922852 seconds
Total simulation time for 400 steps: 4.085421323776245 	 Other agent action time: 0 	 97.9091183746677 steps/s
Curr learning rate 0.0009897474368592148 	 Curr reward per step 0.15678546666666668

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.88it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 205.36it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 205.43it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 205.11it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 203.65it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.01it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 204.05it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.04it/s]
-------------------------------------
| approxkl           | 0.0038327395 |
| clipfrac           | 0.4680731    |
| eplenmean          | 400          |
| eprewmean          | 56.7         |
| explained_variance | 0.114        |
| fps                | 2551         |
| nupdates           | 42           |
| policy_entropy     | 1.5993273    |
| policy_loss        | -0.008590581 |
| serial_timesteps   | 16800        |
| time_elapsed       | 183          |
| time_remaining     | 93.8         |
| total_timesteps    | 504000       |
| true_eprew         | 21.2         |
| value_loss         | 18.831234    |
-------------------------------------
Current reward shaping 0.8992
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6033377647399902 seconds
Total simulation time for 400 steps: 3.6832759380340576 	 Other agent action time: 0 	 108.59897730429051 steps/s
Curr learning rate 0.0009894973743435858 	 Curr reward per step 0.16552186666666663

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 196.75it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 198.02it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 200.06it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 198.90it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 199.91it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 198.38it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 197.52it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 197.13it/s]
-------------------------------------
| approxkl           | 0.0037722879 |
| clipfrac           | 0.46806243   |
| eplenmean          | 400          |
| eprewmean          | 61           |
| explained_variance | 0.146        |
| fps                | 2779         |
| nupdates           | 43           |
| policy_entropy     | 1.5983244    |
| policy_loss        | -0.008243958 |
| serial_timesteps   | 17200        |
| time_elapsed       | 187          |
| time_remaining     | 93.7         |
| total_timesteps    | 516000       |
| true_eprew         | 24.2         |
| value_loss         | 19.944113    |
-------------------------------------
Current reward shaping 0.8968
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6059296131134033 seconds
Total simulation time for 400 steps: 3.6887941360473633 	 Other agent action time: 0 	 108.43652024143861 steps/s
Curr learning rate 0.000989247311827957 	 Curr reward per step 0.16822213333333333

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 196.84it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 198.01it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 200.60it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 201.08it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 202.51it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.38it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.48it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 203.03it/s]
--------------------------------------
| approxkl           | 0.0037336994  |
| clipfrac           | 0.46690622    |
| eplenmean          | 400           |
| eprewmean          | 64.7          |
| explained_variance | 0.0674        |
| fps                | 2780          |
| nupdates           | 44            |
| policy_entropy     | 1.594862      |
| policy_loss        | -0.0074245348 |
| serial_timesteps   | 17600         |
| time_elapsed       | 192           |
| time_remaining     | 93.6          |
| total_timesteps    | 528000        |
| true_eprew         | 26.6          |
| value_loss         | 21.80114      |
--------------------------------------
Current reward shaping 0.8944
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6089715957641602 seconds
Total simulation time for 400 steps: 3.6690170764923096 	 Other agent action time: 0 	 109.02102433996083 steps/s
Curr learning rate 0.000988997249312328 	 Curr reward per step 0.18355373333333333

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.76it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 204.64it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.91it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 205.89it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 205.81it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 205.15it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 205.25it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.89it/s]
--------------------------------------
| approxkl           | 0.0037732224  |
| clipfrac           | 0.45069787    |
| eplenmean          | 400           |
| eprewmean          | 68.8          |
| explained_variance | 0.0639        |
| fps                | 2802          |
| nupdates           | 45            |
| policy_entropy     | 1.5828158     |
| policy_loss        | -0.0060726856 |
| serial_timesteps   | 18000         |
| time_elapsed       | 196           |
| time_remaining     | 93.5          |
| total_timesteps    | 540000        |
| true_eprew         | 29.4          |
| value_loss         | 23.800455     |
--------------------------------------
Current reward shaping 0.892
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6002962589263916 seconds
Total simulation time for 400 steps: 3.6985480785369873 	 Other agent action time: 0 	 108.15054759494315 steps/s
Curr learning rate 0.000988747186796699 	 Curr reward per step 0.1560153333333333

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 203.05it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 202.81it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 203.68it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 204.79it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 193.54it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 192.13it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 191.12it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 192.35it/s]
--------------------------------------
| approxkl           | 0.00316024    |
| clipfrac           | 0.42948958    |
| eplenmean          | 400           |
| eprewmean          | 67.7          |
| explained_variance | 0.127         |
| fps                | 2768          |
| nupdates           | 46            |
| policy_entropy     | 1.5987105     |
| policy_loss        | -0.0066577634 |
| serial_timesteps   | 18400         |
| time_elapsed       | 200           |
| time_remaining     | 93.4          |
| total_timesteps    | 552000        |
| true_eprew         | 29.2          |
| value_loss         | 17.977457     |
--------------------------------------
Current reward shaping 0.8896
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6059603691101074 seconds
Total simulation time for 400 steps: 3.6550958156585693 	 Other agent action time: 0 	 109.43625562054619 steps/s
Curr learning rate 0.0009884971242810703 	 Curr reward per step 0.18737546666666663

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.97it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 204.17it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 203.67it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 202.34it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 200.95it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 200.30it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 199.64it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 200.63it/s]
-------------------------------------
| approxkl           | 0.0031494326 |
| clipfrac           | 0.4369584    |
| eplenmean          | 400          |
| eprewmean          | 70           |
| explained_variance | 0.0453       |
| fps                | 2804         |
| nupdates           | 47           |
| policy_entropy     | 1.5922239    |
| policy_loss        | -0.008153912 |
| serial_timesteps   | 18800        |
| time_elapsed       | 205          |
| time_remaining     | 93.3         |
| total_timesteps    | 564000       |
| true_eprew         | 31.2         |
| value_loss         | 25.25759     |
-------------------------------------
Current reward shaping 0.8872
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6069083213806152 seconds
Total simulation time for 400 steps: 3.7003726959228516 	 Other agent action time: 0 	 108.09721962350669 steps/s
Curr learning rate 0.0009882470617654413 	 Curr reward per step 0.19998533333333335

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 194.86it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 197.35it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 195.47it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 198.40it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 200.49it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 200.77it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.40it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 201.76it/s]
-------------------------------------
| approxkl           | 0.0036579699 |
| clipfrac           | 0.4750208    |
| eplenmean          | 400          |
| eprewmean          | 72.3         |
| explained_variance | 0.0592       |
| fps                | 2768         |
| nupdates           | 48           |
| policy_entropy     | 1.5758744    |
| policy_loss        | -0.007114576 |
| serial_timesteps   | 19200        |
| time_elapsed       | 209          |
| time_remaining     | 93.2         |
| total_timesteps    | 576000       |
| true_eprew         | 33           |
| value_loss         | 28.431261    |
-------------------------------------
Current reward shaping 0.8848
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6021127700805664 seconds
Total simulation time for 400 steps: 3.6355597972869873 	 Other agent action time: 0 	 110.02432150847784 steps/s
Curr learning rate 0.0009879969992498126 	 Curr reward per step 0.1941852

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 201.09it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.66it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.09it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 202.28it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 202.74it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.04it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.38it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 201.69it/s]
--------------------------------------
| approxkl           | 0.003934736   |
| clipfrac           | 0.48384383    |
| eplenmean          | 400           |
| eprewmean          | 75            |
| explained_variance | 0.0739        |
| fps                | 2819          |
| nupdates           | 49            |
| policy_entropy     | 1.572384      |
| policy_loss        | -0.0057301945 |
| serial_timesteps   | 19600         |
| time_elapsed       | 213           |
| time_remaining     | 93.1          |
| total_timesteps    | 588000        |
| true_eprew         | 34.6          |
| value_loss         | 25.34466      |
--------------------------------------
Current reward shaping 0.8824
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6034262180328369 seconds
Total simulation time for 400 steps: 3.6532180309295654 	 Other agent action time: 0 	 109.49250677442308 steps/s
Curr learning rate 0.0009877469367341836 	 Curr reward per step 0.19169660000000005

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 190.30it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 191.97it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 191.46it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 185.57it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 190.74it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 190.54it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 190.83it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 184.72it/s]
-------------------------------------
| approxkl           | 0.0034188845 |
| clipfrac           | 0.46549994   |
| eplenmean          | 400          |
| eprewmean          | 78.1         |
| explained_variance | 0.036        |
| fps                | 2780         |
| nupdates           | 50           |
| policy_entropy     | 1.5689467    |
| policy_loss        | -0.005560776 |
| serial_timesteps   | 20000        |
| time_elapsed       | 217          |
| time_remaining     | 93           |
| total_timesteps    | 600000       |
| true_eprew         | 36.8         |
| value_loss         | 25.829596    |
-------------------------------------
Current reward shaping 0.88
Current self-play randomization 1
data/ppo_poor_runs/ppo_bc_train_random1/
PPO agent on index 0:
X X X P X 
X   ↑0  P 
D ↑1X   X 
O       X 
X O S X X 

Timestep: 1
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X   ↓0  P 
D ←1X   X 
O       X 
X O S X X 


Timestep: 2
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ←0    P 
D ←1X   X 
O       X 
X O S X X 


Timestep: 3
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ←0    P 
D ←1X   X 
O       X 
X O S X X 


Timestep: 4
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ↓0    P 
D ←1X   X 
O       X 
X O S X X 


Timestep: 5
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ↓0X   X 
O ↓1    X 
X O S X X 


Timestep: 6
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ↓0X   X 
O ↓1    X 
X O S X X 


Timestep: 7
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ↓0X   X 
O ↓o    X 
X O S X X 


Timestep: 8
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ↓0X   X 
O ↓o    X 
X O S X X 


Timestep: 9
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ↓0X   X 
O   →o  X 
X O S X X 


Timestep: 10
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D →0X   X 
O   →o  X 
X O S X X 


Timestep: 11
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D   X   X 
O ↓0  →oX 
X O S X X 


Timestep: 12
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D   X   X 
O ↓0  →oX 
X O S X X 


Timestep: 13
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D   X ↑oX 
O ↓0    X 
X O S X X 


Timestep: 14
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X X P X 
X     ↑oP 
D   X   X 
O ↓0    X 
X O S X X 


Timestep: 15
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X     ↑oP 
D ↑0X   X 
O       X 
X O S X X 


Timestep: 16
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X X ø-X 
X     ↑1P 
D ↑0X   X 
O       X 
X O S X X 


Timestep: 17
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       P 
D ↑0X ↓1X 
O       X 
X O S X X 


Timestep: 18
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       P 
D ←0X ↓1X 
O       X 
X O S X X 


Timestep: 19
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       P 
D →0X   X 
O     ↓1X 
X O S X X 


Timestep: 20
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       P 
D →0X   X 
O     ↓1X 
X O S X X 


Timestep: 21
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       P 
D   X   X 
O ↓0  ↓1X 
X O S X X 


Timestep: 22
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       P 
D   X   X 
O ←0  ↓1X 
X O S X X 


Timestep: 23
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       P 
D   X   X 
O ←o←1  X 
X O S X X 


Timestep: 24
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       P 
D ↑oX   X 
O   ←1  X 
X O S X X 


Timestep: 25
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ↑o    P 
D   X   X 
O   ←1  X 
X O S X X 


Timestep: 26
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ↑o    P 
D   X   X 
O ←1    X 
X O S X X 


Timestep: 27
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ←o    P 
D   X   X 
O ←1    X 
X O S X X 


Timestep: 28
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X   →o  P 
D   X   X 
O   →1  X 
X O S X X 


Timestep: 29
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X   ↓o  P 
D   X   X 
O   →1  X 
X O S X X 


Timestep: 30
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     →oP 
D   X   X 
O   →1  X 
X O S X X 


Timestep: 31
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X   ←o  P 
D   X   X 
O   →1  X 
X O S X X 


Timestep: 32
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X   ↓o  P 
D   X   X 
O ←1    X 
X O S X X 


Timestep: 33
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X   ↓o  P 
D   X   X 
O ←1    X 
X O S X X 


Timestep: 34
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     →oP 
D   X   X 
O ←1    X 
X O S X X 


Timestep: 35
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X X ø-X 
X     →0ø-
D   X   X 
O ←1    X 
X O S X X 


Timestep: 36
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X X ø-X 
X   ←0  ø-
D   X   X 
O ↓1    X 
X O S X X 


Timestep: 37
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ←0    ø-
D   X   X 
O ↓o    X 
X O S X X 


Timestep: 38
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø-
D ↓0X   X 
O   →o  X 
X O S X X 


Timestep: 39
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø-
D ←0X   X 
O   →o  X 
X O S X X 


Timestep: 40
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 3 
X X X ø-X 
X       ø-
D ←dX   X 
O     →oX 
X O S X X 


Timestep: 41
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ↑d    ø-
D   X   X 
O     →oX 
X O S X X 


Timestep: 42
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ←d    ø-
D   X   X 
O     →oX 
X O S X X 


Timestep: 43
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø-
D ↓dX ↑oX 
O       X 
X O S X X 


Timestep: 44
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑oø-
D ←dX   X 
O       X 
X O S X X 


Timestep: 45
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑oø-
D ←dX   X 
O       X 
X O S X X 


Timestep: 46
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑oø-
D ←dX   X 
O       X 
X O S X X 


Timestep: 47
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ↑d  ↑oø-
D   X   X 
O       X 
X O S X X 


Timestep: 48
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X   →d→oø-
D   X   X 
O       X 
X O S X X 


Timestep: 49
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ←d  →oø-
D   X   X 
O       X 
X O S X X 


Timestep: 50
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X X ø-X 
X ←d  →1ø=
D   X   X 
O       X 
X O S X X 


Timestep: 51
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X X ø-X 
Xd←0    ø=
D   X ↓1X 
O       X 
X O S X X 


Timestep: 52
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ←d    ø=
D   X   X 
O     ↓1X 
X O S X X 


Timestep: 53
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ←d    ø=
D   X   X 
O     ↓1X 
X O S X X 


Timestep: 54
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D ↓dX   X 
O   ←1  X 
X O S X X 


Timestep: 55
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D ←dX   X 
O   ←1  X 
X O S X X 


Timestep: 56
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ↑d    ø=
D   X   X 
O ←1    X 
X O S X X 


Timestep: 57
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ↑d    ø=
D   X   X 
O ←1    X 
X O S X X 


Timestep: 58
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D ↓dX   X 
O ←1    X 
X O S X X 


Timestep: 59
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D ↓dX   X 
O ↓1    X 
X O S X X 


Timestep: 60
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D →dX   X 
O ↓1    X 
X O S X X 


Timestep: 61
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D →0Xd  X 
O ↓1    X 
X O S X X 


Timestep: 62
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D →0Xd  X 
O ↓1    X 
X O S X X 


Timestep: 63
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D →0Xd  X 
O ↓o    X 
X O S X X 


Timestep: 64
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D ←0Xd  X 
O   →o  X 
X O S X X 


Timestep: 65
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D ←0Xd  X 
O   →o  X 
X O S X X 


Timestep: 66
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D   Xd  X 
O ↓0  →oX 
X O S X X 


Timestep: 67
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D   Xd  X 
O ↓0  →oX 
X O S X X 


Timestep: 68
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D   Xd↑oX 
O ↓o    X 
X O S X X 


Timestep: 69
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D ↑oXd↑oX 
O       X 
X O S X X 


Timestep: 70
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑oø=
D ←oXd  X 
O       X 
X O S X X 


Timestep: 71
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ↑o  ↑oø=
D   Xd  X 
O       X 
X O S X X 


Timestep: 72
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ↑o  ↑oø=
D   Xd  X 
O       X 
X O S X X 


Timestep: 73
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ←o  ↑oø=
D   Xd  X 
O       X 
X O S X X 


Timestep: 74
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 3 
X X X ø=X 
X ↑o  ↑1ø=
D   Xd  X 
O       X 
X O S X X 


Timestep: 75
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X X ø=X 
X ←o  →1ø=
D   Xd  X 
O       X 
X O S X X 


Timestep: 76
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X X ø=X 
X   →o→1ø=
D   Xd  X 
O       X 
X O S X X 


Timestep: 77
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X X ø=X 
X   ↓o  ø=
D   Xd↓1X 
O       X 
X O S X X 


Timestep: 78
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X ø=X 
X   ↓o  ø=
D   Xd↓1X 
O       X 
X O S X X 


Timestep: 79
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X X ø=X 
X     →oø=
D   Xd  X 
O     ↓1X 
X O S X X 


Timestep: 80
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X X ø=X 
X     ↑oø=
D   Xd  X 
O     ↓1X 
X O S X X 


Timestep: 81
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 3 
X X X ø1X 
X     ↑0ø=
D   Xd  X 
O   ←1  X 
X O S X X 


Timestep: 82
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X X ø2X 
X       ø=
D   Xd↓0X 
O ←1    X 
X O S X X 


Timestep: 83
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X X ø3X 
X       ø=
D ↑1Xd→0X 
O       X 
X O S X X 


Timestep: 84
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X X ø4X 
X       ø=
D ↑1Xd←0X 
O       X 
X O S X X 


Timestep: 85
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X X ø5X 
X       ø=
D ↑1Xd→0X 
O       X 
X O S X X 


Timestep: 86
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X X ø6X 
X       ø=
D ↑1Xd  X 
O     ↓0X 
X O S X X 


Timestep: 87
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X X ø7X 
X ↑1    ø=
D   Xd  X 
O     →0X 
X O S X X 


Timestep: 88
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X ø8X 
X ↑1    ø=
D   Xd  X 
O   ←0  X 
X O S X X 


Timestep: 89
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X X ø9X 
X ↑1    ø=
D   Xd  X 
O   ↑0  X 
X O S X X 


Timestep: 90
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X X ø10X 
X ↑1    ø=
D   Xd  X 
O     →0X 
X O S X X 


Timestep: 91
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X ø11X 
X ←1    ø=
D   Xd  X 
O     →0X 
X O S X X 


Timestep: 92
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X X ø12X 
X ←1    ø=
D   Xd  X 
O   ←0  X 
X O S X X 


Timestep: 93
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X X ø13X 
X ←1    ø=
D   Xd  X 
O   ←0  X 
X O S X X 


Timestep: 94
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X X ø14X 
X ←1    ø=
D   Xd  X 
O   ↑0  X 
X O S X X 


Timestep: 95
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X X ø15X 
X       ø=
D ↓1Xd  X 
O     →0X 
X O S X X 


Timestep: 96
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X X ø16X 
X       ø=
D   Xd  X 
O ↓1  →0X 
X O S X X 


Timestep: 97
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X X ø17X 
X       ø=
D   Xd  X 
O ↓1  →0X 
X O S X X 


Timestep: 98
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X X ø18X 
X       ø=
D   Xd  X 
O ↓1  →0X 
X O S X X 


Timestep: 99
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X X ø19X 
X       ø=
D   Xd  X 
O ←1←0  X 
X O S X X 


tot rew 60 tot rew shaped 68
PPO agent on index 1:
X X X P X 
X   ↑0  P 
D ↑1X   X 
O       X 
X O S X X 

Timestep: 1
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X   ↑0  P 
D   X   X 
O ↓1    X 
X O S X X 


Timestep: 2
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X   ↑0  P 
D   X   X 
O   →1  X 
X O S X X 


Timestep: 3
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X X P X 
X   ↑0  P 
D   X   X 
O   ↑1  X 
X O S X X 


Timestep: 4
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X ←0    P 
D   X   X 
O   ↑1  X 
X O S X X 


Timestep: 5
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X ←0    P 
D   X   X 
O ←1    X 
X O S X X 


Timestep: 6
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X X P X 
X ←0    P 
D ↑1X   X 
O       X 
X O S X X 


Timestep: 7
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ←0    P 
D ↑1X   X 
O       X 
X O S X X 


Timestep: 8
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ↓0X   X 
O ↓1    X 
X O S X X 


Timestep: 9
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ↓0X   X 
O ↓o    X 
X O S X X 


Timestep: 10
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ↓0X   X 
O ←o    X 
X O S X X 


Timestep: 11
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ←0X   X 
O ←o    X 
X O S X X 


Timestep: 12
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ←0X   X 
O ←o    X 
X O S X X 


Timestep: 13
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ←0X   X 
O   →o  X 
X O S X X 


Timestep: 14
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ←0X   X 
O   ↑o  X 
X O S X X 


Timestep: 15
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D   Xo  X 
O ↓0↑1  X 
X O S X X 


Timestep: 16
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D   Xo  X 
O ↓o←1  X 
X O S X X 


Timestep: 17
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ↑oXo  X 
O   ←1  X 
X O S X X 


Timestep: 18
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ↑o    P 
D   Xo  X 
O   ←1  X 
X O S X X 


Timestep: 19
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X ↑o    P 
D   Xo  X 
O ←1    X 
X O S X X 


Timestep: 20
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X ↑o    P 
D   Xo  X 
O ←o    X 
X O S X X 


Timestep: 21
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X   →o  P 
D   Xo  X 
O   →o  X 
X O S X X 


Timestep: 22
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X     →oP 
D   Xo  X 
O   →o  X 
X O S X X 


Timestep: 23
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X     →oP 
D   Xo  X 
O   →o  X 
X O S X X 


Timestep: 24
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X     →oP 
D   Xo  X 
O ←o    X 
X O S X X 


Timestep: 25
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X     →oP 
D   Xo  X 
O ←o    X 
X O S X X 


Timestep: 26
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 3 
X X X P X 
X     →0ø-
D ↑oXo  X 
O       X 
X O S X X 


Timestep: 27
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø-
D →oXo↓0X 
O       X 
X O S X X 


Timestep: 28
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø-
D →oXo  X 
O     ↓0X 
X O S X X 


Timestep: 29
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø-
D →oXo  X 
O     ↓0X 
X O S X X 


Timestep: 30
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X X P X 
X ↑o    ø-
D   Xo  X 
O     ↓0X 
X O S X X 


Timestep: 31
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ↑o    ø-
D   Xo  X 
O     ↓0X 
X O S X X 


Timestep: 32
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X   →o  ø-
D   Xo↑0X 
O       X 
X O S X X 


Timestep: 33
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X   ↓o  ø-
D   Xo  X 
O     ↓0X 
X O S X X 


Timestep: 34
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X     →oø-
D   Xo  X 
O     ↓0X 
X O S X X 


Timestep: 35
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X X P X 
X     →1ø=
D   Xo  X 
O     ↓0X 
X O S X X 


Timestep: 36
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X   ←1  ø=
D   Xo  X 
O   ←0  X 
X O S X X 


Timestep: 37
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X ←1    ø=
D   Xo  X 
O   ←0  X 
X O S X X 


Timestep: 38
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X ←1    ø=
D   Xo  X 
O ←0    X 
X O S X X 


Timestep: 39
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ←1    ø=
D   Xo  X 
O ←0    X 
X O S X X 


Timestep: 40
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X ←1    ø=
D   Xo  X 
O ←0    X 
X O S X X 


Timestep: 41
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X ←1    ø=
D   Xo  X 
O ←0    X 
X O S X X 


Timestep: 42
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X ←1    ø=
D   Xo  X 
O ↓0    X 
X O S X X 


Timestep: 43
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X   →1  ø=
D   Xo  X 
O ↓o    X 
X O S X X 


Timestep: 44
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X   ↓1  ø=
D   Xo  X 
O   →o  X 
X O S X X 


Timestep: 45
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X ←1    ø=
D   Xo  X 
O     →oX 
X O S X X 


Timestep: 46
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X ←1    ø=
D   Xo↑oX 
O       X 
X O S X X 


Timestep: 47
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X ←1    ø=
D   Xo↑oX 
O       X 
X O S X X 


Timestep: 48
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X ←1  ↑oø=
D   Xo  X 
O       X 
X O S X X 


Timestep: 49
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 3 
X X X ø-X 
X ↑1  ↑0ø=
D   Xo  X 
O       X 
X O S X X 


Timestep: 50
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ←1  ↑0ø=
D   Xo  X 
O       X 
X O S X X 


Timestep: 51
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X   →1↑0ø=
D   Xo  X 
O       X 
X O S X X 


Timestep: 52
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X X ø-X 
X   ↑1  ø=
D   Xo↓0X 
O       X 
X O S X X 


Timestep: 53
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     →1ø=
D   Xo↓0X 
O       X 
X O S X X 


Timestep: 54
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     →1ø=
D   Xo↓0X 
O       X 
X O S X X 


Timestep: 55
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X   ←1  ø=
D   Xo↓0X 
O       X 
X O S X X 


Timestep: 56
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X   ←1  ø=
D   Xo  X 
O     ↓0X 
X O S X X 


Timestep: 57
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ←1    ø=
D   Xo  X 
O   ←0  X 
X O S X X 


Timestep: 58
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D ↓1Xo  X 
O ←0    X 
X O S X X 


Timestep: 59
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D →1Xo  X 
O ↓0    X 
X O S X X 


Timestep: 60
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D →oX   X 
O ↓0    X 
X O S X X 


Timestep: 61
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D →1Xo  X 
O ↓0    X 
X O S X X 


Timestep: 62
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D →oX   X 
O ↓o    X 
X O S X X 


Timestep: 63
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D →1Xo  X 
O   →o  X 
X O S X X 


Timestep: 64
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D ←1Xo  X 
O   →o  X 
X O S X X 


Timestep: 65
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D ←1Xo  X 
O     →oX 
X O S X X 


Timestep: 66
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D ←1Xo↑oX 
O       X 
X O S X X 


Timestep: 67
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X X ø-X 
X       ø=
D ←dXo↑oX 
O       X 
X O S X X 


Timestep: 68
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø=
D ←dXo↑oX 
O       X 
X O S X X 


Timestep: 69
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑oø=
D ←dXo  X 
O       X 
X O S X X 


Timestep: 70
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑oø=
D ←dXo  X 
O       X 
X O S X X 


Timestep: 71
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑oø=
D →dXo  X 
O       X 
X O S X X 


Timestep: 72
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ↑d  ↑oø=
D   Xo  X 
O       X 
X O S X X 


Timestep: 73
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X XdX ø-X 
X ↑1  →oø=
D   Xo  X 
O       X 
X O S X X 


Timestep: 74
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X XdX ø-X 
X ↑1  →oø=
D   Xo  X 
O       X 
X O S X X 


Timestep: 75
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X XdX ø-X 
X ↑1  →oø=
D   Xo  X 
O       X 
X O S X X 


Timestep: 76
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X X ø-X 
X ↑d  →0ø1
D   Xo  X 
O       X 
X O S X X 


Timestep: 77
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ←d    ø2
D   Xo↓0X 
O       X 
X O S X X 


Timestep: 78
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X   →d  ø3
D   Xo  X 
O     ↓0X 
X O S X X 


Timestep: 79
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     →dø4
D   Xo  X 
O   ←0  X 
X O S X X 


Timestep: 80
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     →dø5
D   Xo  X 
O   ←0  X 
X O S X X 


Timestep: 81
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     →dø6
D   Xo  X 
O ←0    X 
X O S X X 


Timestep: 82
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     →dø7
D   Xo  X 
O ←0    X 
X O S X X 


Timestep: 83
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     →dø8
D ↑0Xo  X 
O       X 
X O S X X 


Timestep: 84
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     →dø9
D ↑0Xo  X 
O       X 
X O S X X 


Timestep: 85
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     →dø10
D ←0Xo  X 
O       X 
X O S X X 


Timestep: 86
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     →dø11
D ←0Xo  X 
O       X 
X O S X X 


Timestep: 87
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     →dø12
D ←0Xo  X 
O       X 
X O S X X 


Timestep: 88
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     →dø13
D ←0Xo  X 
O       X 
X O S X X 


Timestep: 89
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     →dø14
D   Xo  X 
O ↓0    X 
X O S X X 


Timestep: 90
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑dø15
D   Xo  X 
O ↓0    X 
X O S X X 


Timestep: 91
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑dø16
D   Xo  X 
O ↓0    X 
X O S X X 


Timestep: 92
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑dø17
D   Xo  X 
O ↓0    X 
X O S X X 


Timestep: 93
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑dø18
D   Xo  X 
O ↓o    X 
X O S X X 


Timestep: 94
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑dø19
D   Xo  X 
O   →o  X 
X O S X X 


Timestep: 95
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑dø20
D   Xo  X 
O ←o    X 
X O S X X 


Timestep: 96
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑dø20
D ↑oXo  X 
O       X 
X O S X X 


Timestep: 97
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑dø20
D ↑oXo  X 
O       X 
X O S X X 


Timestep: 98
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ↑o  ↑dø20
D   Xo  X 
O       X 
X O S X X 


Timestep: 99
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ↑o  ↑dø20
D   Xo  X 
O       X 
X O S X X 


tot rew 80 tot rew shaped 71
data/ppo_poor_runs/ppo_bc_train_random1/
SP envs: 30/30
Other agent actions took 0.5908961296081543 seconds
Total simulation time for 400 steps: 3.6224539279937744 	 Other agent action time: 0 	 110.42238437012564 steps/s
Curr learning rate 0.0009874968742185546 	 Curr reward per step 0.21237999999999999

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 201.77it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.13it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 203.83it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 202.99it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 204.64it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 204.79it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.93it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.60it/s]
--------------------------------------
| approxkl           | 0.0034862251  |
| clipfrac           | 0.45516664    |
| eplenmean          | 400           |
| eprewmean          | 80.5          |
| explained_variance | 0.0803        |
| fps                | 2830          |
| nupdates           | 51            |
| policy_entropy     | 1.5636787     |
| policy_loss        | -0.0060369903 |
| serial_timesteps   | 20400         |
| time_elapsed       | 224           |
| time_remaining     | 93.8          |
| total_timesteps    | 612000        |
| true_eprew         | 38.2          |
| value_loss         | 30.801758     |
--------------------------------------
Current reward shaping 0.8776
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6005966663360596 seconds
Total simulation time for 400 steps: 4.077354192733765 	 Other agent action time: 0 	 98.10283362501062 steps/s
Curr learning rate 0.0009872468117029258 	 Curr reward per step 0.22246366666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 199.42it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 199.89it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 196.96it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 201.40it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 203.62it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 201.96it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.06it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 202.48it/s]
-------------------------------------
| approxkl           | 0.003537766  |
| clipfrac           | 0.46625006   |
| eplenmean          | 400          |
| eprewmean          | 84           |
| explained_variance | 0.08         |
| fps                | 2550         |
| nupdates           | 52           |
| policy_entropy     | 1.5618328    |
| policy_loss        | -0.005904061 |
| serial_timesteps   | 20800        |
| time_elapsed       | 229          |
| time_remaining     | 93.9         |
| total_timesteps    | 624000       |
| true_eprew         | 41           |
| value_loss         | 32.329575    |
-------------------------------------
Current reward shaping 0.8752
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6047341823577881 seconds
Total simulation time for 400 steps: 3.6326510906219482 	 Other agent action time: 0 	 110.11241928316208 steps/s
Curr learning rate 0.0009869967491872968 	 Curr reward per step 0.2082844

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.63it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 206.95it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 206.05it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 205.37it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 206.40it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 205.80it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 205.65it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.92it/s]
-------------------------------------
| approxkl           | 0.0032904434 |
| clipfrac           | 0.4494376    |
| eplenmean          | 400          |
| eprewmean          | 85.8         |
| explained_variance | 0.0696       |
| fps                | 2827         |
| nupdates           | 53           |
| policy_entropy     | 1.5603541    |
| policy_loss        | -0.005623062 |
| serial_timesteps   | 21200        |
| time_elapsed       | 233          |
| time_remaining     | 93.8         |
| total_timesteps    | 636000       |
| true_eprew         | 42.4         |
| value_loss         | 27.823828    |
-------------------------------------
Current reward shaping 0.8728
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.603874683380127 seconds
Total simulation time for 400 steps: 3.633178472518921 	 Other agent action time: 0 	 110.0964356762457 steps/s
Curr learning rate 0.000986746686671668 	 Curr reward per step 0.2127908

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.78it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 201.21it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 199.96it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 200.60it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 202.16it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 201.41it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 203.70it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 207.68it/s]
--------------------------------------
| approxkl           | 0.0031229756  |
| clipfrac           | 0.43463543    |
| eplenmean          | 400           |
| eprewmean          | 85.4          |
| explained_variance | 0.119         |
| fps                | 2819          |
| nupdates           | 54            |
| policy_entropy     | 1.5561243     |
| policy_loss        | -0.0069242367 |
| serial_timesteps   | 21600         |
| time_elapsed       | 237           |
| time_remaining     | 93.6          |
| total_timesteps    | 648000        |
| true_eprew         | 42.4          |
| value_loss         | 27.90156      |
--------------------------------------
Current reward shaping 0.8704000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.599562406539917 seconds
Total simulation time for 400 steps: 3.659151792526245 	 Other agent action time: 0 	 109.31495130018742 steps/s
Curr learning rate 0.000986496624156039 	 Curr reward per step 0.19973493333333336

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 197.22it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 199.78it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 200.66it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 200.39it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 200.44it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 199.31it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 199.71it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 200.28it/s]
--------------------------------------
| approxkl           | 0.0031641154  |
| clipfrac           | 0.43678114    |
| eplenmean          | 400           |
| eprewmean          | 83.5          |
| explained_variance | 0.16          |
| fps                | 2798          |
| nupdates           | 55            |
| policy_entropy     | 1.5608366     |
| policy_loss        | -0.0068408023 |
| serial_timesteps   | 22000         |
| time_elapsed       | 241           |
| time_remaining     | 93.5          |
| total_timesteps    | 660000        |
| true_eprew         | 41.4          |
| value_loss         | 26.266888     |
--------------------------------------
Current reward shaping 0.868
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5995833873748779 seconds
Total simulation time for 400 steps: 3.666262149810791 	 Other agent action time: 0 	 109.10294563105458 steps/s
Curr learning rate 0.00098624656164041 	 Curr reward per step 0.21632400000000002

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 203.46it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.77it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 203.55it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 204.55it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 203.09it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 201.59it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 203.51it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 203.49it/s]
-------------------------------------
| approxkl           | 0.0031575602 |
| clipfrac           | 0.44865614   |
| eplenmean          | 400          |
| eprewmean          | 83.6         |
| explained_variance | 0.134        |
| fps                | 2800         |
| nupdates           | 56           |
| policy_entropy     | 1.5490419    |
| policy_loss        | -0.004876825 |
| serial_timesteps   | 22400        |
| time_elapsed       | 246          |
| time_remaining     | 93.4         |
| total_timesteps    | 672000       |
| true_eprew         | 41.2         |
| value_loss         | 32.05772     |
-------------------------------------
Current reward shaping 0.8656
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6016499996185303 seconds
Total simulation time for 400 steps: 3.652480125427246 	 Other agent action time: 0 	 109.5146273939575 steps/s
Curr learning rate 0.0009859964991247813 	 Curr reward per step 0.23845813333333335

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 203.41it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 207.79it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 208.20it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 208.29it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 207.13it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.34it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 203.06it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.67it/s]
--------------------------------------
| approxkl           | 0.0032446797  |
| clipfrac           | 0.4434166     |
| eplenmean          | 400           |
| eprewmean          | 87.8          |
| explained_variance | 0.115         |
| fps                | 2814          |
| nupdates           | 57            |
| policy_entropy     | 1.5408055     |
| policy_loss        | -0.0047731916 |
| serial_timesteps   | 22800         |
| time_elapsed       | 250           |
| time_remaining     | 93.3          |
| total_timesteps    | 684000        |
| true_eprew         | 44.2          |
| value_loss         | 34.09775      |
--------------------------------------
Current reward shaping 0.8632
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5996015071868896 seconds
Total simulation time for 400 steps: 3.6829617023468018 	 Other agent action time: 0 	 108.60824312811017 steps/s
Curr learning rate 0.0009857464366091523 	 Curr reward per step 0.2319282666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.65it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 205.66it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 207.34it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 208.25it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 206.67it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 206.01it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 208.70it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 206.69it/s]
--------------------------------------
| approxkl           | 0.0034238822  |
| clipfrac           | 0.4571875     |
| eplenmean          | 400           |
| eprewmean          | 90.8          |
| explained_variance | 0.113         |
| fps                | 2795          |
| nupdates           | 58            |
| policy_entropy     | 1.5307108     |
| policy_loss        | -0.0056056576 |
| serial_timesteps   | 23200         |
| time_elapsed       | 254           |
| time_remaining     | 93.2          |
| total_timesteps    | 696000        |
| true_eprew         | 46            |
| value_loss         | 32.772083     |
--------------------------------------
Current reward shaping 0.8608
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6099212169647217 seconds
Total simulation time for 400 steps: 3.7577176094055176 	 Other agent action time: 0 	 106.44759441177945 steps/s
Curr learning rate 0.0009854963740935233 	 Curr reward per step 0.24597599999999997

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 199.72it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 204.94it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 202.57it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 203.51it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 202.27it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 204.64it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 204.05it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 202.88it/s]
-------------------------------------
| approxkl           | 0.0029506867 |
| clipfrac           | 0.427125     |
| eplenmean          | 400          |
| eprewmean          | 94.7         |
| explained_variance | 0.164        |
| fps                | 2740         |
| nupdates           | 59           |
| policy_entropy     | 1.5254847    |
| policy_loss        | -0.005595802 |
| serial_timesteps   | 23600        |
| time_elapsed       | 259          |
| time_remaining     | 93.1         |
| total_timesteps    | 708000       |
| true_eprew         | 48.8         |
| value_loss         | 36.184982    |
-------------------------------------
Current reward shaping 0.8584
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5999441146850586 seconds
Total simulation time for 400 steps: 3.658367156982422 	 Other agent action time: 0 	 109.3383968409385 steps/s
Curr learning rate 0.0009852463115778945 	 Curr reward per step 0.25315166666666666

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.12it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.29it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 186.80it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 204.82it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 196.91it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 199.83it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 201.65it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 200.01it/s]
--------------------------------------
| approxkl           | 0.0038313954  |
| clipfrac           | 0.4733438     |
| eplenmean          | 400           |
| eprewmean          | 97.1          |
| explained_variance | 0.162         |
| fps                | 2791          |
| nupdates           | 60            |
| policy_entropy     | 1.5015045     |
| policy_loss        | -0.0039147846 |
| serial_timesteps   | 24000         |
| time_elapsed       | 263           |
| time_remaining     | 93            |
| total_timesteps    | 720000        |
| true_eprew         | 50.6          |
| value_loss         | 33.35161      |
--------------------------------------
Current reward shaping 0.856
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5990762710571289 seconds
Total simulation time for 400 steps: 3.627974033355713 	 Other agent action time: 0 	 110.25437236385564 steps/s
Curr learning rate 0.0009849962490622655 	 Curr reward per step 0.24982066666666672

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 197.03it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 199.13it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 202.48it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 205.94it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 206.14it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.94it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.77it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.41it/s]
-------------------------------------
| approxkl           | 0.0033615455 |
| clipfrac           | 0.44189584   |
| eplenmean          | 400          |
| eprewmean          | 99.4         |
| explained_variance | 0.17         |
| fps                | 2819         |
| nupdates           | 61           |
| policy_entropy     | 1.5005698    |
| policy_loss        | -0.003994    |
| serial_timesteps   | 24400        |
| time_elapsed       | 267          |
| time_remaining     | 92.9         |
| total_timesteps    | 732000       |
| true_eprew         | 52.2         |
| value_loss         | 33.274544    |
-------------------------------------
Current reward shaping 0.8536
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6044442653656006 seconds
Total simulation time for 400 steps: 3.6310391426086426 	 Other agent action time: 0 	 110.1613021204251 steps/s
Curr learning rate 0.0009847461865466367 	 Curr reward per step 0.25769506666666664

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 195.35it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 199.12it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 197.94it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 197.44it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 198.71it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 198.82it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 198.18it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 199.40it/s]
-------------------------------------
| approxkl           | 0.0036993918 |
| clipfrac           | 0.46037504   |
| eplenmean          | 400          |
| eprewmean          | 101          |
| explained_variance | 0.177        |
| fps                | 2813         |
| nupdates           | 62           |
| policy_entropy     | 1.4894477    |
| policy_loss        | -0.003452099 |
| serial_timesteps   | 24800        |
| time_elapsed       | 272          |
| time_remaining     | 92.8         |
| total_timesteps    | 744000       |
| true_eprew         | 53.6         |
| value_loss         | 35.704308    |
-------------------------------------
Current reward shaping 0.8512
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6020514965057373 seconds
Total simulation time for 400 steps: 3.6244821548461914 	 Other agent action time: 0 	 110.36059302021158 steps/s
Curr learning rate 0.0009844961240310078 	 Curr reward per step 0.26037439999999995

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 199.07it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 199.83it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 201.24it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 201.80it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 204.85it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 202.81it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.45it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 201.40it/s]
-------------------------------------
| approxkl           | 0.0034413221 |
| clipfrac           | 0.44264576   |
| eplenmean          | 400          |
| eprewmean          | 102          |
| explained_variance | 0.179        |
| fps                | 2825         |
| nupdates           | 63           |
| policy_entropy     | 1.4734752    |
| policy_loss        | -0.001957632 |
| serial_timesteps   | 25200        |
| time_elapsed       | 276          |
| time_remaining     | 92.7         |
| total_timesteps    | 756000       |
| true_eprew         | 54.4         |
| value_loss         | 33.4112      |
-------------------------------------
Current reward shaping 0.8488
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.606879711151123 seconds
Total simulation time for 400 steps: 3.6562821865081787 	 Other agent action time: 0 	 109.4007463307989 steps/s
Curr learning rate 0.000984246061515379 	 Curr reward per step 0.2615596666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.30it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.23it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.64it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 204.70it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 202.86it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 202.34it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 204.56it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 203.21it/s]
--------------------------------------
| approxkl           | 0.003981696   |
| clipfrac           | 0.4830626     |
| eplenmean          | 400           |
| eprewmean          | 103           |
| explained_variance | 0.229         |
| fps                | 2807          |
| nupdates           | 64            |
| policy_entropy     | 1.4663424     |
| policy_loss        | -0.0032232057 |
| serial_timesteps   | 25600         |
| time_elapsed       | 280           |
| time_remaining     | 92.6          |
| total_timesteps    | 768000        |
| true_eprew         | 54.8          |
| value_loss         | 32.713        |
--------------------------------------
Current reward shaping 0.8464
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6125671863555908 seconds
Total simulation time for 400 steps: 4.175156116485596 	 Other agent action time: 0 	 95.80480078831083 steps/s
Curr learning rate 0.00098399599899975 	 Curr reward per step 0.2623048

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 201.44it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 206.52it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 206.80it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 205.99it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 203.33it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.54it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 205.11it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 206.21it/s]
--------------------------------------
| approxkl           | 0.003484131   |
| clipfrac           | 0.44885403    |
| eplenmean          | 400           |
| eprewmean          | 104           |
| explained_variance | 0.24          |
| fps                | 2505          |
| nupdates           | 65            |
| policy_entropy     | 1.4665724     |
| policy_loss        | -0.0003993154 |
| serial_timesteps   | 26000         |
| time_elapsed       | 285           |
| time_remaining     | 92.6          |
| total_timesteps    | 780000        |
| true_eprew         | 55.8          |
| value_loss         | 34.2011       |
--------------------------------------
Current reward shaping 0.844
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6106412410736084 seconds
Total simulation time for 400 steps: 3.603759765625 	 Other agent action time: 0 	 110.99519002777589 steps/s
Curr learning rate 0.000983745936484121 	 Curr reward per step 0.2687866666666666

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 188.50it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 192.20it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 190.98it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 192.30it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 192.09it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 195.24it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 205.03it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 207.07it/s]
-------------------------------------
| approxkl           | 0.0034105664 |
| clipfrac           | 0.43841666   |
| eplenmean          | 400          |
| eprewmean          | 106          |
| explained_variance | 0.272        |
| fps                | 2826         |
| nupdates           | 66           |
| policy_entropy     | 1.4528565    |
| policy_loss        | -0.002261077 |
| serial_timesteps   | 26400        |
| time_elapsed       | 289          |
| time_remaining     | 92.5         |
| total_timesteps    | 792000       |
| true_eprew         | 56.6         |
| value_loss         | 34.727276    |
-------------------------------------
Current reward shaping 0.8416
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6020300388336182 seconds
Total simulation time for 400 steps: 3.5881810188293457 	 Other agent action time: 0 	 111.47709602747443 steps/s
Curr learning rate 0.0009834958739684922 	 Curr reward per step 0.2522464

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.82it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.90it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 205.04it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 204.06it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 205.34it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 205.17it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 204.23it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 203.38it/s]
--------------------------------------
| approxkl           | 0.0032492413  |
| clipfrac           | 0.42798966    |
| eplenmean          | 400           |
| eprewmean          | 104           |
| explained_variance | 0.295         |
| fps                | 2854          |
| nupdates           | 67            |
| policy_entropy     | 1.4652864     |
| policy_loss        | -0.0025813186 |
| serial_timesteps   | 26800         |
| time_elapsed       | 293           |
| time_remaining     | 92.4          |
| total_timesteps    | 804000        |
| true_eprew         | 56.2          |
| value_loss         | 29.942957     |
--------------------------------------
Current reward shaping 0.8392
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6002938747406006 seconds
Total simulation time for 400 steps: 3.640446901321411 	 Other agent action time: 0 	 109.87661977841452 steps/s
Curr learning rate 0.0009832458114528632 	 Curr reward per step 0.2809499333333334

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 196.83it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 201.41it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 202.06it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 198.36it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 203.10it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 204.15it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 205.57it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 206.64it/s]
--------------------------------------
| approxkl           | 0.005272389   |
| clipfrac           | 0.5042606     |
| eplenmean          | 400           |
| eprewmean          | 107           |
| explained_variance | 0.325         |
| fps                | 2815          |
| nupdates           | 68            |
| policy_entropy     | 1.4399045     |
| policy_loss        | -0.0009208593 |
| serial_timesteps   | 27200         |
| time_elapsed       | 298           |
| time_remaining     | 92.3          |
| total_timesteps    | 816000        |
| true_eprew         | 57.8          |
| value_loss         | 34.599712     |
--------------------------------------
Current reward shaping 0.8368
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6026642322540283 seconds
Total simulation time for 400 steps: 3.634439706802368 	 Other agent action time: 0 	 110.05822967742274 steps/s
Curr learning rate 0.0009829957489372342 	 Curr reward per step 0.2910289333333333

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 188.32it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 191.28it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 193.85it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 192.73it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 192.85it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 193.31it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 194.47it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 191.92it/s]
--------------------------------------
| approxkl           | 0.004471143   |
| clipfrac           | 0.46488544    |
| eplenmean          | 400           |
| eprewmean          | 110           |
| explained_variance | 0.349         |
| fps                | 2799          |
| nupdates           | 69            |
| policy_entropy     | 1.415208      |
| policy_loss        | 0.00048400342 |
| serial_timesteps   | 27600         |
| time_elapsed       | 302           |
| time_remaining     | 92.2          |
| total_timesteps    | 828000        |
| true_eprew         | 60.6          |
| value_loss         | 34.475964     |
--------------------------------------
Current reward shaping 0.8344
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6087143421173096 seconds
Total simulation time for 400 steps: 3.6331374645233154 	 Other agent action time: 0 	 110.09767835814102 steps/s
Curr learning rate 0.0009827456864216055 	 Curr reward per step 0.2766180666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 199.22it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 202.07it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 201.55it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 201.65it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 201.01it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 201.25it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 201.40it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 201.90it/s]
--------------------------------------
| approxkl           | 0.004590435   |
| clipfrac           | 0.46414578    |
| eplenmean          | 400           |
| eprewmean          | 112           |
| explained_variance | 0.351         |
| fps                | 2818          |
| nupdates           | 70            |
| policy_entropy     | 1.419468      |
| policy_loss        | 0.00015402751 |
| serial_timesteps   | 28000         |
| time_elapsed       | 306           |
| time_remaining     | 92.1          |
| total_timesteps    | 840000        |
| true_eprew         | 61.4          |
| value_loss         | 32.77955      |
--------------------------------------
Current reward shaping 0.832
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6061930656433105 seconds
Total simulation time for 400 steps: 3.6416609287261963 	 Other agent action time: 0 	 109.83999000146194 steps/s
Curr learning rate 0.0009824956239059765 	 Curr reward per step 0.30512533333333336

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 191.91it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 194.11it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 194.29it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 188.25it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 196.57it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 198.79it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 196.81it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 197.76it/s]
-------------------------------------
| approxkl           | 0.004921729  |
| clipfrac           | 0.47884375   |
| eplenmean          | 400          |
| eprewmean          | 115          |
| explained_variance | 0.373        |
| fps                | 2793         |
| nupdates           | 71           |
| policy_entropy     | 1.4021155    |
| policy_loss        | 0.0012878203 |
| serial_timesteps   | 28400        |
| time_elapsed       | 310          |
| time_remaining     | 92           |
| total_timesteps    | 852000       |
| true_eprew         | 63.2         |
| value_loss         | 36.126076    |
-------------------------------------
Current reward shaping 0.8296
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6115868091583252 seconds
Total simulation time for 400 steps: 3.639347553253174 	 Other agent action time: 0 	 109.90981052151622 steps/s
Curr learning rate 0.0009822455613903475 	 Curr reward per step 0.2810300666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 201.55it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.76it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 203.08it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 203.57it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 205.87it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 205.85it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 205.08it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 206.93it/s]
--------------------------------------
| approxkl           | 0.0037052024  |
| clipfrac           | 0.43614578    |
| eplenmean          | 400           |
| eprewmean          | 115           |
| explained_variance | 0.402         |
| fps                | 2821          |
| nupdates           | 72            |
| policy_entropy     | 1.4285474     |
| policy_loss        | -0.0019255647 |
| serial_timesteps   | 28800         |
| time_elapsed       | 315           |
| time_remaining     | 91.9          |
| total_timesteps    | 864000        |
| true_eprew         | 63            |
| value_loss         | 31.961212     |
--------------------------------------
Current reward shaping 0.8271999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6102714538574219 seconds
Total simulation time for 400 steps: 3.654825210571289 	 Other agent action time: 0 	 109.44435833566871 steps/s
Curr learning rate 0.0009819954988747187 	 Curr reward per step 0.26980613333333336

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 203.58it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 206.05it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 206.28it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 207.43it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 208.23it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 207.70it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 207.20it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 207.30it/s]
--------------------------------------
| approxkl           | 0.004254726   |
| clipfrac           | 0.45266667    |
| eplenmean          | 400           |
| eprewmean          | 115           |
| explained_variance | 0.378         |
| fps                | 2814          |
| nupdates           | 73            |
| policy_entropy     | 1.4058971     |
| policy_loss        | 0.00073178764 |
| serial_timesteps   | 29200         |
| time_elapsed       | 319           |
| time_remaining     | 91.7          |
| total_timesteps    | 876000        |
| true_eprew         | 63            |
| value_loss         | 29.993382     |
--------------------------------------
Current reward shaping 0.8248
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6046500205993652 seconds
Total simulation time for 400 steps: 3.619724988937378 	 Other agent action time: 0 	 110.5056326716759 steps/s
Curr learning rate 0.0009817454363590897 	 Curr reward per step 0.30810233333333337

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 193.71it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 195.91it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 196.36it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 195.33it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 196.48it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 195.28it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 195.18it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 197.07it/s]
-------------------------------------
| approxkl           | 0.0041189273 |
| clipfrac           | 0.455073     |
| eplenmean          | 400          |
| eprewmean          | 116          |
| explained_variance | 0.414        |
| fps                | 2816         |
| nupdates           | 74           |
| policy_entropy     | 1.3775486    |
| policy_loss        | 0.0015637191 |
| serial_timesteps   | 29600        |
| time_elapsed       | 323          |
| time_remaining     | 91.6         |
| total_timesteps    | 888000       |
| true_eprew         | 64           |
| value_loss         | 33.634624    |
-------------------------------------
Current reward shaping 0.8224
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.605879545211792 seconds
Total simulation time for 400 steps: 3.6078333854675293 	 Other agent action time: 0 	 110.86986489210202 steps/s
Curr learning rate 0.000981495373843461 	 Curr reward per step 0.3009234666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 201.24it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 202.32it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 199.95it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 200.78it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 201.60it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 201.59it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 204.45it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 203.76it/s]
-------------------------------------
| approxkl           | 0.004413051  |
| clipfrac           | 0.4620938    |
| eplenmean          | 400          |
| eprewmean          | 117          |
| explained_variance | 0.438        |
| fps                | 2837         |
| nupdates           | 75           |
| policy_entropy     | 1.3940346    |
| policy_loss        | 0.0006761042 |
| serial_timesteps   | 30000        |
| time_elapsed       | 327          |
| time_remaining     | 91.5         |
| total_timesteps    | 900000       |
| true_eprew         | 64.8         |
| value_loss         | 32.661438    |
-------------------------------------
Current reward shaping 0.8200000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6035268306732178 seconds
Total simulation time for 400 steps: 3.6197309494018555 	 Other agent action time: 0 	 110.50545070652232 steps/s
Curr learning rate 0.000981245311327832 	 Curr reward per step 0.27641666666666664

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 196.64it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 199.79it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 201.32it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 200.61it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 202.10it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.01it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.22it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 201.13it/s]
-------------------------------------
| approxkl           | 0.0043765176 |
| clipfrac           | 0.45122916   |
| eplenmean          | 400          |
| eprewmean          | 117          |
| explained_variance | 0.415        |
| fps                | 2826         |
| nupdates           | 76           |
| policy_entropy     | 1.3926964    |
| policy_loss        | 0.002437436  |
| serial_timesteps   | 30400        |
| time_elapsed       | 332          |
| time_remaining     | 91.4         |
| total_timesteps    | 912000       |
| true_eprew         | 64.8         |
| value_loss         | 29.041382    |
-------------------------------------
Current reward shaping 0.8176
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6135270595550537 seconds
Total simulation time for 400 steps: 3.6216769218444824 	 Other agent action time: 0 	 110.44607474161008 steps/s
Curr learning rate 0.0009809952488122032 	 Curr reward per step 0.29918599999999995

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 204.34it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 206.22it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 206.39it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 204.96it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 205.40it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 206.96it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.43it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.67it/s]
-------------------------------------
| approxkl           | 0.004345966  |
| clipfrac           | 0.4313437    |
| eplenmean          | 400          |
| eprewmean          | 117          |
| explained_variance | 0.412        |
| fps                | 2834         |
| nupdates           | 77           |
| policy_entropy     | 1.3578677    |
| policy_loss        | 0.0022492348 |
| serial_timesteps   | 30800        |
| time_elapsed       | 336          |
| time_remaining     | 91.3         |
| total_timesteps    | 924000       |
| true_eprew         | 65           |
| value_loss         | 33.085155    |
-------------------------------------
Current reward shaping 0.8152
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5986912250518799 seconds
Total simulation time for 400 steps: 3.6316192150115967 	 Other agent action time: 0 	 110.14370624171364 steps/s
Curr learning rate 0.0009807451862965742 	 Curr reward per step 0.28421379999999996

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 203.00it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 204.64it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.58it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 202.53it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 203.69it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 205.47it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 205.88it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 207.19it/s]
--------------------------------------
| approxkl           | 0.003326745   |
| clipfrac           | 0.41404164    |
| eplenmean          | 400           |
| eprewmean          | 116           |
| explained_variance | 0.467         |
| fps                | 2826          |
| nupdates           | 78            |
| policy_entropy     | 1.3720447     |
| policy_loss        | 0.00037582323 |
| serial_timesteps   | 31200         |
| time_elapsed       | 340           |
| time_remaining     | 91.2          |
| total_timesteps    | 936000        |
| true_eprew         | 64.2          |
| value_loss         | 26.35711      |
--------------------------------------
Current reward shaping 0.8128
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5987706184387207 seconds
Total simulation time for 400 steps: 3.61745023727417 	 Other agent action time: 0 	 110.57512163634046 steps/s
Curr learning rate 0.0009804951237809452 	 Curr reward per step 0.2941264

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 190.70it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 194.28it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 194.41it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 194.44it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 192.61it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 193.74it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 194.35it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 194.14it/s]
-------------------------------------
| approxkl           | 0.0039193113 |
| clipfrac           | 0.42383322   |
| eplenmean          | 400          |
| eprewmean          | 114          |
| explained_variance | 0.424        |
| fps                | 2813         |
| nupdates           | 79           |
| policy_entropy     | 1.3548653    |
| policy_loss        | 0.0018874264 |
| serial_timesteps   | 31600        |
| time_elapsed       | 344          |
| time_remaining     | 91.1         |
| total_timesteps    | 948000       |
| true_eprew         | 63.2         |
| value_loss         | 29.87558     |
-------------------------------------
Current reward shaping 0.8104
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6061594486236572 seconds
Total simulation time for 400 steps: 3.6137547492980957 	 Other agent action time: 0 	 110.68819766413105 steps/s
Curr learning rate 0.0009802450612653164 	 Curr reward per step 0.32114466666666663

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 196.46it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 198.58it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 198.51it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 198.36it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 198.40it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 199.68it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 199.73it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 198.10it/s]
-------------------------------------
| approxkl           | 0.0061268327 |
| clipfrac           | 0.4697708    |
| eplenmean          | 400          |
| eprewmean          | 121          |
| explained_variance | 0.334        |
| fps                | 2825         |
| nupdates           | 80           |
| policy_entropy     | 1.3099804    |
| policy_loss        | 0.004280388  |
| serial_timesteps   | 32000        |
| time_elapsed       | 349          |
| time_remaining     | 91           |
| total_timesteps    | 960000       |
| true_eprew         | 67.2         |
| value_loss         | 37.392838    |
-------------------------------------
Current reward shaping 0.808
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6117842197418213 seconds
Total simulation time for 400 steps: 4.220645189285278 	 Other agent action time: 0 	 94.77224027631088 steps/s
Curr learning rate 0.0009799949987496874 	 Curr reward per step 0.3058113333333333

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 201.03it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 204.54it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 203.91it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 203.81it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 203.67it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.30it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 204.61it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 205.19it/s]
-------------------------------------
| approxkl           | 0.003816961  |
| clipfrac           | 0.39949998   |
| eplenmean          | 400          |
| eprewmean          | 122          |
| explained_variance | 0.472        |
| fps                | 2480         |
| nupdates           | 81           |
| policy_entropy     | 1.3510194    |
| policy_loss        | 0.0012630472 |
| serial_timesteps   | 32400        |
| time_elapsed       | 354          |
| time_remaining     | 91.1         |
| total_timesteps    | 972000       |
| true_eprew         | 68           |
| value_loss         | 30.66189     |
-------------------------------------
Current reward shaping 0.8056
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6172726154327393 seconds
Total simulation time for 400 steps: 3.667418956756592 	 Other agent action time: 0 	 109.06853149762681 steps/s
Curr learning rate 0.0009797449362340584 	 Curr reward per step 0.3352882

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 183.22it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 185.80it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 184.29it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 195.89it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 192.72it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 197.21it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 197.77it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 177.61it/s]
-------------------------------------
| approxkl           | 0.004673555  |
| clipfrac           | 0.45367715   |
| eplenmean          | 400          |
| eprewmean          | 126          |
| explained_variance | 0.407        |
| fps                | 2771         |
| nupdates           | 82           |
| policy_entropy     | 1.3096408    |
| policy_loss        | 0.0036235973 |
| serial_timesteps   | 32800        |
| time_elapsed       | 358          |
| time_remaining     | 91           |
| total_timesteps    | 984000       |
| true_eprew         | 70.6         |
| value_loss         | 35.52947     |
-------------------------------------
Current reward shaping 0.8032
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6061010360717773 seconds
Total simulation time for 400 steps: 3.6214194297790527 	 Other agent action time: 0 	 110.45392773639713 steps/s
Curr learning rate 0.0009794948737184297 	 Curr reward per step 0.31882639999999995

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 201.96it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.89it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 203.69it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 203.97it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 204.32it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.44it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 204.36it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.78it/s]
-------------------------------------
| approxkl           | 0.004508492  |
| clipfrac           | 0.4648125    |
| eplenmean          | 400          |
| eprewmean          | 128          |
| explained_variance | 0.504        |
| fps                | 2831         |
| nupdates           | 83           |
| policy_entropy     | 1.3275746    |
| policy_loss        | 0.0028075294 |
| serial_timesteps   | 33200        |
| time_elapsed       | 362          |
| time_remaining     | 90.9         |
| total_timesteps    | 996000       |
| true_eprew         | 71.6         |
| value_loss         | 32.229317    |
-------------------------------------
Current reward shaping 0.8008
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6106312274932861 seconds
Total simulation time for 400 steps: 3.607923746109009 	 Other agent action time: 0 	 110.86708815045851 steps/s
Curr learning rate 0.0009792448112028007 	 Curr reward per step 0.3565547333333333

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.53it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 202.90it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.88it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 202.83it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 203.99it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 205.49it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 203.97it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 203.20it/s]
-------------------------------------
| approxkl           | 0.0043313554 |
| clipfrac           | 0.44013548   |
| eplenmean          | 400          |
| eprewmean          | 134          |
| explained_variance | 0.413        |
| fps                | 2839         |
| nupdates           | 84           |
| policy_entropy     | 1.2852184    |
| policy_loss        | 0.003410041  |
| serial_timesteps   | 33600        |
| time_elapsed       | 366          |
| time_remaining     | 90.8         |
| total_timesteps    | 1008000      |
| true_eprew         | 75.4         |
| value_loss         | 39.987923    |
-------------------------------------
Current reward shaping 0.7984
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6117210388183594 seconds
Total simulation time for 400 steps: 3.6108145713806152 	 Other agent action time: 0 	 110.7783277408947 steps/s
Curr learning rate 0.000978994748687172 	 Curr reward per step 0.33903466666666665

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 199.77it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 201.90it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 200.90it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 199.60it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 200.54it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 200.67it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 201.31it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 201.74it/s]
------------------------------------
| approxkl           | 0.004261277 |
| clipfrac           | 0.4425416   |
| eplenmean          | 400         |
| eprewmean          | 136         |
| explained_variance | 0.437       |
| fps                | 2831        |
| nupdates           | 85          |
| policy_entropy     | 1.2944386   |
| policy_loss        | 0.002406325 |
| serial_timesteps   | 34000       |
| time_elapsed       | 371         |
| time_remaining     | 90.7        |
| total_timesteps    | 1020000     |
| true_eprew         | 76.4        |
| value_loss         | 34.53474    |
------------------------------------
Current reward shaping 0.796
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6091482639312744 seconds
Total simulation time for 400 steps: 3.612929582595825 	 Other agent action time: 0 	 110.71347803922798 steps/s
Curr learning rate 0.000978744686171543 	 Curr reward per step 0.33558499999999997

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 197.28it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 199.56it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 198.50it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 199.96it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 200.79it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 198.49it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 200.19it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 202.40it/s]
-------------------------------------
| approxkl           | 0.0042831176 |
| clipfrac           | 0.42270824   |
| eplenmean          | 400          |
| eprewmean          | 136          |
| explained_variance | 0.397        |
| fps                | 2829         |
| nupdates           | 86           |
| policy_entropy     | 1.2785457    |
| policy_loss        | 0.003262949  |
| serial_timesteps   | 34400        |
| time_elapsed       | 375          |
| time_remaining     | 90.6         |
| total_timesteps    | 1032000      |
| true_eprew         | 76.4         |
| value_loss         | 36.488426    |
-------------------------------------
Current reward shaping 0.7936
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6035685539245605 seconds
Total simulation time for 400 steps: 3.6290481090545654 	 Other agent action time: 0 	 110.22174079257589 steps/s
Curr learning rate 0.0009784946236559141 	 Curr reward per step 0.32798026666666674

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.70it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 202.66it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 202.41it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 204.43it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 205.39it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.27it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.93it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.34it/s]
-------------------------------------
| approxkl           | 0.0034201392 |
| clipfrac           | 0.38873962   |
| eplenmean          | 400          |
| eprewmean          | 136          |
| explained_variance | 0.491        |
| fps                | 2824         |
| nupdates           | 87           |
| policy_entropy     | 1.275129     |
| policy_loss        | 0.0012697135 |
| serial_timesteps   | 34800        |
| time_elapsed       | 379          |
| time_remaining     | 90.5         |
| total_timesteps    | 1044000      |
| true_eprew         | 76.2         |
| value_loss         | 33.209145    |
-------------------------------------
Current reward shaping 0.7912
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.612480878829956 seconds
Total simulation time for 400 steps: 3.6198348999023438 	 Other agent action time: 0 	 110.5022773306018 steps/s
Curr learning rate 0.0009782445611402851 	 Curr reward per step 0.36995426666666664

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 196.60it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 197.52it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 197.83it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 198.87it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 199.73it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 198.82it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 200.11it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 198.79it/s]
-------------------------------------
| approxkl           | 0.0037645134 |
| clipfrac           | 0.41565618   |
| eplenmean          | 400          |
| eprewmean          | 137          |
| explained_variance | 0.47         |
| fps                | 2821         |
| nupdates           | 88           |
| policy_entropy     | 1.2711974    |
| policy_loss        | 0.0017192047 |
| serial_timesteps   | 35200        |
| time_elapsed       | 383          |
| time_remaining     | 90.4         |
| total_timesteps    | 1056000      |
| true_eprew         | 77.2         |
| value_loss         | 38.321026    |
-------------------------------------
Current reward shaping 0.7888
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6168367862701416 seconds
Total simulation time for 400 steps: 3.620793104171753 	 Other agent action time: 0 	 110.47303408171369 steps/s
Curr learning rate 0.0009779944986246561 	 Curr reward per step 0.3584216

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 196.31it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 198.98it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 199.96it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 201.98it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 201.11it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 200.33it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 201.08it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 202.68it/s]
-------------------------------------
| approxkl           | 0.003802877  |
| clipfrac           | 0.42437497   |
| eplenmean          | 400          |
| eprewmean          | 142          |
| explained_variance | 0.498        |
| fps                | 2824         |
| nupdates           | 89           |
| policy_entropy     | 1.274427     |
| policy_loss        | 0.0013709957 |
| serial_timesteps   | 35600        |
| time_elapsed       | 388          |
| time_remaining     | 90.3         |
| total_timesteps    | 1068000      |
| true_eprew         | 80.4         |
| value_loss         | 38.33237     |
-------------------------------------
Current reward shaping 0.7864
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6015944480895996 seconds
Total simulation time for 400 steps: 3.618476390838623 	 Other agent action time: 0 	 110.5437639479238 steps/s
Curr learning rate 0.0009777444361090274 	 Curr reward per step 0.39607033333333325

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 201.16it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 199.73it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 202.41it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 203.38it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 204.37it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 204.45it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 203.45it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.79it/s]
-------------------------------------
| approxkl           | 0.004672073  |
| clipfrac           | 0.42538548   |
| eplenmean          | 400          |
| eprewmean          | 147          |
| explained_variance | 0.442        |
| fps                | 2832         |
| nupdates           | 90           |
| policy_entropy     | 1.2321805    |
| policy_loss        | 0.0041944208 |
| serial_timesteps   | 36000        |
| time_elapsed       | 392          |
| time_remaining     | 90.2         |
| total_timesteps    | 1080000      |
| true_eprew         | 84           |
| value_loss         | 43.75913     |
-------------------------------------
Current reward shaping 0.784
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.608330249786377 seconds
Total simulation time for 400 steps: 3.6214897632598877 	 Other agent action time: 0 	 110.45178259455844 steps/s
Curr learning rate 0.0009774943735933984 	 Curr reward per step 0.3899333333333333

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 189.31it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 185.61it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 186.71it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 188.50it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 187.01it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 191.19it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 190.83it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 188.98it/s]
-------------------------------------
| approxkl           | 0.0031613472 |
| clipfrac           | 0.38692707   |
| eplenmean          | 400          |
| eprewmean          | 153          |
| explained_variance | 0.484        |
| fps                | 2798         |
| nupdates           | 91           |
| policy_entropy     | 1.2590789    |
| policy_loss        | 0.0013451898 |
| serial_timesteps   | 36400        |
| time_elapsed       | 396          |
| time_remaining     | 90.1         |
| total_timesteps    | 1092000      |
| true_eprew         | 87.8         |
| value_loss         | 41.181484    |
-------------------------------------
Current reward shaping 0.7816
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6046080589294434 seconds
Total simulation time for 400 steps: 3.6190385818481445 	 Other agent action time: 0 	 110.52659178773686 steps/s
Curr learning rate 0.0009772443110777694 	 Curr reward per step 0.38763793333333324

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.07it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.74it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.79it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 204.48it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 202.71it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 202.86it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.46it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 203.00it/s]
-------------------------------------
| approxkl           | 0.0032909473 |
| clipfrac           | 0.3758854    |
| eplenmean          | 400          |
| eprewmean          | 155          |
| explained_variance | 0.42         |
| fps                | 2832         |
| nupdates           | 92           |
| policy_entropy     | 1.2418526    |
| policy_loss        | 0.0010094274 |
| serial_timesteps   | 36800        |
| time_elapsed       | 400          |
| time_remaining     | 90           |
| total_timesteps    | 1104000      |
| true_eprew         | 89           |
| value_loss         | 41.063793    |
-------------------------------------
Current reward shaping 0.7792
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5981879234313965 seconds
Total simulation time for 400 steps: 3.588646173477173 	 Other agent action time: 0 	 111.46264654239376 steps/s
Curr learning rate 0.0009769942485621406 	 Curr reward per step 0.4002552

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.20it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 202.74it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 200.84it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 201.69it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 200.14it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 204.19it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 204.55it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 198.86it/s]
-------------------------------------
| approxkl           | 0.0041837287 |
| clipfrac           | 0.41955212   |
| eplenmean          | 400          |
| eprewmean          | 158          |
| explained_variance | 0.501        |
| fps                | 2848         |
| nupdates           | 93           |
| policy_entropy     | 1.2344309    |
| policy_loss        | 0.0018600858 |
| serial_timesteps   | 37200        |
| time_elapsed       | 405          |
| time_remaining     | 89.9         |
| total_timesteps    | 1116000      |
| true_eprew         | 91.2         |
| value_loss         | 39.453175    |
-------------------------------------
Current reward shaping 0.7767999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6026866436004639 seconds
Total simulation time for 400 steps: 3.6063318252563477 	 Other agent action time: 0 	 110.91602752654823 steps/s
Curr learning rate 0.0009767441860465116 	 Curr reward per step 0.4069964666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 203.31it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 204.89it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 203.43it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 203.47it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 202.89it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 202.94it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 204.94it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 206.00it/s]
-------------------------------------
| approxkl           | 0.0028928735 |
| clipfrac           | 0.36004165   |
| eplenmean          | 400          |
| eprewmean          | 159          |
| explained_variance | 0.468        |
| fps                | 2842         |
| nupdates           | 94           |
| policy_entropy     | 1.2081286    |
| policy_loss        | 0.0021620835 |
| serial_timesteps   | 37600        |
| time_elapsed       | 409          |
| time_remaining     | 89.8         |
| total_timesteps    | 1128000      |
| true_eprew         | 91.6         |
| value_loss         | 42.711662    |
-------------------------------------
Current reward shaping 0.7744
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6122879981994629 seconds
Total simulation time for 400 steps: 3.601936101913452 	 Other agent action time: 0 	 111.05138699920536 steps/s
Curr learning rate 0.0009764941235308827 	 Curr reward per step 0.44792106666666665

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 197.57it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 201.80it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 202.67it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 201.76it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 201.55it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 200.06it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 197.11it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 199.14it/s]
-------------------------------------
| approxkl           | 0.004128049  |
| clipfrac           | 0.41464588   |
| eplenmean          | 400          |
| eprewmean          | 167          |
| explained_variance | 0.409        |
| fps                | 2836         |
| nupdates           | 95           |
| policy_entropy     | 1.1762267    |
| policy_loss        | 0.0023130344 |
| serial_timesteps   | 38000        |
| time_elapsed       | 413          |
| time_remaining     | 89.7         |
| total_timesteps    | 1140000      |
| true_eprew         | 96.6         |
| value_loss         | 48.766006    |
-------------------------------------
Current reward shaping 0.772
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6055700778961182 seconds
Total simulation time for 400 steps: 3.6352927684783936 	 Other agent action time: 0 	 110.03240329593207 steps/s
Curr learning rate 0.0009762440610152539 	 Curr reward per step 0.4642866666666668

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 198.36it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 200.70it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 200.17it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 199.62it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 197.56it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 198.31it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 200.02it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 200.47it/s]
-------------------------------------
| approxkl           | 0.004460795  |
| clipfrac           | 0.4033228    |
| eplenmean          | 400          |
| eprewmean          | 174          |
| explained_variance | 0.362        |
| fps                | 2813         |
| nupdates           | 96           |
| policy_entropy     | 1.150745     |
| policy_loss        | 0.0040370924 |
| serial_timesteps   | 38400        |
| time_elapsed       | 417          |
| time_remaining     | 89.6         |
| total_timesteps    | 1152000      |
| true_eprew         | 101          |
| value_loss         | 52.605453    |
-------------------------------------
Current reward shaping 0.7696000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.602168083190918 seconds
Total simulation time for 400 steps: 3.591329574584961 	 Other agent action time: 0 	 111.37936290523456 steps/s
Curr learning rate 0.0009759939984996249 	 Curr reward per step 0.429316

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 203.21it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 204.67it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 203.72it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 205.25it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 202.10it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 200.23it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 199.36it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 202.89it/s]
-------------------------------------
| approxkl           | 0.0032466229 |
| clipfrac           | 0.37860423   |
| eplenmean          | 400          |
| eprewmean          | 177          |
| explained_variance | 0.43         |
| fps                | 2848         |
| nupdates           | 97           |
| policy_entropy     | 1.1761504    |
| policy_loss        | 0.0016439222 |
| serial_timesteps   | 38800        |
| time_elapsed       | 421          |
| time_remaining     | 89.5         |
| total_timesteps    | 1164000      |
| true_eprew         | 103          |
| value_loss         | 44.069893    |
-------------------------------------
Current reward shaping 0.7672
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6035254001617432 seconds
Total simulation time for 400 steps: 3.6121273040771484 	 Other agent action time: 0 	 110.73806827032493 steps/s
Curr learning rate 0.0009757439359839961 	 Curr reward per step 0.44533833333333334

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.53it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 202.52it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 202.08it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 202.73it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 200.09it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 201.83it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 203.64it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 203.45it/s]
-------------------------------------
| approxkl           | 0.002927827  |
| clipfrac           | 0.35663545   |
| eplenmean          | 400          |
| eprewmean          | 178          |
| explained_variance | 0.412        |
| fps                | 2834         |
| nupdates           | 98           |
| policy_entropy     | 1.1657052    |
| policy_loss        | 0.0011061274 |
| serial_timesteps   | 39200        |
| time_elapsed       | 426          |
| time_remaining     | 89.4         |
| total_timesteps    | 1176000      |
| true_eprew         | 104          |
| value_loss         | 48.36344     |
-------------------------------------
Current reward shaping 0.7648
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6077115535736084 seconds
Total simulation time for 400 steps: 3.622851610183716 	 Other agent action time: 0 	 110.41026325108466 steps/s
Curr learning rate 0.0009754938734683671 	 Curr reward per step 0.42213413333333344

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 193.11it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 195.38it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 196.29it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 199.96it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 196.33it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 200.09it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 201.44it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 202.83it/s]
------------------------------------
| approxkl           | 0.004245827 |
| clipfrac           | 0.40724996  |
| eplenmean          | 400         |
| eprewmean          | 174         |
| explained_variance | 0.47        |
| fps                | 2817        |
| nupdates           | 99          |
| policy_entropy     | 1.1584313   |
| policy_loss        | 0.001886996 |
| serial_timesteps   | 39600       |
| time_elapsed       | 430         |
| time_remaining     | 89.3        |
| total_timesteps    | 1188000     |
| true_eprew         | 102         |
| value_loss         | 40.433712   |
------------------------------------
Current reward shaping 0.7624
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6132063865661621 seconds
Total simulation time for 400 steps: 4.348670721054077 	 Other agent action time: 0 	 91.98213101382937 steps/s
Curr learning rate 0.0009752438109527382 	 Curr reward per step 0.4736835333333335

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.42it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 202.93it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 203.81it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 205.27it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 204.65it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 205.60it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 203.32it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 200.93it/s]
-------------------------------------
| approxkl           | 0.0032591103 |
| clipfrac           | 0.37631252   |
| eplenmean          | 400          |
| eprewmean          | 178          |
| explained_variance | 0.392        |
| fps                | 2416         |
| nupdates           | 100          |
| policy_entropy     | 1.1626532    |
| policy_loss        | 0.0022796656 |
| serial_timesteps   | 40000        |
| time_elapsed       | 435          |
| time_remaining     | 89.4         |
| total_timesteps    | 1200000      |
| true_eprew         | 104          |
| value_loss         | 53.156998    |
-------------------------------------
Current reward shaping 0.76
Current self-play randomization 1
data/ppo_poor_runs/ppo_bc_train_random1/
PPO agent on index 0:
X X X P X 
X   ↑0  P 
D ↑1X   X 
O       X 
X O S X X 

Timestep: 1
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ←0    P 
D ↑1X   X 
O       X 
X O S X X 


Timestep: 2
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ↓0    P 
D ↑1X   X 
O       X 
X O S X X 


Timestep: 3
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ↓0    P 
D ↑1X   X 
O       X 
X O S X X 


Timestep: 4
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ↓0    P 
D ↑1X   X 
O       X 
X O S X X 


Timestep: 5
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ↓0X   X 
O ↓1    X 
X O S X X 


Timestep: 6
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ↓0X   X 
O ↓1    X 
X O S X X 


Timestep: 7
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X ↑0    P 
D   X   X 
O ↓o    X 
X O S X X 


Timestep: 8
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ↑0    P 
D   X   X 
O ↓o    X 
X O S X X 


Timestep: 9
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X   →0  P 
D   X   X 
O   →o  X 
X O S X X 


Timestep: 10
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X     →0P 
D   X   X 
O   →o  X 
X O S X X 


Timestep: 11
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X   ←0  P 
D   X   X 
O   →o  X 
X O S X X 


Timestep: 12
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ←0    P 
D   X   X 
O   →o  X 
X O S X X 


Timestep: 13
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X ←0    P 
D   X   X 
O     →oX 
X O S X X 


Timestep: 14
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X ↑0    P 
D   X   X 
O     →oX 
X O S X X 


Timestep: 15
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X   →0  P 
D   X   X 
O     →oX 
X O S X X 


Timestep: 16
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ←0    P 
D   X   X 
O     →oX 
X O S X X 


Timestep: 17
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X X P X 
X ↑0    P 
D   X ↑oX 
O       X 
X O S X X 


Timestep: 18
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ←0    P 
D   X ↑oX 
O       X 
X O S X X 


Timestep: 19
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X X P X 
X ←0  ↑oP 
D   X   X 
O       X 
X O S X X 


Timestep: 20
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X X P X 
X     ↑oP 
D ↓0X   X 
O       X 
X O S X X 


Timestep: 21
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X X ø-X 
X     ↑1P 
D ↓0X   X 
O       X 
X O S X X 


Timestep: 22
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       P 
D   X ↓1X 
O ↓0    X 
X O S X X 


Timestep: 23
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       P 
D   X   X 
O ↓o  ↓1X 
X O S X X 


Timestep: 24
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       P 
D ↑oX   X 
O     →1X 
X O S X X 


Timestep: 25
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ↑o    P 
D   X   X 
O     →1X 
X O S X X 


Timestep: 26
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X ↑0    P 
D   X   X 
O     ↓1X 
X O S X X 


Timestep: 27
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X       P 
D ↓0X   X 
O     ↓1X 
X O S X X 


Timestep: 28
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X       P 
D →0X   X 
O     ↓1X 
X O S X X 


Timestep: 29
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X       P 
D   X   X 
O ↓0  ↓1X 
X O S X X 


Timestep: 30
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X       P 
D   X   X 
O ↓0←1  X 
X O S X X 


Timestep: 31
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X       P 
D   X   X 
O ↓o←1  X 
X O S X X 


Timestep: 32
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X       P 
D ↑oX   X 
O   ←1  X 
X O S X X 


Timestep: 33
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X ↑o    P 
D   X   X 
O   ←1  X 
X O S X X 


Timestep: 34
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X   →o  P 
D   X   X 
O ←1    X 
X O S X X 


Timestep: 35
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X   ↓o  P 
D   X   X 
O ←1    X 
X O S X X 


Timestep: 36
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X     →oP 
D   X   X 
O ←1    X 
X O S X X 


Timestep: 37
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X     →oP 
D   X   X 
O ←1    X 
X O S X X 


Timestep: 38
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X     →oP 
D ↑1X   X 
O       X 
X O S X X 


Timestep: 39
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 3 
X XoX ø-X 
X ↑1  →0ø-
D   X   X 
O       X 
X O S X X 


Timestep: 40
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X ↑1  →0ø-
D   X   X 
O       X 
X O S X X 


Timestep: 41
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X ↑1    ø-
D   X ↓0X 
O       X 
X O S X X 


Timestep: 42
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X ↑1    ø-
D   X   X 
O     ↓0X 
X O S X X 


Timestep: 43
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X       ø-
D ↓1X   X 
O     ↓0X 
X O S X X 


Timestep: 44
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X       ø-
D   X   X 
O ↓1  →0X 
X O S X X 


Timestep: 45
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X       ø-
D   X ↑0X 
O ↓1    X 
X O S X X 


Timestep: 46
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X       ø-
D   X ↑0X 
O ↓1    X 
X O S X X 


Timestep: 47
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X       ø-
D   X   X 
O ↓1  ↓0X 
X O S X X 


Timestep: 48
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X       ø-
D   X   X 
O ↓o←0  X 
X O S X X 


Timestep: 49
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X       ø-
D ↑oX   X 
O   ↓0  X 
X O S X X 


Timestep: 50
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X       ø-
D ↑oX   X 
O ←0    X 
X O S X X 


Timestep: 51
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X ↑o    ø-
D   X   X 
O ←o    X 
X O S X X 


Timestep: 52
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X ↑o    ø-
D   X   X 
O   →o  X 
X O S X X 


Timestep: 53
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X ↑o    ø-
D   X   X 
O     →oX 
X O S X X 


Timestep: 54
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X ↑o    ø-
D   X ↑oX 
O       X 
X O S X X 


Timestep: 55
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X   →o  ø-
D   X ↑oX 
O       X 
X O S X X 


Timestep: 56
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X     →oø-
D   X   X 
O     ↓oX 
X O S X X 


Timestep: 57
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X     →oø-
D   X   X 
O   ←o  X 
X O S X X 


Timestep: 58
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X     →oø-
D   X   X 
O   ←o  X 
X O S X X 


Timestep: 59
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X     →oø-
D   X   X 
O   ↓o  X 
X O S X X 


Timestep: 60
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X     →oø-
D   X   X 
O   ↓o  X 
X O S X X 


Timestep: 61
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X     ↑oø-
D   X   X 
O   ↓o  X 
X O S X X 


Timestep: 62
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X XoX ø-X 
X     ↑oø-
D   X   X 
O     →oX 
X O S X X 


Timestep: 63
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 3 
X XoX ø=X 
X     ↑1ø-
D   X ↑oX 
O       X 
X O S X X 


Timestep: 64
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø=X 
X     ↑1ø-
D   X ↑oX 
O       X 
X O S X X 


Timestep: 65
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X XoX ø=X 
X   ←1↑oø-
D   X   X 
O       X 
X O S X X 


Timestep: 66
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X XoX ø1X 
X   ←1↑0ø-
D   X   X 
O       X 
X O S X X 


Timestep: 67
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø2X 
X   ←1  ø-
D   X ↓0X 
O       X 
X O S X X 


Timestep: 68
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X XoX ø3X 
X ←1    ø-
D   X →0X 
O       X 
X O S X X 


Timestep: 69
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X XoX ø4X 
X ←1    ø-
D   X   X 
O     ↓0X 
X O S X X 


Timestep: 70
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø5X 
X ←1    ø-
D   X   X 
O     ↓0X 
X O S X X 


Timestep: 71
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø6X 
X ←1    ø-
D   X   X 
O     ↓0X 
X O S X X 


Timestep: 72
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X XoX ø7X 
X       ø-
D ↓1X   X 
O     ↓0X 
X O S X X 


Timestep: 73
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X XoX ø8X 
X ↑1    ø-
D   X   X 
O     ↓0X 
X O S X X 


Timestep: 74
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø9X 
X ↑1    ø-
D   X   X 
O     →0X 
X O S X X 


Timestep: 75
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø10X 
X ↑1    ø-
D   X   X 
O     ↓0X 
X O S X X 


Timestep: 76
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø11X 
X ↑1    ø-
D   X   X 
O     ↓0X 
X O S X X 


Timestep: 77
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X XoX ø12X 
X ↑1    ø-
D   X   X 
O     →0X 
X O S X X 


Timestep: 78
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X XoX ø13X 
X ←1    ø-
D   X   X 
O     →0X 
X O S X X 


Timestep: 79
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X XoX ø14X 
X ←1    ø-
D   X   X 
O     ↓0X 
X O S X X 


Timestep: 80
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X XoX ø15X 
X ←1    ø-
D   X   X 
O     →0X 
X O S X X 


Timestep: 81
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø16X 
X ←1    ø-
D   X   X 
O     →0X 
X O S X X 


Timestep: 82
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X XoX ø17X 
X       ø-
D ↓1X   X 
O     ↓0X 
X O S X X 


Timestep: 83
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X XoX ø18X 
X       ø-
D ←1X   X 
O     ↓0X 
X O S X X 


Timestep: 84
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø19X 
X       ø-
D ←1X   X 
O     ↓0X 
X O S X X 


Timestep: 85
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 3 
X XoX ø20X 
X       ø-
D ←dX   X 
O     ↓0X 
X O S X X 


Timestep: 86
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X XoX ø20X 
X ↑d    ø-
D   X   X 
O     →0X 
X O S X X 


Timestep: 87
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø20X 
X ↑d    ø-
D   X   X 
O     →0X 
X O S X X 


Timestep: 88
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø20X 
X ↑d    ø-
D   X   X 
O     →0X 
X O S X X 


Timestep: 89
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø20X 
X ↑d    ø-
D   X   X 
O     →0X 
X O S X X 


Timestep: 90
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X XoX ø20X 
X ↑d    ø-
D   X   X 
O     →0X 
X O S X X 


Timestep: 91
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X XoX ø20X 
X   →d  ø-
D   X   X 
O     →0X 
X O S X X 


Timestep: 92
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø20X 
X   →d  ø-
D   X   X 
O     →0X 
X O S X X 


Timestep: 93
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø20X 
X   →d  ø-
D   X   X 
O     ↓0X 
X O S X X 


Timestep: 94
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X XoX ø20X 
X   →d  ø-
D   X   X 
O     ↓0X 
X O S X X 


Timestep: 95
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X XoX ø20X 
X     →dø-
D   X   X 
O     ↓0X 
X O S X X 


Timestep: 96
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X XoX ø20X 
X     →dø-
D   X   X 
O     ↓0X 
X O S X X 


Timestep: 97
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X XoX ø20X 
X     →dø-
D   X   X 
O     ↓0X 
X O S X X 


Timestep: 98
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X XoX ø20X 
X     →dø-
D   X   X 
O     ↓0X 
X O S X X 


Timestep: 99
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X XoX ø20X 
X     ↑dø-
D   X   X 
O     ↓0X 
X O S X X 


tot rew 60 tot rew shaped 65
PPO agent on index 1:
X X X P X 
X   ↑0  P 
D ↑1X   X 
O       X 
X O S X X 

Timestep: 1
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X   ↑0  P 
D ←1X   X 
O       X 
X O S X X 


Timestep: 2
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X   ↑0  P 
D   X   X 
O ↓1    X 
X O S X X 


Timestep: 3
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X   ↑0  P 
D   X   X 
O ↓o    X 
X O S X X 


Timestep: 4
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X   ↑0  P 
D   X   X 
O ↓o    X 
X O S X X 


Timestep: 5
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X     →0P 
D   X   X 
O   →o  X 
X O S X X 


Timestep: 6
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X   ←0  P 
D   X   X 
O   ↓o  X 
X O S X X 


Timestep: 7
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X ←0    P 
D   X   X 
O   ↓o  X 
X O S X X 


Timestep: 8
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X ←0    P 
D   X   X 
O     →oX 
X O S X X 


Timestep: 9
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X X P X 
X       P 
D ↓0X ↑oX 
O       X 
X O S X X 


Timestep: 10
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X X P X 
X     ↑oP 
D   X   X 
O ↓0    X 
X O S X X 


Timestep: 11
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X     →oP 
D   X   X 
O ↓0    X 
X O S X X 


Timestep: 12
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X X P X 
X     →1ø-
D   X   X 
O ↓0    X 
X O S X X 


Timestep: 13
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X   ←1  ø-
D   X   X 
O ↓0    X 
X O S X X 


Timestep: 14
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X   ↓1  ø-
D   X   X 
O ↓0    X 
X O S X X 


Timestep: 15
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X   ↓1  ø-
D   X   X 
O ↓o    X 
X O S X X 


Timestep: 16
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X   ↓1  ø-
D   X   X 
O   →o  X 
X O S X X 


Timestep: 17
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X   ↓1  ø-
D   X   X 
O     →oX 
X O S X X 


Timestep: 18
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X X P X 
X   ↑1  ø-
D   X   X 
O     →oX 
X O S X X 


Timestep: 19
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X ←1    ø-
D   X   X 
O     →oX 
X O S X X 


Timestep: 20
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X ←1    ø-
D   X   X 
O     →oX 
X O S X X 


Timestep: 21
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X X P X 
X ↑1    ø-
D   X ↑oX 
O       X 
X O S X X 


Timestep: 22
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X ←1    ø-
D   X ↑oX 
O       X 
X O S X X 


Timestep: 23
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X     ↑oø-
D ↓1X   X 
O       X 
X O S X X 


Timestep: 24
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X     ↑oø-
D   X   X 
O ↓1    X 
X O S X X 


Timestep: 25
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X X ø-X 
X     ↑0ø-
D   X   X 
O ↓1    X 
X O S X X 


Timestep: 26
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X X ø-X 
X   ←0  ø-
D   X   X 
O ↓1    X 
X O S X X 


Timestep: 27
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X X ø-X 
X   ←0  ø-
D   X   X 
O ↓o    X 
X O S X X 


Timestep: 28
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ←0    ø-
D   X   X 
O   →o  X 
X O S X X 


Timestep: 29
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ←0    ø-
D   X   X 
O   →o  X 
X O S X X 


Timestep: 30
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ←0    ø-
D   X   X 
O   →o  X 
X O S X X 


Timestep: 31
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X X ø-X 
X ←0    ø-
D   X   X 
O     →oX 
X O S X X 


Timestep: 32
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X X ø-X 
X       ø-
D ↓0X ↑oX 
O       X 
X O S X X 


Timestep: 33
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X X ø-X 
X     ↑oø-
D   X   X 
O ↓0    X 
X O S X X 


Timestep: 34
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 3 
X X X ø=X 
X     ↑1ø-
D   X   X 
O ←0    X 
X O S X X 


Timestep: 35
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X X ø=X 
X   ←1  ø-
D   X   X 
O ←0    X 
X O S X X 


Timestep: 36
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X X ø=X 
X ←1    ø-
D   X   X 
O ←o    X 
X O S X X 


Timestep: 37
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X X ø=X 
X       ø-
D ↓1X   X 
O ←o    X 
X O S X X 


Timestep: 38
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X X ø=X 
X       ø-
D ←1X   X 
O   →o  X 
X O S X X 


Timestep: 39
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X X ø=X 
X       ø-
D   X   X 
O ↓1  →oX 
X O S X X 


Timestep: 40
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X X ø=X 
X       ø-
D   X ↑oX 
O   →1  X 
X O S X X 


Timestep: 41
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X ø=X 
X       ø-
D   X ↑oX 
O ←1    X 
X O S X X 


Timestep: 42
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X X ø=X 
X     ↑oø-
D   X   X 
O ←1    X 
X O S X X 


Timestep: 43
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X ø=X 
X     ↑oø-
D   X   X 
O ←1    X 
X O S X X 


Timestep: 44
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X X ø=X 
X     →oø-
D   X   X 
O   →1  X 
X O S X X 


Timestep: 45
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X ø=X 
X     →oø-
D   X   X 
O ←1    X 
X O S X X 


Timestep: 46
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X X ø=X 
X     →oø-
D   X   X 
O ←o    X 
X O S X X 


Timestep: 47
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X ø=X 
X     →oø-
D   X   X 
O   →o  X 
X O S X X 


Timestep: 48
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 3 
X X X ø=X 
X     →0ø=
D   X   X 
O     →oX 
X O S X X 


Timestep: 49
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X X ø=X 
X   ←0  ø=
D   X ↑oX 
O       X 
X O S X X 


Timestep: 50
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X ø=X 
X   ←0  ø=
D   X ←oX 
O       X 
X O S X X 


Timestep: 51
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X ø=X 
X ←0    ø=
D   X ←oX 
O       X 
X O S X X 


Timestep: 52
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X X ø=X 
X ←0  ↑oø=
D   X   X 
O       X 
X O S X X 


Timestep: 53
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X X ø1X 
X ←0  ↑1ø=
D   X   X 
O       X 
X O S X X 


Timestep: 54
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X X ø2X 
X       ø=
D ↓0X ↓1X 
O       X 
X O S X X 


Timestep: 55
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X ø3X 
X       ø=
D ↓0X ←1X 
O       X 
X O S X X 


Timestep: 56
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X X ø4X 
X       ø=
D ←0X ←1X 
O       X 
X O S X X 


Timestep: 57
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 3 
X X X ø5X 
X       ø=
D ←dX   X 
O     ↓1X 
X O S X X 


Timestep: 58
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X X ø6X 
X ↑d    ø=
D   X   X 
O     ↓1X 
X O S X X 


Timestep: 59
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X X ø7X 
X   →d  ø=
D   X   X 
O     →1X 
X O S X X 


Timestep: 60
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X X ø8X 
X   →d  ø=
D   X   X 
O     ↓1X 
X O S X X 


Timestep: 61
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X X ø9X 
X     →dø=
D   X   X 
O     →1X 
X O S X X 


Timestep: 62
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X X ø10X 
X     →dø=
D   X   X 
O     →1X 
X O S X X 


Timestep: 63
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X X ø11X 
X     →dø=
D   X   X 
O     →1X 
X O S X X 


Timestep: 64
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X X ø12X 
X     →dø=
D   X   X 
O     →1X 
X O S X X 


Timestep: 65
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X ø13X 
X     →dø=
D   X   X 
O     →1X 
X O S X X 


Timestep: 66
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X X ø14X 
X   ←d  ø=
D   X   X 
O     ↓1X 
X O S X X 


Timestep: 67
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X X ø15X 
X     →dø=
D   X   X 
O     →1X 
X O S X X 


Timestep: 68
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X ø16X 
X     →dø=
D   X   X 
O     →1X 
X O S X X 


Timestep: 69
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X X ø17X 
X     →dø=
D   X   X 
O     →1X 
X O S X X 


Timestep: 70
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X ø18X 
X     →dø=
D   X   X 
O     →1X 
X O S X X 


Timestep: 71
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X X ø19X 
X     →dø=
D   X   X 
O     →1X 
X O S X X 


Timestep: 72
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X X ø20X 
X     ↑dø=
D   X   X 
O     →1X 
X O S X X 


Timestep: 73
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 5 
X X X P X 
X     ↑sø=
D   X   X 
O     →1X 
X O S X X 


Timestep: 74
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X     ↑sø=
D   X   X 
O     ↓1X 
X O S X X 


Timestep: 75
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X     ↑sø=
D   X   X 
O     ↓1X 
X O S X X 


Timestep: 76
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X ↓sX 
O   ←1  X 
X O S X X 


Timestep: 77
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X ↓sX 
O ←1    X 
X O S X X 


Timestep: 78
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O ←1  ↓sX 
X O S X X 


Timestep: 79
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O ←1  ↓sX 
X O S X X 


Timestep: 80
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O ←1←s  X 
X O S X X 


Timestep: 81
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O ←o←s  X 
X O S X X 


Timestep: 82
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O ←o←s  X 
X O S X X 


Timestep: 83
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O →o←s  X 
X O S X X 


Timestep: 84
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O →o↓s  X 
X O S X X 


Timestep: 85
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O →o↓s  X 
X O S X X 


Timestep: 86
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O ↓o↓s  X 
X O S X X 


Timestep: 87
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O ↓o↓s  X 
X O S X X 


Timestep: 88
Joint action taken: ('interact', 'stay') 	 Reward: 20 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O ↓o↓0  X 
X O S X X 


Timestep: 89
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O   →o→0X 
X O S X X 


Timestep: 90
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O   →o→0X 
X O S X X 


Timestep: 91
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O   →o→0X 
X O S X X 


Timestep: 92
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O   →o←0X 
X O S X X 


Timestep: 93
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O   →o←0X 
X O S X X 


Timestep: 94
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O   →o←0X 
X O S X X 


Timestep: 95
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X   X 
O   →o←0X 
X O S X X 


Timestep: 96
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X ↑0X 
O     →oX 
X O S X X 


Timestep: 97
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X ↑0X 
O   ←o  X 
X O S X X 


Timestep: 98
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X ↑0X 
O     →oX 
X O S X X 


Timestep: 99
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X X P X 
X       ø=
D   X ↑0X 
O   ←o  X 
X O S X X 


tot rew 80 tot rew shaped 71
data/ppo_poor_runs/ppo_bc_train_random1/
SP envs: 30/30
Other agent actions took 0.6006112098693848 seconds
Total simulation time for 400 steps: 3.593412399291992 	 Other agent action time: 0 	 111.31480485758098 steps/s
Curr learning rate 0.0009749937484371093 	 Curr reward per step 0.4608466666666666

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.61it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.79it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.43it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 203.32it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 198.68it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 200.84it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.21it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 200.00it/s]
-------------------------------------
| approxkl           | 0.0041069887 |
| clipfrac           | 0.418948     |
| eplenmean          | 400          |
| eprewmean          | 180          |
| explained_variance | 0.386        |
| fps                | 2845         |
| nupdates           | 101          |
| policy_entropy     | 1.1566808    |
| policy_loss        | 0.0037219399 |
| serial_timesteps   | 40400        |
| time_elapsed       | 441          |
| time_remaining     | 89.7         |
| total_timesteps    | 1212000      |
| true_eprew         | 105          |
| value_loss         | 52.0068      |
-------------------------------------
Current reward shaping 0.7576
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6018552780151367 seconds
Total simulation time for 400 steps: 3.637291669845581 	 Other agent action time: 0 	 109.97193414983455 steps/s
Curr learning rate 0.0009747436859214803 	 Curr reward per step 0.4843498666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.38it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 201.87it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 202.66it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 203.90it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 201.21it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 201.91it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 201.87it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.13it/s]
-------------------------------------
| approxkl           | 0.003281633  |
| clipfrac           | 0.37780198   |
| eplenmean          | 400          |
| eprewmean          | 187          |
| explained_variance | 0.415        |
| fps                | 2818         |
| nupdates           | 102          |
| policy_entropy     | 1.1530497    |
| policy_loss        | 0.0020763564 |
| serial_timesteps   | 40800        |
| time_elapsed       | 446          |
| time_remaining     | 89.6         |
| total_timesteps    | 1224000      |
| true_eprew         | 109          |
| value_loss         | 51.045662    |
-------------------------------------
Current reward shaping 0.7552
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6048116683959961 seconds
Total simulation time for 400 steps: 3.62136173248291 	 Other agent action time: 0 	 110.45568754208612 steps/s
Curr learning rate 0.0009744936234058516 	 Curr reward per step 0.48443573333333334

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 200.09it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.25it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 201.20it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 197.86it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 199.82it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 199.40it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 201.31it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 203.22it/s]
-------------------------------------
| approxkl           | 0.0047004437 |
| clipfrac           | 0.43759388   |
| eplenmean          | 400          |
| eprewmean          | 190          |
| explained_variance | 0.464        |
| fps                | 2824         |
| nupdates           | 103          |
| policy_entropy     | 1.1468328    |
| policy_loss        | 0.0041838237 |
| serial_timesteps   | 41200        |
| time_elapsed       | 450          |
| time_remaining     | 89.5         |
| total_timesteps    | 1236000      |
| true_eprew         | 111          |
| value_loss         | 52.174507    |
-------------------------------------
Current reward shaping 0.7528
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6059780120849609 seconds
Total simulation time for 400 steps: 3.6193628311157227 	 Other agent action time: 0 	 110.51668999891177 steps/s
Curr learning rate 0.0009742435608902226 	 Curr reward per step 0.4606194666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 198.95it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 199.83it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 198.70it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 201.38it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 202.52it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 200.77it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 201.67it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 199.18it/s]
-------------------------------------
| approxkl           | 0.003723413  |
| clipfrac           | 0.4165834    |
| eplenmean          | 400          |
| eprewmean          | 190          |
| explained_variance | 0.427        |
| fps                | 2826         |
| nupdates           | 104          |
| policy_entropy     | 1.1555862    |
| policy_loss        | 0.0030099472 |
| serial_timesteps   | 41600        |
| time_elapsed       | 454          |
| time_remaining     | 89.4         |
| total_timesteps    | 1248000      |
| true_eprew         | 111          |
| value_loss         | 46.597767    |
-------------------------------------
Current reward shaping 0.7504
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6033914089202881 seconds
Total simulation time for 400 steps: 3.6461498737335205 	 Other agent action time: 0 	 109.7047608716136 steps/s
Curr learning rate 0.0009739934983745936 	 Curr reward per step 0.4965476

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 197.89it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 201.92it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 201.78it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 200.58it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 200.44it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 201.46it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 201.13it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 200.26it/s]
-------------------------------------
| approxkl           | 0.0036308537 |
| clipfrac           | 0.3908647    |
| eplenmean          | 400          |
| eprewmean          | 192          |
| explained_variance | 0.416        |
| fps                | 2808         |
| nupdates           | 105          |
| policy_entropy     | 1.133778     |
| policy_loss        | 0.0040027956 |
| serial_timesteps   | 42000        |
| time_elapsed       | 458          |
| time_remaining     | 89.3         |
| total_timesteps    | 1260000      |
| true_eprew         | 113          |
| value_loss         | 57.79151     |
-------------------------------------
Current reward shaping 0.748
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.59981369972229 seconds
Total simulation time for 400 steps: 3.6086032390594482 	 Other agent action time: 0 	 110.84621209403353 steps/s
Curr learning rate 0.0009737434358589648 	 Curr reward per step 0.5037216666666666

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 203.28it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 205.57it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 206.91it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 206.53it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 205.44it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.68it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 205.79it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 205.83it/s]
-------------------------------------
| approxkl           | 0.0033739635 |
| clipfrac           | 0.3708022    |
| eplenmean          | 400          |
| eprewmean          | 195          |
| explained_variance | 0.377        |
| fps                | 2842         |
| nupdates           | 106          |
| policy_entropy     | 1.1141129    |
| policy_loss        | 0.002367908  |
| serial_timesteps   | 42400        |
| time_elapsed       | 463          |
| time_remaining     | 89.2         |
| total_timesteps    | 1272000      |
| true_eprew         | 115          |
| value_loss         | 54.730473    |
-------------------------------------
Current reward shaping 0.7456
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6023678779602051 seconds
Total simulation time for 400 steps: 3.6270506381988525 	 Other agent action time: 0 	 110.28244154833055 steps/s
Curr learning rate 0.0009734933733433358 	 Curr reward per step 0.5210157333333334

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.11it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 204.34it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 204.19it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 200.07it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 201.26it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 200.18it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 199.73it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 201.02it/s]
-------------------------------------
| approxkl           | 0.0030487662 |
| clipfrac           | 0.3515208    |
| eplenmean          | 400          |
| eprewmean          | 201          |
| explained_variance | 0.375        |
| fps                | 2821         |
| nupdates           | 107          |
| policy_entropy     | 1.1088668    |
| policy_loss        | 0.001683532  |
| serial_timesteps   | 42800        |
| time_elapsed       | 467          |
| time_remaining     | 89.2         |
| total_timesteps    | 1284000      |
| true_eprew         | 118          |
| value_loss         | 57.82198     |
-------------------------------------
Current reward shaping 0.7432000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5982270240783691 seconds
Total simulation time for 400 steps: 3.6032521724700928 	 Other agent action time: 0 	 111.01082601326594 steps/s
Curr learning rate 0.0009732433108277069 	 Curr reward per step 0.5083381999999999

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 199.90it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.47it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 202.26it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 200.36it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 202.32it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.27it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 202.12it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 202.52it/s]
-------------------------------------
| approxkl           | 0.0039213873 |
| clipfrac           | 0.41196874   |
| eplenmean          | 400          |
| eprewmean          | 204          |
| explained_variance | 0.366        |
| fps                | 2840         |
| nupdates           | 108          |
| policy_entropy     | 1.114654     |
| policy_loss        | 0.0047105793 |
| serial_timesteps   | 43200        |
| time_elapsed       | 471          |
| time_remaining     | 89.1         |
| total_timesteps    | 1296000      |
| true_eprew         | 121          |
| value_loss         | 55.818382    |
-------------------------------------
Current reward shaping 0.7408
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6004576683044434 seconds
Total simulation time for 400 steps: 3.623511791229248 	 Other agent action time: 0 	 110.39014719593423 steps/s
Curr learning rate 0.000972993248312078 	 Curr reward per step 0.49063333333333325

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 182.77it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 194.48it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 202.38it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 202.12it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 203.71it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.34it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 204.65it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 205.14it/s]
-------------------------------------
| approxkl           | 0.0038797557 |
| clipfrac           | 0.38485414   |
| eplenmean          | 400          |
| eprewmean          | 202          |
| explained_variance | 0.357        |
| fps                | 2820         |
| nupdates           | 109          |
| policy_entropy     | 1.1102382    |
| policy_loss        | 0.0040543955 |
| serial_timesteps   | 43600        |
| time_elapsed       | 475          |
| time_remaining     | 89           |
| total_timesteps    | 1308000      |
| true_eprew         | 120          |
| value_loss         | 51.243736    |
-------------------------------------
Current reward shaping 0.7384
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.600752592086792 seconds
Total simulation time for 400 steps: 3.568434715270996 	 Other agent action time: 0 	 112.09396609897709 steps/s
Curr learning rate 0.0009727431857964491 	 Curr reward per step 0.5238283333333332

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 187.50it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 190.33it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 191.38it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 190.47it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 190.28it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 191.24it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 191.92it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 199.34it/s]
-------------------------------------
| approxkl           | 0.0036526136 |
| clipfrac           | 0.3829063    |
| eplenmean          | 400          |
| eprewmean          | 203          |
| explained_variance | 0.323        |
| fps                | 2841         |
| nupdates           | 110          |
| policy_entropy     | 1.100498     |
| policy_loss        | 0.0032050323 |
| serial_timesteps   | 44000        |
| time_elapsed       | 480          |
| time_remaining     | 88.9         |
| total_timesteps    | 1320000      |
| true_eprew         | 120          |
| value_loss         | 58.085648    |
-------------------------------------
Current reward shaping 0.736
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6209855079650879 seconds
Total simulation time for 400 steps: 3.6511876583099365 	 Other agent action time: 0 	 109.55339397294966 steps/s
Curr learning rate 0.0009724931232808203 	 Curr reward per step 0.5198826666666665

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 190.30it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 192.36it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 193.08it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 190.89it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 190.53it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 190.48it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 189.64it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 195.98it/s]
-------------------------------------
| approxkl           | 0.0038780118 |
| clipfrac           | 0.39181253   |
| eplenmean          | 400          |
| eprewmean          | 206          |
| explained_variance | 0.386        |
| fps                | 2786         |
| nupdates           | 111          |
| policy_entropy     | 1.0990369    |
| policy_loss        | 0.004403761  |
| serial_timesteps   | 44400        |
| time_elapsed       | 484          |
| time_remaining     | 88.8         |
| total_timesteps    | 1332000      |
| true_eprew         | 122          |
| value_loss         | 55.37057     |
-------------------------------------
Current reward shaping 0.7336
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6077756881713867 seconds
Total simulation time for 400 steps: 3.585672616958618 	 Other agent action time: 0 	 111.55508121633302 steps/s
Curr learning rate 0.0009722430607651913 	 Curr reward per step 0.5074736

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 180.90it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 184.80it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 186.69it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 186.49it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 183.06it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 193.20it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 195.99it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 195.79it/s]
-------------------------------------
| approxkl           | 0.004244407  |
| clipfrac           | 0.40802082   |
| eplenmean          | 400          |
| eprewmean          | 206          |
| explained_variance | 0.378        |
| fps                | 2822         |
| nupdates           | 112          |
| policy_entropy     | 1.1081537    |
| policy_loss        | 0.0044325525 |
| serial_timesteps   | 44800        |
| time_elapsed       | 488          |
| time_remaining     | 88.7         |
| total_timesteps    | 1344000      |
| true_eprew         | 123          |
| value_loss         | 53.67484     |
-------------------------------------
Current reward shaping 0.7312000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6015944480895996 seconds
Total simulation time for 400 steps: 3.588221788406372 	 Other agent action time: 0 	 111.47582941846272 steps/s
Curr learning rate 0.0009719929982495624 	 Curr reward per step 0.5124385333333334

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 202.04it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 203.98it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 203.27it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 203.50it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 201.82it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 201.81it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 204.15it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.12it/s]
-------------------------------------
| approxkl           | 0.0038060343 |
| clipfrac           | 0.3972083    |
| eplenmean          | 400          |
| eprewmean          | 207          |
| explained_variance | 0.389        |
| fps                | 2851         |
| nupdates           | 113          |
| policy_entropy     | 1.1010565    |
| policy_loss        | 0.0027550939 |
| serial_timesteps   | 45200        |
| time_elapsed       | 492          |
| time_remaining     | 88.6         |
| total_timesteps    | 1356000      |
| true_eprew         | 124          |
| value_loss         | 49.789726    |
-------------------------------------
Current reward shaping 0.7288
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6044080257415771 seconds
Total simulation time for 400 steps: 3.6327810287475586 	 Other agent action time: 0 	 110.10848075748304 steps/s
Curr learning rate 0.0009717429357339335 	 Curr reward per step 0.5165272000000001

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 186.38it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 186.62it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 181.90it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 190.16it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 191.25it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 192.29it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 189.39it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 195.23it/s]
------------------------------------
| approxkl           | 0.004737011 |
| clipfrac           | 0.42776045  |
| eplenmean          | 400         |
| eprewmean          | 205         |
| explained_variance | 0.458       |
| fps                | 2793        |
| nupdates           | 114         |
| policy_entropy     | 1.094403    |
| policy_loss        | 0.006388028 |
| serial_timesteps   | 45600       |
| time_elapsed       | 497         |
| time_remaining     | 88.5        |
| total_timesteps    | 1368000     |
| true_eprew         | 123         |
| value_loss         | 54.25207    |
------------------------------------
Current reward shaping 0.7263999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.601876974105835 seconds
Total simulation time for 400 steps: 3.6120128631591797 	 Other agent action time: 0 	 110.7415768309716 steps/s
Curr learning rate 0.0009714928732183045 	 Curr reward per step 0.5262896

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 195.55it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 198.82it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 199.73it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 199.36it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 199.15it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 199.19it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 200.64it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 196.94it/s]
-------------------------------------
| approxkl           | 0.0053218887 |
| clipfrac           | 0.42978132   |
| eplenmean          | 400          |
| eprewmean          | 207          |
| explained_variance | 0.24         |
| fps                | 2826         |
| nupdates           | 115          |
| policy_entropy     | 1.0511407    |
| policy_loss        | 0.006793208  |
| serial_timesteps   | 46000        |
| time_elapsed       | 501          |
| time_remaining     | 88.4         |
| total_timesteps    | 1380000      |
| true_eprew         | 124          |
| value_loss         | 57.864525    |
-------------------------------------
Current reward shaping 0.724
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5997252464294434 seconds
Total simulation time for 400 steps: 3.6105926036834717 	 Other agent action time: 0 	 110.78513803853863 steps/s
Curr learning rate 0.0009712428107026758 	 Curr reward per step 0.5059176666666666

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 192.46it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 195.16it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 193.77it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 196.05it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 196.77it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 196.53it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 195.67it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 197.01it/s]
-------------------------------------
| approxkl           | 0.0028359836 |
| clipfrac           | 0.34833333   |
| eplenmean          | 400          |
| eprewmean          | 206          |
| explained_variance | 0.349        |
| fps                | 2821         |
| nupdates           | 116          |
| policy_entropy     | 1.0853766    |
| policy_loss        | 0.002414622  |
| serial_timesteps   | 46400        |
| time_elapsed       | 505          |
| time_remaining     | 88.3         |
| total_timesteps    | 1392000      |
| true_eprew         | 123          |
| value_loss         | 56.605053    |
-------------------------------------
Current reward shaping 0.7216
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6053502559661865 seconds
Total simulation time for 400 steps: 3.614253520965576 	 Other agent action time: 0 	 110.67292254947762 steps/s
Curr learning rate 0.0009709927481870468 	 Curr reward per step 0.5321674666666666

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 196.04it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 199.35it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 198.53it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 199.49it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 199.84it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 198.94it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 200.03it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 196.02it/s]
-------------------------------------
| approxkl           | 0.0033626116 |
| clipfrac           | 0.36469787   |
| eplenmean          | 400          |
| eprewmean          | 210          |
| explained_variance | 0.36         |
| fps                | 2824         |
| nupdates           | 117          |
| policy_entropy     | 1.0648127    |
| policy_loss        | 0.002942555  |
| serial_timesteps   | 46800        |
| time_elapsed       | 509          |
| time_remaining     | 88.2         |
| total_timesteps    | 1404000      |
| true_eprew         | 125          |
| value_loss         | 54.881676    |
-------------------------------------
Current reward shaping 0.7192000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6001498699188232 seconds
Total simulation time for 400 steps: 3.7500412464141846 	 Other agent action time: 0 	 106.66549344823414 steps/s
Curr learning rate 0.0009707426856714179 	 Curr reward per step 0.5464562666666665

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 192.15it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 195.61it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 191.98it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 193.34it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 194.69it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 194.01it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 194.32it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 195.06it/s]
-------------------------------------
| approxkl           | 0.0033072568 |
| clipfrac           | 0.3755937    |
| eplenmean          | 400          |
| eprewmean          | 211          |
| explained_variance | 0.386        |
| fps                | 2729         |
| nupdates           | 118          |
| policy_entropy     | 1.0536133    |
| policy_loss        | 0.0033298165 |
| serial_timesteps   | 47200        |
| time_elapsed       | 514          |
| time_remaining     | 88.2         |
| total_timesteps    | 1416000      |
| true_eprew         | 127          |
| value_loss         | 54.89638     |
-------------------------------------
Current reward shaping 0.7168
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5977809429168701 seconds
Total simulation time for 400 steps: 3.606600284576416 	 Other agent action time: 0 	 110.90777142967445 steps/s
Curr learning rate 0.000970492623155789 	 Curr reward per step 0.5417605333333334

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 195.04it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 197.90it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 197.80it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 196.42it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 196.42it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 196.22it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 196.56it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 197.14it/s]
-------------------------------------
| approxkl           | 0.0026924491 |
| clipfrac           | 0.33152092   |
| eplenmean          | 400          |
| eprewmean          | 214          |
| explained_variance | 0.324        |
| fps                | 2826         |
| nupdates           | 119          |
| policy_entropy     | 1.053562     |
| policy_loss        | 0.0016520466 |
| serial_timesteps   | 47600        |
| time_elapsed       | 518          |
| time_remaining     | 88.1         |
| total_timesteps    | 1428000      |
| true_eprew         | 129          |
| value_loss         | 55.872566    |
-------------------------------------
Current reward shaping 0.7143999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5985884666442871 seconds
Total simulation time for 400 steps: 3.5626561641693115 	 Other agent action time: 0 	 112.27578008310724 steps/s
Curr learning rate 0.00097024256064016 	 Curr reward per step 0.5447965333333333

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 198.84it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 201.26it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 201.99it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 200.28it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 200.90it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 201.68it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 199.68it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 201.31it/s]
-------------------------------------
| approxkl           | 0.0040507657 |
| clipfrac           | 0.39652076   |
| eplenmean          | 400          |
| eprewmean          | 217          |
| explained_variance | 0.307        |
| fps                | 2864         |
| nupdates           | 120          |
| policy_entropy     | 1.0458156    |
| policy_loss        | 0.0043398533 |
| serial_timesteps   | 48000        |
| time_elapsed       | 522          |
| time_remaining     | 88           |
| total_timesteps    | 1440000      |
| true_eprew         | 132          |
| value_loss         | 57.82641     |
-------------------------------------
Current reward shaping 0.712
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.609870195388794 seconds
Total simulation time for 400 steps: 3.829071521759033 	 Other agent action time: 0 	 104.46396671542045 steps/s
Curr learning rate 0.0009699924981245312 	 Curr reward per step 0.5600673333333334

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 195.99it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 197.07it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 196.57it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 198.74it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 196.65it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 184.98it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 194.78it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 183.04it/s]
------------------------------------
| approxkl           | 0.004083964 |
| clipfrac           | 0.41390628  |
| eplenmean          | 400         |
| eprewmean          | 220         |
| explained_variance | 0.281       |
| fps                | 2678        |
| nupdates           | 121         |
| policy_entropy     | 1.0380708   |
| policy_loss        | 0.004958868 |
| serial_timesteps   | 48400       |
| time_elapsed       | 527         |
| time_remaining     | 87.9        |
| total_timesteps    | 1452000     |
| true_eprew         | 133         |
| value_loss         | 61.21466    |
------------------------------------
Current reward shaping 0.7096
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6009631156921387 seconds
Total simulation time for 400 steps: 3.6511380672454834 	 Other agent action time: 0 	 109.55488196637022 steps/s
Curr learning rate 0.0009697424356089022 	 Curr reward per step 0.5501752666666667

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 199.29it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 202.29it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 202.63it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 201.93it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 201.95it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 203.19it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 204.59it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 204.51it/s]
-------------------------------------
| approxkl           | 0.0051212907 |
| clipfrac           | 0.4358645    |
| eplenmean          | 400          |
| eprewmean          | 221          |
| explained_variance | 0.357        |
| fps                | 2809         |
| nupdates           | 122          |
| policy_entropy     | 1.0386246    |
| policy_loss        | 0.005517668  |
| serial_timesteps   | 48800        |
| time_elapsed       | 531          |
| time_remaining     | 87.8         |
| total_timesteps    | 1464000      |
| true_eprew         | 134          |
| value_loss         | 54.163223    |
-------------------------------------
Current reward shaping 0.7072
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6059486865997314 seconds
Total simulation time for 400 steps: 4.533365964889526 	 Other agent action time: 0 	 88.23465899244859 steps/s
Curr learning rate 0.0009694923730932734 	 Curr reward per step 0.5616906666666668

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 191.61it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 193.95it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 191.30it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 191.64it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 190.83it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 191.63it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 191.10it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 195.01it/s]
-------------------------------------
| approxkl           | 0.0050725783 |
| clipfrac           | 0.41341656   |
| eplenmean          | 400          |
| eprewmean          | 222          |
| explained_variance | 0.234        |
| fps                | 2313         |
| nupdates           | 123          |
| policy_entropy     | 1.016091     |
| policy_loss        | 0.00595374   |
| serial_timesteps   | 49200        |
| time_elapsed       | 536          |
| time_remaining     | 87.9         |
| total_timesteps    | 1476000      |
| true_eprew         | 135          |
| value_loss         | 62.14168     |
-------------------------------------
Current reward shaping 0.7048
Current self-play randomization 1
SP envs: 30/30
