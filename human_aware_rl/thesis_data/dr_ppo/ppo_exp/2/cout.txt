INFO - PPO - Running command 'ppo_run'
INFO - PPO - Started run with ID "2"
Creating env with params {'RUN_TYPE': 'ppo', 'SEEDS': [9456], 'LOCAL_TESTING': False, 'EX_NAME': 'ppo_bc_train_simple', 'SAVE_DIR': '../../thesis_data/dr_ppo/ppo_bc_train_simple/', 'GPU_ID': 0, 'NUM_AGENTS': 10, 'PPO_RUN_TOT_TIMESTEPS': 8000000.0, 'mdp_params': {'layout_name': 'simple', 'start_order_list': None, 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}}, 'env_params': {'horizon': 400}, 'mdp_generation_params': {'padded_mdp_shape': [11, 7], 'mdp_shape_fn': [[5, 11], [5, 7]], 'prop_empty_fn': [0.6, 1], 'prop_feats_fn': [0, 0.6]}, 'ENTROPY': 0.1, 'GAMMA': 0.99, 'sim_threads': 30, 'TOTAL_BATCH_SIZE': 12000, 'BATCH_SIZE': 400, 'MAX_GRAD_NORM': 0.1, 'LR': 0.001, 'LR_ANNEALING': 3, 'VF_COEF': 0.5, 'STEPS_PER_UPDATE': 8, 'MINIBATCHES': 10, 'CLIPPING': 0.05, 'LAM': 0.98, 'SELF_PLAY_HORIZON': [500000.0, 3000000.0], 'REW_SHAPING_HORIZON': 1000000.0, 'OTHER_AGENT_TYPE': 'bc_train', 'HM_PARAMS': [True, 0.3], 'NUM_HIDDEN_LAYERS': 3, 'SIZE_HIDDEN_LAYERS': 64, 'NUM_FILTERS': 25, 'NUM_CONV_LAYERS': 3, 'NETWORK_TYPE': 'conv_and_mlp', 'SAVE_BEST_THRESH': 50, 'TRAJECTORY_SELF_PLAY': True, 'VIZ_FREQUENCY': 50, 'grad_updates_per_agent': 53280.0}
Computing MediumLevelPlanner to be saved in /home/ubuntu/human_aware_rl/overcooked_ai/overcooked_ai_py/data/planners/simple_am.pkl
It took 0.2010209560394287 seconds to create mlp
self_play_randomization: 1
trajectory_sp: True
LOADING BC MODEL FROM: seed0/worker1
Loading a model without an environment, this model cannot be trained until it has a valid environment.
WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/ubuntu/human_aware_rl/stable-baselines/stable_baselines/common/policies.py:436: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING - tensorflow - From /home/ubuntu/human_aware_rl/stable-baselines/stable_baselines/common/policies.py:436: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
Mlp with different params or mdp found, computing from scratch
Computing MediumLevelPlanner to be saved in /home/ubuntu/human_aware_rl/overcooked_ai/overcooked_ai_py/data/planners/simple_am.pkl
It took 0.1997208595275879 seconds to create mlp
WARNING:tensorflow:From /home/ubuntu/human_aware_rl/baselines/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING - tensorflow - From /home/ubuntu/human_aware_rl/baselines/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
(30, 5, 4, 20)
WARNING:tensorflow:From /home/ubuntu/human_aware_rl/human_aware_rl/baselines_utils.py:127: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.conv2d instead.
WARNING - tensorflow - From /home/ubuntu/human_aware_rl/human_aware_rl/baselines_utils.py:127: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.conv2d instead.
WARNING:tensorflow:From /home/ubuntu/human_aware_rl/human_aware_rl/baselines_utils.py:143: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING - tensorflow - From /home/ubuntu/human_aware_rl/human_aware_rl/baselines_utils.py:143: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
Last layer conv network output shape (30, 64)
(1200, 5, 4, 20)
Last layer conv network output shape (1200, 64)
TOT NUM UPDATES 0
LOADING BC MODEL FROM: seed4/worker11
Loading a model without an environment, this model cannot be trained until it has a valid environment.
Loaded MediumLevelPlanner from /home/ubuntu/human_aware_rl/overcooked_ai/overcooked_ai_py/data/planners/simple_am.pkl
TOT NUM UPDATES 66
SP envs: 30/30
Other agent actions took 0.6112885475158691 seconds
Total simulation time for 400 steps: 5.351114273071289 	 Other agent action time: 0 	 74.75078639470331 steps/s
Curr learning rate 0.001 	 Curr reward per step 0.016583333333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8:  10%|█         | 1/10 [00:00<00:01,  8.64it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 58.70it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.81it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 179.94it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 182.10it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 188.97it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 187.33it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.13it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 187.53it/s]
Logging to /tmp/openai-2019-12-31-15-56-02-405579
---------------------------------------
| approxkl           | 0.00025494967  |
| clipfrac           | 0.009937501    |
| eplenmean          | 400            |
| eprewmean          | 6.63           |
| explained_variance | 5.07e-05       |
| fps                | 2016           |
| nupdates           | 1              |
| policy_entropy     | 1.7915077      |
| policy_loss        | -0.00047481395 |
| serial_timesteps   | 400            |
| time_elapsed       | 5.95           |
| time_remaining     | 6.45           |
| total_timesteps    | 12000          |
| true_eprew         | 0              |
| value_loss         | 0.5521105      |
---------------------------------------
Current reward shaping 0.988
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6038789749145508 seconds
Total simulation time for 400 steps: 3.531623125076294 	 Other agent action time: 0 	 113.26236855790177 steps/s
Curr learning rate 0.00098989898989899 	 Curr reward per step 0.023896666666666663

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.74it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 179.40it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.68it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 173.15it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.86it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.85it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 176.86it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 188.70it/s]
--------------------------------------
| approxkl           | 0.00040013186 |
| clipfrac           | 0.031177083   |
| eplenmean          | 400           |
| eprewmean          | 8.1           |
| explained_variance | 0.00192       |
| fps                | 2991          |
| nupdates           | 2             |
| policy_entropy     | 1.790135      |
| policy_loss        | -0.0010106511 |
| serial_timesteps   | 800           |
| time_elapsed       | 9.96          |
| time_remaining     | 5.31          |
| total_timesteps    | 24000         |
| true_eprew         | 0.333         |
| value_loss         | 1.0656314     |
--------------------------------------
Current reward shaping 0.976
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6077044010162354 seconds
Total simulation time for 400 steps: 3.518122434616089 	 Other agent action time: 0 	 113.69700953675012 steps/s
Curr learning rate 0.0009797979797979799 	 Curr reward per step 0.030701333333333327

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.00it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.90it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 177.42it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 177.32it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.37it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 177.26it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.39it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 177.71it/s]
--------------------------------------
| approxkl           | 0.00038822857 |
| clipfrac           | 0.052666664   |
| eplenmean          | 400           |
| eprewmean          | 9.49          |
| explained_variance | 0.00281       |
| fps                | 3002          |
| nupdates           | 3             |
| policy_entropy     | 1.7888813     |
| policy_loss        | -0.001297195  |
| serial_timesteps   | 1200          |
| time_elapsed       | 14            |
| time_remaining     | 4.89          |
| total_timesteps    | 36000         |
| true_eprew         | 0.889         |
| value_loss         | 1.9571838     |
--------------------------------------
Current reward shaping 0.964
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.602074384689331 seconds
Total simulation time for 400 steps: 3.548208713531494 	 Other agent action time: 0 	 112.73293999717517 steps/s
Curr learning rate 0.0009696969696969698 	 Curr reward per step 0.027774999999999994

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.08it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.66it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 174.83it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 173.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 173.96it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.11it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 173.29it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 173.03it/s]
--------------------------------------
| approxkl           | 0.00037825116 |
| clipfrac           | 0.058614574   |
| eplenmean          | 400           |
| eprewmean          | 10.5          |
| explained_variance | 0.00437       |
| fps                | 2969          |
| nupdates           | 4             |
| policy_entropy     | 1.7882773     |
| policy_loss        | -0.0017208832 |
| serial_timesteps   | 1600          |
| time_elapsed       | 18            |
| time_remaining     | 4.65          |
| total_timesteps    | 48000         |
| true_eprew         | 1             |
| value_loss         | 1.2906945     |
--------------------------------------
Current reward shaping 0.952
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6070859432220459 seconds
Total simulation time for 400 steps: 3.570741653442383 	 Other agent action time: 0 	 112.0215458921199 steps/s
Curr learning rate 0.0009595959595959597 	 Curr reward per step 0.03570333333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.82it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 186.98it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 187.61it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.48it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 189.54it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 190.95it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.12it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 185.48it/s]
--------------------------------------
| approxkl           | 0.00044339296 |
| clipfrac           | 0.08912499    |
| eplenmean          | 400           |
| eprewmean          | 12.2          |
| explained_variance | 0.00506       |
| fps                | 2980          |
| nupdates           | 5             |
| policy_entropy     | 1.787102      |
| policy_loss        | -0.002916336  |
| serial_timesteps   | 2000          |
| time_elapsed       | 22            |
| time_remaining     | 4.48          |
| total_timesteps    | 60000         |
| true_eprew         | 1.8           |
| value_loss         | 2.647114      |
--------------------------------------
Current reward shaping 0.94
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.610926628112793 seconds
Total simulation time for 400 steps: 3.5443201065063477 	 Other agent action time: 0 	 112.85662354980735 steps/s
Curr learning rate 0.0009494949494949496 	 Curr reward per step 0.029396666666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 171.20it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 178.80it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 177.27it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 177.79it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.01it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.35it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 174.71it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 176.22it/s]
-------------------------------------
| approxkl           | 0.0005274847 |
| clipfrac           | 0.12108334   |
| eplenmean          | 400          |
| eprewmean          | 12.4         |
| explained_variance | 0.0193       |
| fps                | 2981         |
| nupdates           | 6            |
| policy_entropy     | 1.7860677    |
| policy_loss        | -0.003702192 |
| serial_timesteps   | 2400         |
| time_elapsed       | 26.1         |
| time_remaining     | 4.34         |
| total_timesteps    | 72000        |
| true_eprew         | 1.6          |
| value_loss         | 1.1963494    |
-------------------------------------
Current reward shaping 0.928
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6157371997833252 seconds
Total simulation time for 400 steps: 3.6008970737457275 	 Other agent action time: 0 	 111.08343054746403 steps/s
Curr learning rate 0.0009393939393939395 	 Curr reward per step 0.041319999999999996

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.82it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 169.41it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 171.24it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 171.82it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 172.21it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.07it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.91it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.44it/s]
--------------------------------------
| approxkl           | 0.0005869678  |
| clipfrac           | 0.13630205    |
| eplenmean          | 400           |
| eprewmean          | 13.5          |
| explained_variance | 0.0109        |
| fps                | 2925          |
| nupdates           | 7             |
| policy_entropy     | 1.7838542     |
| policy_loss        | -0.0038945768 |
| serial_timesteps   | 2800          |
| time_elapsed       | 30.2          |
| time_remaining     | 4.24          |
| total_timesteps    | 84000         |
| true_eprew         | 2.4           |
| value_loss         | 3.1139207     |
--------------------------------------
Current reward shaping 0.916
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6156835556030273 seconds
Total simulation time for 400 steps: 3.541778802871704 	 Other agent action time: 0 	 112.93760064171049 steps/s
Curr learning rate 0.0009292929292929292 	 Curr reward per step 0.04122033333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.72it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 183.19it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.50it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.98it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.09it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 177.59it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.80it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 179.53it/s]
-------------------------------------
| approxkl           | 0.0006155114 |
| clipfrac           | 0.14819792   |
| eplenmean          | 400          |
| eprewmean          | 15           |
| explained_variance | 0.0225       |
| fps                | 2987         |
| nupdates           | 8            |
| policy_entropy     | 1.7810997    |
| policy_loss        | -0.004628482 |
| serial_timesteps   | 3200         |
| time_elapsed       | 34.2         |
| time_remaining     | 4.13         |
| total_timesteps    | 96000        |
| true_eprew         | 3            |
| value_loss         | 3.0738134    |
-------------------------------------
Current reward shaping 0.904
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6075630187988281 seconds
Total simulation time for 400 steps: 3.5421602725982666 	 Other agent action time: 0 	 112.92543792960267 steps/s
Curr learning rate 0.0009191919191919192 	 Curr reward per step 0.03628200000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.50it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 174.50it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.74it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 176.47it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 175.08it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.05it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 173.73it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 173.20it/s]
--------------------------------------
| approxkl           | 0.0006800186  |
| clipfrac           | 0.16408332    |
| eplenmean          | 400           |
| eprewmean          | 15.2          |
| explained_variance | 0.0204        |
| fps                | 2978          |
| nupdates           | 9             |
| policy_entropy     | 1.7784675     |
| policy_loss        | -0.0037186095 |
| serial_timesteps   | 3600          |
| time_elapsed       | 38.2          |
| time_remaining     | 4.03          |
| total_timesteps    | 108000        |
| true_eprew         | 3.4           |
| value_loss         | 2.9369385     |
--------------------------------------
Current reward shaping 0.892
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6054842472076416 seconds
Total simulation time for 400 steps: 3.5397419929504395 	 Other agent action time: 0 	 113.00258628923197 steps/s
Curr learning rate 0.0009090909090909091 	 Curr reward per step 0.05213533333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.24it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 179.50it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.40it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.23it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.35it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 178.68it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 183.31it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 177.85it/s]
-------------------------------------
| approxkl           | 0.0007328681 |
| clipfrac           | 0.17376043   |
| eplenmean          | 400          |
| eprewmean          | 17           |
| explained_variance | 0.0304       |
| fps                | 2988         |
| nupdates           | 10           |
| policy_entropy     | 1.7761362    |
| policy_loss        | -0.003910712 |
| serial_timesteps   | 4000         |
| time_elapsed       | 42.2         |
| time_remaining     | 3.94         |
| total_timesteps    | 120000       |
| true_eprew         | 4            |
| value_loss         | 4.641041     |
-------------------------------------
Current reward shaping 0.88
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6010971069335938 seconds
Total simulation time for 400 steps: 3.5474095344543457 	 Other agent action time: 0 	 112.75833706680474 steps/s
Curr learning rate 0.000898989898989899 	 Curr reward per step 0.051373333333333326

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.96it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 175.39it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 177.81it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 177.02it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.76it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 181.42it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.67it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 185.49it/s]
--------------------------------------
| approxkl           | 0.0007792893  |
| clipfrac           | 0.18555208    |
| eplenmean          | 400           |
| eprewmean          | 18.5          |
| explained_variance | 0.0304        |
| fps                | 2983          |
| nupdates           | 11            |
| policy_entropy     | 1.7771631     |
| policy_loss        | -0.0042940155 |
| serial_timesteps   | 4400          |
| time_elapsed       | 46.2          |
| time_remaining     | 3.85          |
| total_timesteps    | 132000        |
| true_eprew         | 5             |
| value_loss         | 4.5447726     |
--------------------------------------
Current reward shaping 0.868
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6085457801818848 seconds
Total simulation time for 400 steps: 3.580155611038208 	 Other agent action time: 0 	 111.72698716411496 steps/s
Curr learning rate 0.0008888888888888889 	 Curr reward per step 0.054864666666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.39it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.76it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.51it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 184.99it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 182.11it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.85it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 180.92it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.18it/s]
--------------------------------------
| approxkl           | 0.00086285325 |
| clipfrac           | 0.19631249    |
| eplenmean          | 400           |
| eprewmean          | 20.9          |
| explained_variance | 0.038         |
| fps                | 2963          |
| nupdates           | 12            |
| policy_entropy     | 1.7742665     |
| policy_loss        | -0.005283964  |
| serial_timesteps   | 4800          |
| time_elapsed       | 50.3          |
| time_remaining     | 3.77          |
| total_timesteps    | 144000        |
| true_eprew         | 6.4           |
| value_loss         | 6.0717096     |
--------------------------------------
Current reward shaping 0.856
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6078817844390869 seconds
Total simulation time for 400 steps: 3.5705103874206543 	 Other agent action time: 0 	 112.02880165514965 steps/s
Curr learning rate 0.0008787878787878789 	 Curr reward per step 0.069136

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 179.39it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 186.93it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 186.06it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 187.78it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 187.52it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 187.00it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.71it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 189.43it/s]
-------------------------------------
| approxkl           | 0.0010500646 |
| clipfrac           | 0.23246872   |
| eplenmean          | 400          |
| eprewmean          | 23.1         |
| explained_variance | 0.0405       |
| fps                | 2980         |
| nupdates           | 13           |
| policy_entropy     | 1.7698004    |
| policy_loss        | -0.005262614 |
| serial_timesteps   | 5200         |
| time_elapsed       | 54.3         |
| time_remaining     | 3.69         |
| total_timesteps    | 156000       |
| true_eprew         | 8            |
| value_loss         | 8.086725     |
-------------------------------------
Current reward shaping 0.844
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6088161468505859 seconds
Total simulation time for 400 steps: 3.5692734718322754 	 Other agent action time: 0 	 112.06762472998777 steps/s
Curr learning rate 0.0008686868686868688 	 Curr reward per step 0.05572300000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.07it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.04it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.12it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 182.51it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.32it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.37it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.07it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 188.33it/s]
--------------------------------------
| approxkl           | 0.00096329657 |
| clipfrac           | 0.21866667    |
| eplenmean          | 400           |
| eprewmean          | 23.5          |
| explained_variance | 0.0536        |
| fps                | 2975          |
| nupdates           | 14            |
| policy_entropy     | 1.7670052     |
| policy_loss        | -0.0048897658 |
| serial_timesteps   | 5600          |
| time_elapsed       | 58.4          |
| time_remaining     | 3.61          |
| total_timesteps    | 168000        |
| true_eprew         | 8.2           |
| value_loss         | 4.879527      |
--------------------------------------
Current reward shaping 0.832
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6082861423492432 seconds
Total simulation time for 400 steps: 3.562858819961548 	 Other agent action time: 0 	 112.26939382468065 steps/s
Curr learning rate 0.0008585858585858587 	 Curr reward per step 0.08019733333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.42it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.21it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 184.03it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 184.79it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 186.72it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.79it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 176.87it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.28it/s]
-------------------------------------
| approxkl           | 0.0010057712 |
| clipfrac           | 0.22639582   |
| eplenmean          | 400          |
| eprewmean          | 27.2         |
| explained_variance | 0.048        |
| fps                | 2978         |
| nupdates           | 15           |
| policy_entropy     | 1.7617207    |
| policy_loss        | -0.004994358 |
| serial_timesteps   | 6000         |
| time_elapsed       | 62.4         |
| time_remaining     | 3.54         |
| total_timesteps    | 180000       |
| true_eprew         | 9.8          |
| value_loss         | 9.436566     |
-------------------------------------
Current reward shaping 0.8200000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6166036128997803 seconds
Total simulation time for 400 steps: 3.614039421081543 	 Other agent action time: 0 	 110.6794789416811 steps/s
Curr learning rate 0.0008484848484848486 	 Curr reward per step 0.067055

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 171.00it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.57it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.67it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 179.22it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.77it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.22it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.74it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.84it/s]
--------------------------------------
| approxkl           | 0.0010924259  |
| clipfrac           | 0.23470834    |
| eplenmean          | 400           |
| eprewmean          | 27.5          |
| explained_variance | 0.0596        |
| fps                | 2935          |
| nupdates           | 16            |
| policy_entropy     | 1.7623789     |
| policy_loss        | -0.0055996906 |
| serial_timesteps   | 6400          |
| time_elapsed       | 66.5          |
| time_remaining     | 3.46          |
| total_timesteps    | 192000        |
| true_eprew         | 9             |
| value_loss         | 5.957947      |
--------------------------------------
Current reward shaping 0.808
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6096932888031006 seconds
Total simulation time for 400 steps: 3.5679149627685547 	 Other agent action time: 0 	 112.11029527722167 steps/s
Curr learning rate 0.0008383838383838385 	 Curr reward per step 0.072268

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 165.75it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.86it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.33it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.15it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.21it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 173.49it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 174.10it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 173.66it/s]
-------------------------------------
| approxkl           | 0.0011631872 |
| clipfrac           | 0.24863538   |
| eplenmean          | 400          |
| eprewmean          | 28.4         |
| explained_variance | 0.0682       |
| fps                | 2957         |
| nupdates           | 17           |
| policy_entropy     | 1.7604681    |
| policy_loss        | -0.005370095 |
| serial_timesteps   | 6800         |
| time_elapsed       | 70.5         |
| time_remaining     | 3.39         |
| total_timesteps    | 204000       |
| true_eprew         | 9.4          |
| value_loss         | 7.502867     |
-------------------------------------
Current reward shaping 0.796
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6047103404998779 seconds
Total simulation time for 400 steps: 3.5729289054870605 	 Other agent action time: 0 	 111.95296928122674 steps/s
Curr learning rate 0.0008282828282828282 	 Curr reward per step 0.080405

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.93it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 187.20it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 187.52it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 187.22it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 188.37it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 190.03it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.03it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 187.80it/s]
--------------------------------------
| approxkl           | 0.0011237363  |
| clipfrac           | 0.2343334     |
| eplenmean          | 400           |
| eprewmean          | 29            |
| explained_variance | 0.0752        |
| fps                | 2978          |
| nupdates           | 18            |
| policy_entropy     | 1.7487224     |
| policy_loss        | -0.0054634316 |
| serial_timesteps   | 7200          |
| time_elapsed       | 74.6          |
| time_remaining     | 3.31          |
| total_timesteps    | 216000        |
| true_eprew         | 9.4           |
| value_loss         | 8.891562      |
--------------------------------------
Current reward shaping 0.784
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6207637786865234 seconds
Total simulation time for 400 steps: 3.5929462909698486 	 Other agent action time: 0 	 111.32924558469463 steps/s
Curr learning rate 0.0008181818181818183 	 Curr reward per step 0.10409466666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.39it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 178.89it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.21it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 182.43it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 182.16it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 180.69it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 180.44it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.69it/s]
-------------------------------------
| approxkl           | 0.0013123163 |
| clipfrac           | 0.27790618   |
| eplenmean          | 400          |
| eprewmean          | 33.4         |
| explained_variance | 0.0464       |
| fps                | 2953         |
| nupdates           | 19           |
| policy_entropy     | 1.7444332    |
| policy_loss        | -0.005237592 |
| serial_timesteps   | 7600         |
| time_elapsed       | 78.6         |
| time_remaining     | 3.24         |
| total_timesteps    | 228000       |
| true_eprew         | 12.6         |
| value_loss         | 13.925548    |
-------------------------------------
Current reward shaping 0.772
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6088433265686035 seconds
Total simulation time for 400 steps: 3.5988223552703857 	 Other agent action time: 0 	 111.14747006453652 steps/s
Curr learning rate 0.0008080808080808081 	 Curr reward per step 0.09822166666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 176.87it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 185.06it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 189.34it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 190.88it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 188.61it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 187.85it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.92it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 187.12it/s]
-------------------------------------
| approxkl           | 0.0011469031 |
| clipfrac           | 0.2442188    |
| eplenmean          | 400          |
| eprewmean          | 36.4         |
| explained_variance | 0.0691       |
| fps                | 2958         |
| nupdates           | 20           |
| policy_entropy     | 1.7363096    |
| policy_loss        | -0.006299061 |
| serial_timesteps   | 8000         |
| time_elapsed       | 82.7         |
| time_remaining     | 3.17         |
| total_timesteps    | 240000       |
| true_eprew         | 14.6         |
| value_loss         | 11.026332    |
-------------------------------------
Current reward shaping 0.76
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.595587968826294 seconds
Total simulation time for 400 steps: 3.642857789993286 	 Other agent action time: 0 	 109.80390206248957 steps/s
Curr learning rate 0.000797979797979798 	 Curr reward per step 0.11075666666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 181.34it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 183.45it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.45it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 181.36it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.50it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 181.96it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.67it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.72it/s]
-------------------------------------
| approxkl           | 0.0012388246 |
| clipfrac           | 0.26336455   |
| eplenmean          | 400          |
| eprewmean          | 40.4         |
| explained_variance | 0.0649       |
| fps                | 2920         |
| nupdates           | 21           |
| policy_entropy     | 1.7355407    |
| policy_loss        | -0.005128507 |
| serial_timesteps   | 8400         |
| time_elapsed       | 86.8         |
| time_remaining     | 3.1          |
| total_timesteps    | 252000       |
| true_eprew         | 17.4         |
| value_loss         | 14.6061535   |
-------------------------------------
Current reward shaping 0.748
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6005010604858398 seconds
Total simulation time for 400 steps: 3.555948495864868 	 Other agent action time: 0 	 112.48756849688654 steps/s
Curr learning rate 0.0007878787878787879 	 Curr reward per step 0.10111333333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 185.90it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 195.78it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 193.93it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 191.88it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 185.55it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 187.45it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.12it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.35it/s]
--------------------------------------
| approxkl           | 0.0011894612  |
| clipfrac           | 0.25158328    |
| eplenmean          | 400           |
| eprewmean          | 41.9          |
| explained_variance | 0.0727        |
| fps                | 2995          |
| nupdates           | 22            |
| policy_entropy     | 1.7352371     |
| policy_loss        | -0.0055793417 |
| serial_timesteps   | 8800          |
| time_elapsed       | 90.8          |
| time_remaining     | 3.03          |
| total_timesteps    | 264000        |
| true_eprew         | 19.2          |
| value_loss         | 12.745376     |
--------------------------------------
Current reward shaping 0.736
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6044454574584961 seconds
Total simulation time for 400 steps: 3.5641467571258545 	 Other agent action time: 0 	 112.22882424812438 steps/s
Curr learning rate 0.0007777777777777778 	 Curr reward per step 0.13476799999999997

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.33it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.00it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.62it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 181.17it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 179.52it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.00it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.24it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.22it/s]
--------------------------------------
| approxkl           | 0.001306868   |
| clipfrac           | 0.27302086    |
| eplenmean          | 400           |
| eprewmean          | 45.8          |
| explained_variance | 0.0777        |
| fps                | 2974          |
| nupdates           | 23            |
| policy_entropy     | 1.7248977     |
| policy_loss        | -0.0057736356 |
| serial_timesteps   | 9200          |
| time_elapsed       | 94.8          |
| time_remaining     | 2.96          |
| total_timesteps    | 276000        |
| true_eprew         | 22.2          |
| value_loss         | 19.229414     |
--------------------------------------
Current reward shaping 0.724
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6273536682128906 seconds
Total simulation time for 400 steps: 3.6218886375427246 	 Other agent action time: 0 	 110.43961867126333 steps/s
Curr learning rate 0.0007676767676767678 	 Curr reward per step 0.14166833333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 158.54it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.70it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 165.33it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.83it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 168.88it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.52it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 186.37it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 189.77it/s]
-------------------------------------
| approxkl           | 0.0014290094 |
| clipfrac           | 0.2903229    |
| eplenmean          | 400          |
| eprewmean          | 49.1         |
| explained_variance | 0.09         |
| fps                | 2916         |
| nupdates           | 24           |
| policy_entropy     | 1.7137601    |
| policy_loss        | -0.006374435 |
| serial_timesteps   | 9600         |
| time_elapsed       | 99           |
| time_remaining     | 2.89         |
| total_timesteps    | 288000       |
| true_eprew         | 25           |
| value_loss         | 20.735022    |
-------------------------------------
Current reward shaping 0.712
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6042048931121826 seconds
Total simulation time for 400 steps: 3.5567753314971924 	 Other agent action time: 0 	 112.46141876260248 steps/s
Curr learning rate 0.0007575757575757577 	 Curr reward per step 0.14431733333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.08it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.42it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 179.46it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 183.14it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 182.85it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.95it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 186.68it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 163.80it/s]
-------------------------------------
| approxkl           | 0.0013927048 |
| clipfrac           | 0.28130203   |
| eplenmean          | 400          |
| eprewmean          | 55.3         |
| explained_variance | 0.0866       |
| fps                | 2978         |
| nupdates           | 25           |
| policy_entropy     | 1.7065485    |
| policy_loss        | -0.005077975 |
| serial_timesteps   | 10000        |
| time_elapsed       | 103          |
| time_remaining     | 2.81         |
| total_timesteps    | 300000       |
| true_eprew         | 29.4         |
| value_loss         | 22.10514     |
-------------------------------------
Current reward shaping 0.7
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.605741024017334 seconds
Total simulation time for 400 steps: 3.619112253189087 	 Other agent action time: 0 	 110.52434188730352 steps/s
Curr learning rate 0.0007474747474747475 	 Curr reward per step 0.14979999999999996

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.86it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 176.37it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.86it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.37it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.31it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.71it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 174.76it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 172.86it/s]
--------------------------------------
| approxkl           | 0.0012196263  |
| clipfrac           | 0.2613854     |
| eplenmean          | 400           |
| eprewmean          | 57.5          |
| explained_variance | 0.0907        |
| fps                | 2922          |
| nupdates           | 26            |
| policy_entropy     | 1.6990287     |
| policy_loss        | -0.0055305874 |
| serial_timesteps   | 10400         |
| time_elapsed       | 107           |
| time_remaining     | 2.75          |
| total_timesteps    | 312000        |
| true_eprew         | 31            |
| value_loss         | 22.919447     |
--------------------------------------
Current reward shaping 0.688
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6041293144226074 seconds
Total simulation time for 400 steps: 3.596745729446411 	 Other agent action time: 0 	 111.21164243699971 steps/s
Curr learning rate 0.0007373737373737374 	 Curr reward per step 0.14741999999999997

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 180.47it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.05it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 181.05it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 182.51it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.61it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.59it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.88it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 180.64it/s]
--------------------------------------
| approxkl           | 0.0012846     |
| clipfrac           | 0.26592708    |
| eplenmean          | 400           |
| eprewmean          | 58.3          |
| explained_variance | 0.0877        |
| fps                | 2953          |
| nupdates           | 27            |
| policy_entropy     | 1.6953789     |
| policy_loss        | -0.0046388786 |
| serial_timesteps   | 10800         |
| time_elapsed       | 111           |
| time_remaining     | 2.68          |
| total_timesteps    | 324000        |
| true_eprew         | 31.6          |
| value_loss         | 21.358767     |
--------------------------------------
Current reward shaping 0.6759999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6113941669464111 seconds
Total simulation time for 400 steps: 3.568105936050415 	 Other agent action time: 0 	 112.10429487493452 steps/s
Curr learning rate 0.0007272727272727272 	 Curr reward per step 0.15124799999999997

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.44it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.16it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 175.58it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 179.76it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 182.76it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.22it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 183.63it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 188.27it/s]
--------------------------------------
| approxkl           | 0.0011180239  |
| clipfrac           | 0.2397604     |
| eplenmean          | 400           |
| eprewmean          | 59.5          |
| explained_variance | 0.0874        |
| fps                | 2970          |
| nupdates           | 28            |
| policy_entropy     | 1.6982689     |
| policy_loss        | -0.0061500473 |
| serial_timesteps   | 11200         |
| time_elapsed       | 115           |
| time_remaining     | 2.61          |
| total_timesteps    | 336000        |
| true_eprew         | 32.4          |
| value_loss         | 22.859856     |
--------------------------------------
Current reward shaping 0.6639999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.606572151184082 seconds
Total simulation time for 400 steps: 3.5994203090667725 	 Other agent action time: 0 	 111.12900568805999 steps/s
Curr learning rate 0.0007171717171717171 	 Curr reward per step 0.17548799999999998

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.39it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.47it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 167.86it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.82it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 166.15it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 169.55it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.42it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 170.16it/s]
-------------------------------------
| approxkl           | 0.0011445761 |
| clipfrac           | 0.24594793   |
| eplenmean          | 400          |
| eprewmean          | 63.2         |
| explained_variance | 0.0835       |
| fps                | 2924         |
| nupdates           | 29           |
| policy_entropy     | 1.6856362    |
| policy_loss        | -0.006361378 |
| serial_timesteps   | 11600        |
| time_elapsed       | 119          |
| time_remaining     | 2.54         |
| total_timesteps    | 348000       |
| true_eprew         | 35.4         |
| value_loss         | 29.005508    |
-------------------------------------
Current reward shaping 0.652
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6010034084320068 seconds
Total simulation time for 400 steps: 3.575183153152466 	 Other agent action time: 0 	 111.88237996906386 steps/s
Curr learning rate 0.0007070707070707071 	 Curr reward per step 0.19401966666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 164.41it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 170.20it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 171.75it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 169.87it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 169.52it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 168.84it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 163.96it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.28it/s]
--------------------------------------
| approxkl           | 0.0015510952  |
| clipfrac           | 0.29017714    |
| eplenmean          | 400           |
| eprewmean          | 68.4          |
| explained_variance | 0.0911        |
| fps                | 2943          |
| nupdates           | 30            |
| policy_entropy     | 1.6849697     |
| policy_loss        | -0.0034903053 |
| serial_timesteps   | 12000         |
| time_elapsed       | 123           |
| time_remaining     | 2.47          |
| total_timesteps    | 360000        |
| true_eprew         | 39.8          |
| value_loss         | 32.46899      |
--------------------------------------
Current reward shaping 0.64
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6087801456451416 seconds
Total simulation time for 400 steps: 3.6080870628356934 	 Other agent action time: 0 	 110.8620698541651 steps/s
Curr learning rate 0.000696969696969697 	 Curr reward per step 0.17486666666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.98it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 176.90it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.49it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 183.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.95it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.00it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 180.15it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.59it/s]
-------------------------------------
| approxkl           | 0.0013966756 |
| clipfrac           | 0.2762291    |
| eplenmean          | 400          |
| eprewmean          | 71.5         |
| explained_variance | 0.106        |
| fps                | 2939         |
| nupdates           | 31           |
| policy_entropy     | 1.6788642    |
| policy_loss        | -0.004587254 |
| serial_timesteps   | 12400        |
| time_elapsed       | 127          |
| time_remaining     | 2.4          |
| total_timesteps    | 372000       |
| true_eprew         | 42.6         |
| value_loss         | 28.827402    |
-------------------------------------
Current reward shaping 0.628
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6084225177764893 seconds
Total simulation time for 400 steps: 3.55829119682312 	 Other agent action time: 0 	 112.41350914650387 steps/s
Curr learning rate 0.0006868686868686869 	 Curr reward per step 0.16934166666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.15it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.91it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 179.83it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 181.80it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.15it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.45it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.77it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.05it/s]
--------------------------------------
| approxkl           | 0.0013219283  |
| clipfrac           | 0.2632813     |
| eplenmean          | 400           |
| eprewmean          | 71.3          |
| explained_variance | 0.11          |
| fps                | 2980          |
| nupdates           | 32            |
| policy_entropy     | 1.6764425     |
| policy_loss        | -0.0052739466 |
| serial_timesteps   | 12800         |
| time_elapsed       | 131           |
| time_remaining     | 2.33          |
| total_timesteps    | 384000        |
| true_eprew         | 42.8          |
| value_loss         | 26.948185     |
--------------------------------------
Current reward shaping 0.616
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6011636257171631 seconds
Total simulation time for 400 steps: 3.543647527694702 	 Other agent action time: 0 	 112.87804356214217 steps/s
Curr learning rate 0.0006767676767676768 	 Curr reward per step 0.20643533333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 171.42it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.90it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 171.74it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 177.65it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.62it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 178.97it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 177.74it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.97it/s]
--------------------------------------
| approxkl           | 0.001159643   |
| clipfrac           | 0.24142703    |
| eplenmean          | 400           |
| eprewmean          | 74.9          |
| explained_variance | 0.098         |
| fps                | 2977          |
| nupdates           | 33            |
| policy_entropy     | 1.6575384     |
| policy_loss        | -0.0063869134 |
| serial_timesteps   | 13200         |
| time_elapsed       | 136           |
| time_remaining     | 2.26          |
| total_timesteps    | 396000        |
| true_eprew         | 45.6          |
| value_loss         | 35.997456     |
--------------------------------------
Current reward shaping 0.604
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6110105514526367 seconds
Total simulation time for 400 steps: 3.5925233364105225 	 Other agent action time: 0 	 111.34235258709853 steps/s
Curr learning rate 0.0006666666666666668 	 Curr reward per step 0.20694833333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.85it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 185.39it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 184.90it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 187.48it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.64it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 181.22it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.21it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.91it/s]
--------------------------------------
| approxkl           | 0.0012256833  |
| clipfrac           | 0.25247914    |
| eplenmean          | 400           |
| eprewmean          | 76.4          |
| explained_variance | 0.121         |
| fps                | 2958          |
| nupdates           | 34            |
| policy_entropy     | 1.6546161     |
| policy_loss        | -0.0041938196 |
| serial_timesteps   | 13600         |
| time_elapsed       | 140           |
| time_remaining     | 2.19          |
| total_timesteps    | 408000        |
| true_eprew         | 46.8          |
| value_loss         | 34.822166     |
--------------------------------------
Current reward shaping 0.5920000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.614124059677124 seconds
Total simulation time for 400 steps: 3.5678536891937256 	 Other agent action time: 0 	 112.11222063604106 steps/s
Curr learning rate 0.0006565656565656567 	 Curr reward per step 0.190832

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.20it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 182.73it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.24it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.10it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 179.35it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 181.70it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 183.73it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.59it/s]
--------------------------------------
| approxkl           | 0.0010902238  |
| clipfrac           | 0.22476041    |
| eplenmean          | 400           |
| eprewmean          | 79.2          |
| explained_variance | 0.126         |
| fps                | 2971          |
| nupdates           | 35            |
| policy_entropy     | 1.6571758     |
| policy_loss        | -0.0038349957 |
| serial_timesteps   | 14000         |
| time_elapsed       | 144           |
| time_remaining     | 2.12          |
| total_timesteps    | 420000        |
| true_eprew         | 49.2          |
| value_loss         | 32.68972      |
--------------------------------------
Current reward shaping 0.5800000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6175050735473633 seconds
Total simulation time for 400 steps: 3.582420825958252 	 Other agent action time: 0 	 111.65634062352379 steps/s
Curr learning rate 0.0006464646464646465 	 Curr reward per step 0.21336999999999995

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 189.63it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 191.57it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 191.37it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 190.48it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 189.56it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 190.10it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.46it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 189.06it/s]
--------------------------------------
| approxkl           | 0.0011262542  |
| clipfrac           | 0.23904172    |
| eplenmean          | 400           |
| eprewmean          | 82.4          |
| explained_variance | 0.116         |
| fps                | 2976          |
| nupdates           | 36            |
| policy_entropy     | 1.6268656     |
| policy_loss        | -0.0058050686 |
| serial_timesteps   | 14400         |
| time_elapsed       | 148           |
| time_remaining     | 2.05          |
| total_timesteps    | 432000        |
| true_eprew         | 52.2          |
| value_loss         | 35.341377     |
--------------------------------------
Current reward shaping 0.5680000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6020503044128418 seconds
Total simulation time for 400 steps: 3.5182766914367676 	 Other agent action time: 0 	 113.69202455667322 steps/s
Curr learning rate 0.0006363636363636364 	 Curr reward per step 0.20722066666666664

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.66it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 169.69it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.12it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.55it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 172.73it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.52it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 174.59it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.64it/s]
-------------------------------------
| approxkl           | 0.0009613874 |
| clipfrac           | 0.20598957   |
| eplenmean          | 400          |
| eprewmean          | 81.9         |
| explained_variance | 0.13         |
| fps                | 2991         |
| nupdates           | 37           |
| policy_entropy     | 1.6390775    |
| policy_loss        | -0.004875087 |
| serial_timesteps   | 14800        |
| time_elapsed       | 152          |
| time_remaining     | 1.98         |
| total_timesteps    | 444000       |
| true_eprew         | 52.6         |
| value_loss         | 34.875633    |
-------------------------------------
Current reward shaping 0.556
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6136677265167236 seconds
Total simulation time for 400 steps: 3.5673623085021973 	 Other agent action time: 0 	 112.1276633569482 steps/s
Curr learning rate 0.0006262626262626263 	 Curr reward per step 0.2047793333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 176.37it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.14it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 188.35it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 188.60it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 186.47it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.75it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.18it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.96it/s]
--------------------------------------
| approxkl           | 0.0017470643  |
| clipfrac           | 0.29270825    |
| eplenmean          | 400           |
| eprewmean          | 82.4          |
| explained_variance | 0.133         |
| fps                | 2978          |
| nupdates           | 38            |
| policy_entropy     | 1.6358654     |
| policy_loss        | -0.0012941512 |
| serial_timesteps   | 15200         |
| time_elapsed       | 156           |
| time_remaining     | 1.91          |
| total_timesteps    | 456000        |
| true_eprew         | 53.4          |
| value_loss         | 34.511864     |
--------------------------------------
Current reward shaping 0.544
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5994400978088379 seconds
Total simulation time for 400 steps: 3.542448043823242 	 Other agent action time: 0 	 112.9162644170481 steps/s
Curr learning rate 0.0006161616161616161 	 Curr reward per step 0.214176

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 180.43it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 190.92it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 188.44it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 189.69it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 188.62it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 190.65it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 191.06it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 190.40it/s]
-------------------------------------
| approxkl           | 0.0011337894 |
| clipfrac           | 0.23776038   |
| eplenmean          | 400          |
| eprewmean          | 84.2         |
| explained_variance | 0.12         |
| fps                | 3005         |
| nupdates           | 39           |
| policy_entropy     | 1.6378853    |
| policy_loss        | -0.003262084 |
| serial_timesteps   | 15600        |
| time_elapsed       | 160          |
| time_remaining     | 1.84         |
| total_timesteps    | 468000       |
| true_eprew         | 54.6         |
| value_loss         | 35.877056    |
-------------------------------------
Current reward shaping 0.532
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.599992036819458 seconds
Total simulation time for 400 steps: 3.5462229251861572 	 Other agent action time: 0 	 112.79606737610897 steps/s
Curr learning rate 0.0006060606060606061 	 Curr reward per step 0.22283133333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.40it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 179.77it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 185.04it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 184.86it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.95it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.46it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.22it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 187.53it/s]
-------------------------------------
| approxkl           | 0.0010361148 |
| clipfrac           | 0.22347918   |
| eplenmean          | 400          |
| eprewmean          | 85.5         |
| explained_variance | 0.119        |
| fps                | 2991         |
| nupdates           | 40           |
| policy_entropy     | 1.6190336    |
| policy_loss        | -0.00492277  |
| serial_timesteps   | 16000        |
| time_elapsed       | 164          |
| time_remaining     | 1.77         |
| total_timesteps    | 480000       |
| true_eprew         | 55.6         |
| value_loss         | 38.304157    |
-------------------------------------
Current reward shaping 0.52
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5967419147491455 seconds
Total simulation time for 400 steps: 3.5468320846557617 	 Other agent action time: 0 	 112.77669493587601 steps/s
Curr learning rate 0.000595959595959596 	 Curr reward per step 0.25042666666666663

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 185.63it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 194.85it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 194.81it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 196.53it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 198.04it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 198.36it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 198.52it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 193.58it/s]
--------------------------------------
| approxkl           | 0.0012193712  |
| clipfrac           | 0.2619166     |
| eplenmean          | 400           |
| eprewmean          | 90.7          |
| explained_variance | 0.122         |
| fps                | 3012          |
| nupdates           | 41            |
| policy_entropy     | 1.6194861     |
| policy_loss        | -0.0043608965 |
| serial_timesteps   | 16400         |
| time_elapsed       | 168           |
| time_remaining     | 1.7           |
| total_timesteps    | 492000        |
| true_eprew         | 60            |
| value_loss         | 43.489693     |
--------------------------------------
Current reward shaping 0.508
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6071240901947021 seconds
Total simulation time for 400 steps: 3.5412631034851074 	 Other agent action time: 0 	 112.9540472737942 steps/s
Curr learning rate 0.0005858585858585859 	 Curr reward per step 0.243671

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.62it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.67it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 177.73it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 177.05it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.79it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 188.67it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.41it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.83it/s]
-------------------------------------
| approxkl           | 0.0010791076 |
| clipfrac           | 0.23093756   |
| eplenmean          | 400          |
| eprewmean          | 95.3         |
| explained_variance | 0.136        |
| fps                | 2991         |
| nupdates           | 42           |
| policy_entropy     | 1.6074289    |
| policy_loss        | -0.004890935 |
| serial_timesteps   | 16800        |
| time_elapsed       | 172          |
| time_remaining     | 1.64         |
| total_timesteps    | 504000       |
| true_eprew         | 64           |
| value_loss         | 41.46062     |
-------------------------------------
Current reward shaping 0.496
Current self-play randomization 0.9984
SP envs: 30/30
Other agent actions took 0.6041350364685059 seconds
Total simulation time for 400 steps: 3.5583860874176025 	 Other agent action time: 0 	 112.41051144348663 steps/s
Curr learning rate 0.0005757575757575758 	 Curr reward per step 0.26478799999999997

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 188.69it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 195.65it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 193.76it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 197.46it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 194.11it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 193.75it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 191.82it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 195.30it/s]
--------------------------------------
| approxkl           | 0.0011835357  |
| clipfrac           | 0.24739587    |
| eplenmean          | 400           |
| eprewmean          | 100           |
| explained_variance | 0.111         |
| fps                | 3001          |
| nupdates           | 43            |
| policy_entropy     | 1.5966413     |
| policy_loss        | -0.0043838085 |
| serial_timesteps   | 17200         |
| time_elapsed       | 176           |
| time_remaining     | 1.57          |
| total_timesteps    | 516000        |
| true_eprew         | 68.2          |
| value_loss         | 45.01272      |
--------------------------------------
Current reward shaping 0.484
Current self-play randomization 0.9936
SP envs: 30/30
Other agent actions took 0.6068453788757324 seconds
Total simulation time for 400 steps: 3.561934471130371 	 Other agent action time: 0 	 112.29852857822536 steps/s
Curr learning rate 0.0005656565656565657 	 Curr reward per step 0.25957766666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 176.86it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.32it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 181.19it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 183.29it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 179.33it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 181.75it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 180.67it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.80it/s]
--------------------------------------
| approxkl           | 0.0011919697  |
| clipfrac           | 0.25174993    |
| eplenmean          | 400           |
| eprewmean          | 102           |
| explained_variance | 0.128         |
| fps                | 2976          |
| nupdates           | 44            |
| policy_entropy     | 1.5901765     |
| policy_loss        | -0.0034081056 |
| serial_timesteps   | 17600         |
| time_elapsed       | 180           |
| time_remaining     | 1.5           |
| total_timesteps    | 528000        |
| true_eprew         | 70.2          |
| value_loss         | 44.116642     |
--------------------------------------
Current reward shaping 0.472
Current self-play randomization 0.9888
SP envs: 30/30
Other agent actions took 0.6042523384094238 seconds
Total simulation time for 400 steps: 3.5597519874572754 	 Other agent action time: 0 	 112.36737879756598 steps/s
Curr learning rate 0.0005555555555555557 	 Curr reward per step 0.232152

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 176.49it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 187.89it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 186.58it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 188.40it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 190.70it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 190.26it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 189.27it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 185.56it/s]
--------------------------------------
| approxkl           | 0.00082504825 |
| clipfrac           | 0.18110415    |
| eplenmean          | 400           |
| eprewmean          | 100           |
| explained_variance | 0.16          |
| fps                | 2988          |
| nupdates           | 45            |
| policy_entropy     | 1.5960882     |
| policy_loss        | -0.004556716  |
| serial_timesteps   | 18000         |
| time_elapsed       | 184           |
| time_remaining     | 1.43          |
| total_timesteps    | 540000        |
| true_eprew         | 69.2          |
| value_loss         | 38.81059      |
--------------------------------------
Current reward shaping 0.45999999999999996
Current self-play randomization 0.984
SP envs: 28/30
Other agent actions took 5.1656224727630615 seconds
Total simulation time for 400 steps: 7.961086750030518 	 Other agent action time: 0 	 50.24439659553599 steps/s
Curr learning rate 0.0005454545454545455 	 Curr reward per step 0.2319116666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.71it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.76it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 174.34it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 177.42it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.09it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.31it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.37it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 179.19it/s]
--------------------------------------
| approxkl           | 0.00074781163 |
| clipfrac           | 0.16297917    |
| eplenmean          | 400           |
| eprewmean          | 97.3          |
| explained_variance | 0.173         |
| fps                | 1420          |
| nupdates           | 46            |
| policy_entropy     | 1.5813177     |
| policy_loss        | -0.004496673  |
| serial_timesteps   | 18400         |
| time_elapsed       | 192           |
| time_remaining     | 1.39          |
| total_timesteps    | 552000        |
| true_eprew         | 67.8          |
| value_loss         | 39.209354     |
--------------------------------------
Current reward shaping 0.44799999999999995
Current self-play randomization 0.9792
SP envs: 29/30
Other agent actions took 5.177363872528076 seconds
Total simulation time for 400 steps: 7.973130702972412 	 Other agent action time: 0 	 50.168499037759226 steps/s
Curr learning rate 0.0005353535353535353 	 Curr reward per step 0.2592933333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 171.49it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 175.64it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.62it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 171.73it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 172.99it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 180.98it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.10it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.88it/s]
-------------------------------------
| approxkl           | 0.0009843186 |
| clipfrac           | 0.21072915   |
| eplenmean          | 400          |
| eprewmean          | 96.7         |
| explained_variance | 0.13         |
| fps                | 1419         |
| nupdates           | 47           |
| policy_entropy     | 1.5592688    |
| policy_loss        | -0.00405396  |
| serial_timesteps   | 18800        |
| time_elapsed       | 201          |
| time_remaining     | 1.35         |
| total_timesteps    | 564000       |
| true_eprew         | 68           |
| value_loss         | 44.9284      |
-------------------------------------
Current reward shaping 0.43600000000000005
Current self-play randomization 0.9744
SP envs: 27/30
Other agent actions took 5.172399520874023 seconds
Total simulation time for 400 steps: 8.005545139312744 	 Other agent action time: 0 	 49.965366884976305 steps/s
Curr learning rate 0.0005252525252525252 	 Curr reward per step 0.2411313333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.14it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.67it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 175.66it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 175.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.29it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 173.85it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 186.37it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.75it/s]
-------------------------------------
| approxkl           | 0.0010492465 |
| clipfrac           | 0.21113542   |
| eplenmean          | 400          |
| eprewmean          | 97.9         |
| explained_variance | 0.157        |
| fps                | 1413         |
| nupdates           | 48           |
| policy_entropy     | 1.5517112    |
| policy_loss        | -0.003751317 |
| serial_timesteps   | 19200        |
| time_elapsed       | 209          |
| time_remaining     | 1.31         |
| total_timesteps    | 576000       |
| true_eprew         | 69.6         |
| value_loss         | 38.93383     |
-------------------------------------
Current reward shaping 0.42400000000000004
Current self-play randomization 0.9696
SP envs: 30/30
Other agent actions took 0.5892438888549805 seconds
Total simulation time for 400 steps: 3.482537269592285 	 Other agent action time: 0 	 114.85878514283054 steps/s
Curr learning rate 0.0005151515151515151 	 Curr reward per step 0.284294

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.71it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 183.49it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 179.01it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 184.94it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 186.28it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 187.20it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.93it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 187.40it/s]
--------------------------------------
| approxkl           | 0.0011420265  |
| clipfrac           | 0.24105215    |
| eplenmean          | 400           |
| eprewmean          | 104           |
| explained_variance | 0.171         |
| fps                | 3040          |
| nupdates           | 49            |
| policy_entropy     | 1.5521011     |
| policy_loss        | -0.0039813914 |
| serial_timesteps   | 19600         |
| time_elapsed       | 213           |
| time_remaining     | 1.23          |
| total_timesteps    | 588000        |
| true_eprew         | 74.8          |
| value_loss         | 48.969357     |
--------------------------------------
Current reward shaping 0.41200000000000003
Current self-play randomization 0.9648
SP envs: 28/30
Other agent actions took 5.161839246749878 seconds
Total simulation time for 400 steps: 7.974695205688477 	 Other agent action time: 0 	 50.15865681169528 steps/s
Curr learning rate 0.000505050505050505 	 Curr reward per step 0.2806363333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 165.21it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 175.54it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.58it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 172.39it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 175.95it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.15it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.19it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 172.50it/s]
--------------------------------------
| approxkl           | 0.0010869231  |
| clipfrac           | 0.22543748    |
| eplenmean          | 400           |
| eprewmean          | 106           |
| explained_variance | 0.163         |
| fps                | 1417          |
| nupdates           | 50            |
| policy_entropy     | 1.5488346     |
| policy_loss        | -0.0039264294 |
| serial_timesteps   | 20000         |
| time_elapsed       | 222           |
| time_remaining     | 1.18          |
| total_timesteps    | 600000        |
| true_eprew         | 76.8          |
| value_loss         | 50.139988     |
--------------------------------------
Current reward shaping 0.4
Current self-play randomization 0.96
../../thesis_data/dr_ppo/ppo_bc_train_simple/
PPO agent on index 0:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑1O 
X ←0    X 
X D X S X 


Timestep: 2
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑1O 
X ←0    X 
X D X S X 


Timestep: 3
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 4
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   →0→1O 
X       X 
X D X S X 


Timestep: 5
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O   →0→oO 
X       X 
X D X S X 


Timestep: 6
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑0→oO 
X       X 
X D X S X 


Timestep: 7
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑0→oO 
X       X 
X D X S X 


Timestep: 8
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O ←0  ↑oO 
X       X 
X D X S X 


Timestep: 9
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ←o  ↑oO 
X       X 
X D X S X 


Timestep: 10
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →o↑oO 
X       X 
X D X S X 


Timestep: 11
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   →o←oO 
X       X 
X D X S X 


Timestep: 12
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o←oO 
X       X 
X D X S X 


Timestep: 13
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o←oO 
X       X 
X D X S X 


Timestep: 14
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑0←oO 
X       X 
X D X S X 


Timestep: 15
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←0  ↑oO 
X       X 
X D X S X 


Timestep: 16
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←0  ↑oO 
X       X 
X D X S X 


Timestep: 17
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←0←o  O 
X       X 
X D X S X 


Timestep: 18
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←0↑o  O 
X       X 
X D X S X 


Timestep: 19
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ↓0    X 
X D X S X 


Timestep: 20
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 21
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 22
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 23
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X ↓d    X 
X D X S X 


Timestep: 24
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X ←d    X 
X D X S X 


Timestep: 25
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
Xd←0    X 
X D X S X 


Timestep: 26
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X ←d    X 
X D X S X 


Timestep: 27
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X   →d  X 
X D X S X 


Timestep: 28
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X   ↓d  X 
X D X S X 


Timestep: 29
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X   ↓d  X 
X D X S X 


Timestep: 30
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X   ↑d  X 
X D X S X 


Timestep: 31
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X   ↓d  X 
X D X S X 


Timestep: 32
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑1  O 
X     →dX 
X D X S X 


Timestep: 33
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←1  ↑dO 
X       X 
X D X S X 


Timestep: 34
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ←1  ↑0O 
X       X 
X D X S X 


Timestep: 35
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ←1    O 
X     ↓0X 
X D X S X 


Timestep: 36
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ←1    O 
X     ↓0X 
X D X S X 


Timestep: 37
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O       O 
X ↓1  ↓0X 
X D X S X 


Timestep: 38
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O       O 
X ↓1  ↓0X 
X D X S X 


Timestep: 39
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O       O 
X ↓1  ↓0X 
X D X S X 


Timestep: 40
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O       O 
X ↓1←0  X 
X D X S X 


Timestep: 41
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   ↑0  O 
X   →1  X 
X D X S X 


Timestep: 42
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   ↑0  O 
X ←1    X 
X D X S X 


Timestep: 43
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   ↑0  O 
X ←1    X 
X D X S X 


Timestep: 44
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   ↑0  O 
X ←1    X 
X D X S X 


Timestep: 45
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   ↑0  O 
X ←1    X 
X D X S X 


Timestep: 46
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ←0    O 
X ↓1    X 
X D X S X 


Timestep: 47
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ←o    O 
X ↓1    X 
X D X S X 


Timestep: 48
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ↓o    O 
X ↓1    X 
X D X S X 


Timestep: 49
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ↓o    O 
X ↓1    X 
X D X S X 


Timestep: 50
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   →o  O 
X ↓d    X 
X D X S X 


Timestep: 51
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   ↑o  O 
X   →d  X 
X D X S X 


Timestep: 52
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø1XdX 
O   ↑0  O 
X   →d  X 
X D X S X 


Timestep: 53
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X ø2XdX 
O   ↑d→0O 
X       X 
X D X S X 


Timestep: 54
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø3XdX 
O   ↑d  O 
X     ↓0X 
X D X S X 


Timestep: 55
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø4XdX 
O   ↑d  O 
X     ↓0X 
X D X S X 


Timestep: 56
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø5XdX 
O   ↑d  O 
X   ←0  X 
X D X S X 


Timestep: 57
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø6XdX 
O   ↑0→dO 
X       X 
X D X S X 


Timestep: 58
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø7XdX 
O   ↑0→dO 
X       X 
X D X S X 


Timestep: 59
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø8XdX 
O   ↑0→dO 
X       X 
X D X S X 


Timestep: 60
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø9XdX 
O ←0  →dO 
X       X 
X D X S X 


Timestep: 61
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø10XdX 
O ←o  ↑dO 
X       X 
X D X S X 


Timestep: 62
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø11XdX 
O ↑o  ↑dO 
X       X 
X D X S X 


Timestep: 63
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø12XdX 
O ↑o  ↑dO 
X       X 
X D X S X 


Timestep: 64
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø13XdX 
O     ↑dO 
X ↓o    X 
X D X S X 


Timestep: 65
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø14XdX 
O     ↑dO 
X ←o    X 
X D X S X 


Timestep: 66
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø15XdX 
O     →dO 
X ←o    X 
X D X S X 


Timestep: 67
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø16XdX 
O     →dO 
X ↓o    X 
X D X S X 


Timestep: 68
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø17XdX 
O ↑o  →dO 
X       X 
X D X S X 


Timestep: 69
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø18XdX 
O ↑o  →dO 
X       X 
X D X S X 


Timestep: 70
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø19XdX 
O ←o  ↑dO 
X       X 
X D X S X 


Timestep: 71
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O ↑o  ↑dO 
X       X 
X D X S X 


Timestep: 72
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   →o↑dO 
X       X 
X D X S X 


Timestep: 73
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   →o↑dO 
X       X 
X D X S X 


Timestep: 74
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   →o↑dO 
X       X 
X D X S X 


Timestep: 75
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   →o→dO 
X       X 
X D X S X 


Timestep: 76
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   →o→dO 
X       X 
X D X S X 


Timestep: 77
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   →o→dO 
X       X 
X D X S X 


Timestep: 78
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   →o→dO 
X       X 
X D X S X 


Timestep: 79
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑o↑dO 
X       X 
X D X S X 


Timestep: 80
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑dO 
X   ↓o  X 
X D X S X 


Timestep: 81
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑dO 
X   ↓0  X 
X D XoS X 


Timestep: 82
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑dO 
X     →0X 
X D XoS X 


Timestep: 83
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑dO 
X     ↓0X 
X D XoS X 


Timestep: 84
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑dO 
X     ↓0X 
X D XoS X 


Timestep: 85
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →dO 
X   ←0  X 
X D XoS X 


Timestep: 86
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑0→dO 
X       X 
X D XoS X 


Timestep: 87
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑0→dO 
X       X 
X D XoS X 


Timestep: 88
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑0→dO 
X       X 
X D XoS X 


Timestep: 89
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑0→dO 
X       X 
X D XoS X 


Timestep: 90
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑0↑dO 
X       X 
X D XoS X 


Timestep: 91
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑0→dO 
X       X 
X D XoS X 


Timestep: 92
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑0→dO 
X       X 
X D XoS X 


Timestep: 93
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑0↑dO 
X       X 
X D XoS X 


Timestep: 94
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑0↑dO 
X       X 
X D XoS X 


Timestep: 95
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑0↑dO 
X       X 
X D XoS X 


Timestep: 96
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑0↑dO 
X       X 
X D XoS X 


Timestep: 97
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑dO 
X   ↓0  X 
X D XoS X 


Timestep: 98
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →dO 
X     →0X 
X D XoS X 


Timestep: 99
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑dO 
X   ←0  X 
X D XoS X 


tot rew 20 tot rew shaped 26
PPO agent on index 1:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 


Timestep: 2
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X ↑0    X 
X D X S X 


Timestep: 3
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X ↑0    X 
X D X S X 


Timestep: 4
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X ↑0    X 
X D X S X 


Timestep: 5
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   →0  X 
X D X S X 


Timestep: 6
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑1  O 
X   →0  X 
X D X S X 


Timestep: 7
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←1    O 
X ←0    X 
X D X S X 


Timestep: 8
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o    O 
X ←0    X 
X D X S X 


Timestep: 9
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o    O 
X ←0    X 
X D X S X 


Timestep: 10
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o    O 
X ←0    X 
X D X S X 


Timestep: 11
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ↓o    O 
X ↓0    X 
X D X S X 


Timestep: 12
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   →o  O 
X ↓d    X 
X D X S X 


Timestep: 13
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →o  O 
X   →d  X 
X D X S X 


Timestep: 14
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →o  O 
X   →d  X 
X D X S X 


Timestep: 15
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o    O 
X   →d  X 
X D X S X 


Timestep: 16
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o    O 
X   →d  X 
X D X S X 


Timestep: 17
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o↑d  O 
X       X 
X D X S X 


Timestep: 18
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O →o↑d  O 
X       X 
X D X S X 


Timestep: 19
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O →o↑d  O 
X       X 
X D X S X 


Timestep: 20
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O →o↑d  O 
X       X 
X D X S X 


Timestep: 21
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O →o↑d  O 
X       X 
X D X S X 


Timestep: 22
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o  →dO 
X       X 
X D X S X 


Timestep: 23
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o  ↑dO 
X       X 
X D X S X 


Timestep: 24
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o  ↑dO 
X       X 
X D X S X 


Timestep: 25
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o  ↑dO 
X       X 
X D X S X 


Timestep: 26
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o  ↑dO 
X       X 
X D X S X 


Timestep: 27
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o  →dO 
X       X 
X D X S X 


Timestep: 28
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o  →dO 
X       X 
X D X S X 


Timestep: 29
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →o→dO 
X       X 
X D X S X 


Timestep: 30
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →o→dO 
X       X 
X D X S X 


Timestep: 31
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o→dO 
X       X 
X D X S X 


Timestep: 32
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o→dO 
X       X 
X D X S X 


Timestep: 33
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o←dO 
X       X 
X D X S X 


Timestep: 34
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o←dO 
X       X 
X D X S X 


Timestep: 35
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑1←dO 
X       X 
X D X S X 


Timestep: 36
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑1←dO 
X       X 
X D X S X 


Timestep: 37
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑1↑dO 
X       X 
X D X S X 


Timestep: 38
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑1→dO 
X       X 
X D X S X 


Timestep: 39
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑1→dO 
X       X 
X D X S X 


Timestep: 40
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑1→dO 
X       X 
X D X S X 


Timestep: 41
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑1→dO 
X       X 
X D X S X 


Timestep: 42
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑1→dO 
X       X 
X D X S X 


Timestep: 43
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     ↑dO 
X   ↓1  X 
X D X S X 


Timestep: 44
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     ↑dO 
X   ↓1  X 
X D X S X 


Timestep: 45
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     ↑dO 
X ←1    X 
X D X S X 


Timestep: 46
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O     ↑0O 
X ←1    X 
X D X S X 


Timestep: 47
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O     →0O 
X ↓1    X 
X D X S X 


Timestep: 48
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ↑1  →0O 
X       X 
X D X S X 


Timestep: 49
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ←1  →0O 
X       X 
X D X S X 


Timestep: 50
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ←1  →0O 
X       X 
X D X S X 


Timestep: 51
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ↑1  →oO 
X       X 
X D X S X 


Timestep: 52
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ↑1←o  O 
X       X 
X D X S X 


Timestep: 53
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ↑1←o  O 
X       X 
X D X S X 


Timestep: 54
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ←o    O 
X ↓1    X 
X D X S X 


Timestep: 55
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   →o  O 
X ↓d    X 
X D X S X 


Timestep: 56
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 57
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 58
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø1XdX 
O   ↑0  O 
X ↓d    X 
X D X S X 


Timestep: 59
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X ø2XdX 
O   ↓0  O 
X →d    X 
X D X S X 


Timestep: 60
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø3XdX 
O   ↓0  O 
X ←d    X 
X D X S X 


Timestep: 61
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø4XdX 
O ↑d↓0  O 
X       X 
X D X S X 


Timestep: 62
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø5XdX 
O ↑d↓0  O 
X       X 
X D X S X 


Timestep: 63
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X ø6XdX 
O ←0    O 
X ↓d    X 
X D X S X 


Timestep: 64
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X ø7XdX 
O   →0  O 
X ↓d    X 
X D X S X 


Timestep: 65
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø8XdX 
O   →0  O 
X   →d  X 
X D X S X 


Timestep: 66
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø9XdX 
O   →0  O 
X   →d  X 
X D X S X 


Timestep: 67
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø10XdX 
O   →0  O 
X   →d  X 
X D X S X 


Timestep: 68
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø11XdX 
O ←0    O 
X   →d  X 
X D X S X 


Timestep: 69
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø12XdX 
O ←0↑d  O 
X       X 
X D X S X 


Timestep: 70
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø13XdX 
O ←0↑d  O 
X       X 
X D X S X 


Timestep: 71
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø14XdX 
O ←0↑d  O 
X       X 
X D X S X 


Timestep: 72
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø15XdX 
O →0↑d  O 
X       X 
X D X S X 


Timestep: 73
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø16XdX 
O →0↑d  O 
X       X 
X D X S X 


Timestep: 74
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø17XdX 
O →0↑d  O 
X       X 
X D X S X 


Timestep: 75
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø18XdX 
O   ↑d  O 
X ↓0    X 
X D X S X 


Timestep: 76
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø19XdX 
O   ↑d  O 
X   →0  X 
X D X S X 


Timestep: 77
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑d  O 
X ←0    X 
X D X S X 


Timestep: 78
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 5 
X X P XdX 
O   ↑s  O 
X ←0    X 
X D X S X 


Timestep: 79
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑s  O 
X ←0    X 
X D X S X 


Timestep: 80
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P XdX 
O ←s    O 
X ←0    X 
X D X S X 


Timestep: 81
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s  O 
X ←0    X 
X D X S X 


Timestep: 82
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P XdX 
O ↑0→s  O 
X       X 
X D X S X 


Timestep: 83
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O ↑0↑s  O 
X       X 
X D X S X 


Timestep: 84
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O ↑0  →sO 
X       X 
X D X S X 


Timestep: 85
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X P XdX 
O ↑0    O 
X     ↓sX 
X D X S X 


Timestep: 86
Joint action taken: ('←', 'interact') 	 Reward: 20 + shape * 0 
X X P XdX 
O ←0    O 
X     ↓1X 
X D X S X 


Timestep: 87
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O ←0    O 
X     →1X 
X D X S X 


Timestep: 88
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →0  O 
X     →1X 
X D X S X 


Timestep: 89
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P XdX 
O ←0    O 
X     →1X 
X D X S X 


Timestep: 90
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X P XdX 
O ←0    O 
X     →1X 
X D X S X 


Timestep: 91
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P XdX 
O ←0    O 
X   ←1  X 
X D X S X 


Timestep: 92
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O ←0    O 
X     →1X 
X D X S X 


Timestep: 93
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →0  O 
X     →1X 
X D X S X 


Timestep: 94
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →0  O 
X   ←1  X 
X D X S X 


Timestep: 95
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →0  O 
X   ↑1  X 
X D X S X 


Timestep: 96
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →0  O 
X     →1X 
X D X S X 


Timestep: 97
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X P XdX 
O ←0    O 
X     ↓1X 
X D X S X 


Timestep: 98
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →0  O 
X   ←1  X 
X D X S X 


Timestep: 99
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →0  O 
X   ↑1  X 
X D X S X 


tot rew 40 tot rew shaped 34
../../thesis_data/dr_ppo/ppo_bc_train_simple/
SP envs: 26/30
Other agent actions took 5.236820459365845 seconds
Total simulation time for 400 steps: 8.041419267654419 	 Other agent action time: 0 	 49.74246295165194 steps/s
Curr learning rate 0.000494949494949495 	 Curr reward per step 0.2596333333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 171.08it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.36it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.99it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.28it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.91it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 177.96it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.05it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.96it/s]
--------------------------------------
| approxkl           | 0.0011339642  |
| clipfrac           | 0.23130207    |
| eplenmean          | 400           |
| eprewmean          | 107           |
| explained_variance | 0.11          |
| fps                | 1408          |
| nupdates           | 51            |
| policy_entropy     | 1.4980427     |
| policy_loss        | -0.0037537566 |
| serial_timesteps   | 20400         |
| time_elapsed       | 232           |
| time_remaining     | 1.14          |
| total_timesteps    | 612000        |
| true_eprew         | 78.2          |
| value_loss         | 42.84203      |
--------------------------------------
Current reward shaping 0.388
Current self-play randomization 0.9552
SP envs: 28/30
Other agent actions took 5.169138669967651 seconds
Total simulation time for 400 steps: 7.9649786949157715 	 Other agent action time: 0 	 50.21984556660888 steps/s
Curr learning rate 0.0004848484848484849 	 Curr reward per step 0.291874

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.58it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 187.16it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 191.10it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.93it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 186.13it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.72it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 190.55it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.72it/s]
-------------------------------------
| approxkl           | 0.001322867  |
| clipfrac           | 0.24985418   |
| eplenmean          | 400          |
| eprewmean          | 112          |
| explained_variance | 0.163        |
| fps                | 1425         |
| nupdates           | 52           |
| policy_entropy     | 1.5210252    |
| policy_loss        | -0.002711296 |
| serial_timesteps   | 20800        |
| time_elapsed       | 241          |
| time_remaining     | 1.08         |
| total_timesteps    | 624000       |
| true_eprew         | 82.2         |
| value_loss         | 49.374813    |
-------------------------------------
Current reward shaping 0.376
Current self-play randomization 0.9504
SP envs: 30/30
Other agent actions took 0.6107833385467529 seconds
Total simulation time for 400 steps: 3.5741021633148193 	 Other agent action time: 0 	 111.9162188774755 steps/s
Curr learning rate 0.0004747474747474748 	 Curr reward per step 0.2767026666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 165.12it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.12it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.45it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.33it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 169.97it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.86it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 171.72it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.65it/s]
-------------------------------------
| approxkl           | 0.0010297907 |
| clipfrac           | 0.20686455   |
| eplenmean          | 400          |
| eprewmean          | 110          |
| explained_variance | 0.187        |
| fps                | 2943         |
| nupdates           | 53           |
| policy_entropy     | 1.5268096    |
| policy_loss        | -0.004316921 |
| serial_timesteps   | 21200        |
| time_elapsed       | 245          |
| time_remaining     | 1            |
| total_timesteps    | 636000       |
| true_eprew         | 82           |
| value_loss         | 45.717865    |
-------------------------------------
Current reward shaping 0.364
Current self-play randomization 0.9456
SP envs: 29/30
Other agent actions took 5.171151161193848 seconds
Total simulation time for 400 steps: 7.945376873016357 	 Other agent action time: 0 	 50.34374157360081 steps/s
Curr learning rate 0.0004646464646464647 	 Curr reward per step 0.3042846666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.93it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.27it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.57it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 183.52it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.66it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 160.80it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 183.41it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.99it/s]
--------------------------------------
| approxkl           | 0.0011029611  |
| clipfrac           | 0.22546872    |
| eplenmean          | 400           |
| eprewmean          | 115           |
| explained_variance | 0.156         |
| fps                | 1425          |
| nupdates           | 54            |
| policy_entropy     | 1.4936447     |
| policy_loss        | -0.0047612647 |
| serial_timesteps   | 21600         |
| time_elapsed       | 253           |
| time_remaining     | 0.938         |
| total_timesteps    | 648000        |
| true_eprew         | 86            |
| value_loss         | 51.835564     |
--------------------------------------
Current reward shaping 0.352
Current self-play randomization 0.9408
SP envs: 26/30
Other agent actions took 5.198264122009277 seconds
Total simulation time for 400 steps: 7.990367889404297 	 Other agent action time: 0 	 50.06027326106271 steps/s
Curr learning rate 0.00045454545454545455 	 Curr reward per step 0.27574133333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.72it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 167.43it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.40it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.35it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 168.65it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 167.37it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 168.48it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 159.51it/s]
--------------------------------------
| approxkl           | 0.0009917596  |
| clipfrac           | 0.19132294    |
| eplenmean          | 400           |
| eprewmean          | 116           |
| explained_variance | 0.148         |
| fps                | 1411          |
| nupdates           | 55            |
| policy_entropy     | 1.4877515     |
| policy_loss        | -0.0038554776 |
| serial_timesteps   | 22000         |
| time_elapsed       | 262           |
| time_remaining     | 0.872         |
| total_timesteps    | 660000        |
| true_eprew         | 87.2          |
| value_loss         | 46.014744     |
--------------------------------------
Current reward shaping 0.33999999999999997
Current self-play randomization 0.9359999999999999
SP envs: 27/30
Other agent actions took 5.172765731811523 seconds
Total simulation time for 400 steps: 7.971209287643433 	 Other agent action time: 0 	 50.18059187331336 steps/s
Curr learning rate 0.0004444444444444444 	 Curr reward per step 0.2770883333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.07it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.93it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.06it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 179.32it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.24it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.18it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.56it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 192.40it/s]
--------------------------------------
| approxkl           | 0.0010489383  |
| clipfrac           | 0.20096874    |
| eplenmean          | 400           |
| eprewmean          | 114           |
| explained_variance | 0.152         |
| fps                | 1421          |
| nupdates           | 56            |
| policy_entropy     | 1.4801666     |
| policy_loss        | -0.0021966095 |
| serial_timesteps   | 22400         |
| time_elapsed       | 270           |
| time_remaining     | 0.804         |
| total_timesteps    | 672000        |
| true_eprew         | 87            |
| value_loss         | 45.191967     |
--------------------------------------
Current reward shaping 0.32799999999999996
Current self-play randomization 0.9312
SP envs: 29/30
Other agent actions took 5.222041606903076 seconds
Total simulation time for 400 steps: 8.017013549804688 	 Other agent action time: 0 	 49.89389097511815 steps/s
Curr learning rate 0.00043434343434343433 	 Curr reward per step 0.3115306666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.21it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 187.29it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 185.63it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 184.79it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 188.63it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.71it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.39it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 185.38it/s]
--------------------------------------
| approxkl           | 0.0009659144  |
| clipfrac           | 0.19190623    |
| eplenmean          | 400           |
| eprewmean          | 117           |
| explained_variance | 0.169         |
| fps                | 1415          |
| nupdates           | 57            |
| policy_entropy     | 1.4840798     |
| policy_loss        | -0.0041327057 |
| serial_timesteps   | 22800         |
| time_elapsed       | 279           |
| time_remaining     | 0.733         |
| total_timesteps    | 684000        |
| true_eprew         | 89.6          |
| value_loss         | 52.26585      |
--------------------------------------
Current reward shaping 0.31599999999999995
Current self-play randomization 0.9264
SP envs: 28/30
Other agent actions took 5.201915740966797 seconds
Total simulation time for 400 steps: 8.024667978286743 	 Other agent action time: 0 	 49.846299072101864 steps/s
Curr learning rate 0.00042424242424242425 	 Curr reward per step 0.29005099999999995

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 161.65it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 167.52it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.38it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 170.42it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.50it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 169.46it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.80it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 166.64it/s]
--------------------------------------
| approxkl           | 0.00085065386 |
| clipfrac           | 0.17295834    |
| eplenmean          | 400           |
| eprewmean          | 117           |
| explained_variance | 0.2           |
| fps                | 1406          |
| nupdates           | 58            |
| policy_entropy     | 1.484802      |
| policy_loss        | -0.0033287983 |
| serial_timesteps   | 23200         |
| time_elapsed       | 287           |
| time_remaining     | 0.66          |
| total_timesteps    | 696000        |
| true_eprew         | 90.2          |
| value_loss         | 47.221397     |
--------------------------------------
Current reward shaping 0.30400000000000005
Current self-play randomization 0.9216
SP envs: 25/30
Other agent actions took 5.199946880340576 seconds
Total simulation time for 400 steps: 8.007124900817871 	 Other agent action time: 0 	 49.95550899413881 steps/s
Curr learning rate 0.0004141414141414141 	 Curr reward per step 0.25762266666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.86it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.44it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.51it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 178.69it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.26it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.03it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 177.18it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 177.10it/s]
--------------------------------------
| approxkl           | 0.0010681162  |
| clipfrac           | 0.20039582    |
| eplenmean          | 400           |
| eprewmean          | 114           |
| explained_variance | 0.199         |
| fps                | 1413          |
| nupdates           | 59            |
| policy_entropy     | 1.4554874     |
| policy_loss        | -0.0027214652 |
| serial_timesteps   | 23600         |
| time_elapsed       | 296           |
| time_remaining     | 0.585         |
| total_timesteps    | 708000        |
| true_eprew         | 89            |
| value_loss         | 40.17353      |
--------------------------------------
Current reward shaping 0.29200000000000004
Current self-play randomization 0.9168000000000001
SP envs: 27/30
Other agent actions took 5.145416975021362 seconds
Total simulation time for 400 steps: 7.888524055480957 	 Other agent action time: 0 	 50.706570352926725 steps/s
Curr learning rate 0.00040404040404040404 	 Curr reward per step 0.2801396666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.68it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.19it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.59it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.67it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.06it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.30it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.01it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 166.98it/s]
--------------------------------------
| approxkl           | 0.00094578817 |
| clipfrac           | 0.18359372    |
| eplenmean          | 400           |
| eprewmean          | 112           |
| explained_variance | 0.225         |
| fps                | 1428          |
| nupdates           | 60            |
| policy_entropy     | 1.4696081     |
| policy_loss        | -0.0027557216 |
| serial_timesteps   | 24000         |
| time_elapsed       | 304           |
| time_remaining     | 0.507         |
| total_timesteps    | 720000        |
| true_eprew         | 88.2          |
| value_loss         | 43.2684       |
--------------------------------------
Current reward shaping 0.28
Current self-play randomization 0.912
SP envs: 27/30
Other agent actions took 5.18542742729187 seconds
Total simulation time for 400 steps: 7.998310804367065 	 Other agent action time: 0 	 50.01055970238124 steps/s
Curr learning rate 0.00039393939393939396 	 Curr reward per step 0.2955933333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.93it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 170.52it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.75it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 171.76it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 169.92it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 168.24it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.11it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 169.99it/s]
-------------------------------------
| approxkl           | 0.0009199434 |
| clipfrac           | 0.18423957   |
| eplenmean          | 400          |
| eprewmean          | 111          |
| explained_variance | 0.2          |
| fps                | 1412         |
| nupdates           | 61           |
| policy_entropy     | 1.4431981    |
| policy_loss        | -0.004068342 |
| serial_timesteps   | 24400        |
| time_elapsed       | 313          |
| time_remaining     | 0.427        |
| total_timesteps    | 732000       |
| true_eprew         | 88.4         |
| value_loss         | 45.934914    |
-------------------------------------
Current reward shaping 0.268
Current self-play randomization 0.9072
SP envs: 26/30
Other agent actions took 5.184864044189453 seconds
Total simulation time for 400 steps: 7.984617710113525 	 Other agent action time: 0 	 50.096324523257955 steps/s
Curr learning rate 0.0003838383838383839 	 Curr reward per step 0.2979243333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.38it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 176.05it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.20it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.90it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.96it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 176.80it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.18it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 176.57it/s]
--------------------------------------
| approxkl           | 0.0010765989  |
| clipfrac           | 0.1993854     |
| eplenmean          | 400           |
| eprewmean          | 114           |
| explained_variance | 0.176         |
| fps                | 1417          |
| nupdates           | 62            |
| policy_entropy     | 1.4341526     |
| policy_loss        | -0.0028855517 |
| serial_timesteps   | 24800         |
| time_elapsed       | 321           |
| time_remaining     | 0.345         |
| total_timesteps    | 744000        |
| true_eprew         | 91.4          |
| value_loss         | 46.39735      |
--------------------------------------
Current reward shaping 0.256
Current self-play randomization 0.9024
SP envs: 29/30
Other agent actions took 5.16008734703064 seconds
Total simulation time for 400 steps: 7.9523046016693115 	 Other agent action time: 0 	 50.29988412617316 steps/s
Curr learning rate 0.0003737373737373737 	 Curr reward per step 0.3127946666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.17it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.58it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 186.31it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.21it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 187.84it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 188.72it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.53it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 190.05it/s]
-------------------------------------
| approxkl           | 0.0009183086 |
| clipfrac           | 0.18067706   |
| eplenmean          | 400          |
| eprewmean          | 117          |
| explained_variance | 0.274        |
| fps                | 1426         |
| nupdates           | 63           |
| policy_entropy     | 1.4317932    |
| policy_loss        | -0.004092783 |
| serial_timesteps   | 25200        |
| time_elapsed       | 329          |
| time_remaining     | 0.261        |
| total_timesteps    | 756000       |
| true_eprew         | 95.2         |
| value_loss         | 48.298958    |
-------------------------------------
Current reward shaping 0.244
Current self-play randomization 0.8976
SP envs: 29/30
Other agent actions took 5.159065008163452 seconds
Total simulation time for 400 steps: 7.926037311553955 	 Other agent action time: 0 	 50.46658049627288 steps/s
Curr learning rate 0.0003636363636363636 	 Curr reward per step 0.299574

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 163.78it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 169.81it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.88it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.67it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 172.86it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 168.40it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 175.34it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 176.25it/s]
--------------------------------------
| approxkl           | 0.000885789   |
| clipfrac           | 0.17306252    |
| eplenmean          | 400           |
| eprewmean          | 121           |
| explained_variance | 0.247         |
| fps                | 1424          |
| nupdates           | 64            |
| policy_entropy     | 1.4265354     |
| policy_loss        | -0.0019582075 |
| serial_timesteps   | 25600         |
| time_elapsed       | 338           |
| time_remaining     | 0.176         |
| total_timesteps    | 768000        |
| true_eprew         | 98.6          |
| value_loss         | 45.75057      |
--------------------------------------
Current reward shaping 0.23199999999999998
Current self-play randomization 0.8928
SP envs: 26/30
Other agent actions took 5.180501699447632 seconds
Total simulation time for 400 steps: 7.958441972732544 	 Other agent action time: 0 	 50.2610939893125 steps/s
Curr learning rate 0.00035353535353535354 	 Curr reward per step 0.2955993333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.07it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.62it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.56it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.40it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.64it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.32it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.21it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.98it/s]
--------------------------------------
| approxkl           | 0.00085470994 |
| clipfrac           | 0.17471875    |
| eplenmean          | 400           |
| eprewmean          | 120           |
| explained_variance | 0.228         |
| fps                | 1424          |
| nupdates           | 65            |
| policy_entropy     | 1.4211835     |
| policy_loss        | -0.002971003  |
| serial_timesteps   | 26000         |
| time_elapsed       | 346           |
| time_remaining     | 0.0888        |
| total_timesteps    | 780000        |
| true_eprew         | 99            |
| value_loss         | 44.016457     |
--------------------------------------
Current reward shaping 0.21999999999999997
Current self-play randomization 0.888
SP envs: 26/30
Other agent actions took 5.1856794357299805 seconds
Total simulation time for 400 steps: 7.930939197540283 	 Other agent action time: 0 	 50.43538855070995 steps/s
Curr learning rate 0.0003434343434343434 	 Curr reward per step 0.2712333333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.99it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 188.05it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 188.04it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.65it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 186.91it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.00it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 181.52it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.58it/s]
--------------------------------------
| approxkl           | 0.00091309985 |
| clipfrac           | 0.18870834    |
| eplenmean          | 400           |
| eprewmean          | 116           |
| explained_variance | 0.223         |
| fps                | 1429          |
| nupdates           | 66            |
| policy_entropy     | 1.4205501     |
| policy_loss        | -0.0032690489 |
| serial_timesteps   | 26400         |
| time_elapsed       | 355           |
| time_remaining     | 0             |
| total_timesteps    | 792000        |
| true_eprew         | 96.6          |
| value_loss         | 39.890556     |
--------------------------------------
Current reward shaping 0.20799999999999996
Current self-play randomization 0.8832
LOADING BC MODEL FROM: seed2/worker3
Loading a model without an environment, this model cannot be trained until it has a valid environment.
Loaded MediumLevelPlanner from /home/ubuntu/human_aware_rl/overcooked_ai/overcooked_ai_py/data/planners/simple_am.pkl
TOT NUM UPDATES 66
SP envs: 25/30
Other agent actions took 5.191826581954956 seconds
Total simulation time for 400 steps: 7.959539175033569 	 Other agent action time: 0 	 50.25416562489788 steps/s
Curr learning rate 0.001 	 Curr reward per step 0.29451200000000005

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.58it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.81it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.11it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.90it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.45it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.99it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 180.47it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 180.88it/s]
-------------------------------------
| approxkl           | 0.0034700935 |
| clipfrac           | 0.39584374   |
| eplenmean          | 400          |
| eprewmean          | 118          |
| explained_variance | 0.252        |
| fps                | 1422         |
| nupdates           | 1            |
| policy_entropy     | 1.421249     |
| policy_loss        | 0.00281765   |
| serial_timesteps   | 400          |
| time_elapsed       | 8.44         |
| time_remaining     | 9.14         |
| total_timesteps    | 12000        |
| true_eprew         | 100          |
| value_loss         | 43.52378     |
-------------------------------------
Current reward shaping 0.988
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6027684211730957 seconds
Total simulation time for 400 steps: 3.531431198120117 	 Other agent action time: 0 	 113.26852416463092 steps/s
Curr learning rate 0.00098989898989899 	 Curr reward per step 0.47923366666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.76it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 178.82it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 181.06it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 178.27it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 179.72it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 180.72it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 179.54it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 179.55it/s]
-------------------------------------
| approxkl           | 0.003179739  |
| clipfrac           | 0.40296873   |
| eplenmean          | 400          |
| eprewmean          | 155          |
| explained_variance | 0.236        |
| fps                | 2988         |
| nupdates           | 2            |
| policy_entropy     | 1.4284606    |
| policy_loss        | 0.0005344056 |
| serial_timesteps   | 800          |
| time_elapsed       | 12.5         |
| time_remaining     | 6.64         |
| total_timesteps    | 24000        |
| true_eprew         | 102          |
| value_loss         | 109.71875    |
-------------------------------------
Current reward shaping 0.976
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5957176685333252 seconds
Total simulation time for 400 steps: 3.5059659481048584 	 Other agent action time: 0 	 114.09123931058687 steps/s
Curr learning rate 0.0009797979797979799 	 Curr reward per step 0.5270893333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 179.09it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 188.65it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 189.34it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 189.31it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 189.38it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 188.77it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 189.80it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 188.58it/s]
-------------------------------------
| approxkl           | 0.0053082006 |
| clipfrac           | 0.48520833   |
| eplenmean          | 400          |
| eprewmean          | 173          |
| explained_variance | 0.212        |
| fps                | 3031         |
| nupdates           | 3            |
| policy_entropy     | 1.3970022    |
| policy_loss        | 0.005514257  |
| serial_timesteps   | 1200         |
| time_elapsed       | 16.4         |
| time_remaining     | 5.74         |
| total_timesteps    | 36000        |
| true_eprew         | 106          |
| value_loss         | 126.78459    |
-------------------------------------
Current reward shaping 0.964
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.608680248260498 seconds
Total simulation time for 400 steps: 3.4978466033935547 	 Other agent action time: 0 	 114.35607256531101 steps/s
Curr learning rate 0.0009696969696969698 	 Curr reward per step 0.5325863333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.98it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.22it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.35it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.78it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.52it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.13it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.79it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 185.91it/s]
-------------------------------------
| approxkl           | 0.004375701  |
| clipfrac           | 0.45555216   |
| eplenmean          | 400          |
| eprewmean          | 198          |
| explained_variance | 0.189        |
| fps                | 3024         |
| nupdates           | 4            |
| policy_entropy     | 1.3934643    |
| policy_loss        | 0.0051776045 |
| serial_timesteps   | 1600         |
| time_elapsed       | 20.4         |
| time_remaining     | 5.27         |
| total_timesteps    | 48000        |
| true_eprew         | 111          |
| value_loss         | 130.12244    |
-------------------------------------
Current reward shaping 0.952
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6071479320526123 seconds
Total simulation time for 400 steps: 3.5056910514831543 	 Other agent action time: 0 	 114.10018570540373 steps/s
Curr learning rate 0.0009595959595959597 	 Curr reward per step 0.5252226666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 161.03it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.27it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.22it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.37it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 168.41it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 187.57it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.50it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.57it/s]
--------------------------------------
| approxkl           | 0.0024899808  |
| clipfrac           | 0.36127082    |
| eplenmean          | 400           |
| eprewmean          | 209           |
| explained_variance | 0.242         |
| fps                | 3005          |
| nupdates           | 5             |
| policy_entropy     | 1.3828368     |
| policy_loss        | -0.0010161228 |
| serial_timesteps   | 2000          |
| time_elapsed       | 24.4          |
| time_remaining     | 4.96          |
| total_timesteps    | 60000         |
| true_eprew         | 114           |
| value_loss         | 123.00769     |
--------------------------------------
Current reward shaping 0.94
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6099328994750977 seconds
Total simulation time for 400 steps: 3.524940252304077 	 Other agent action time: 0 	 113.4771007078886 steps/s
Curr learning rate 0.0009494949494949496 	 Curr reward per step 0.5187116666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.56it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.71it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.41it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 176.13it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 179.86it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 180.87it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 181.16it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 180.76it/s]
-------------------------------------
| approxkl           | 0.003651391  |
| clipfrac           | 0.41784376   |
| eplenmean          | 400          |
| eprewmean          | 209          |
| explained_variance | 0.261        |
| fps                | 2996         |
| nupdates           | 6            |
| policy_entropy     | 1.3675115    |
| policy_loss        | 0.0027591116 |
| serial_timesteps   | 2400         |
| time_elapsed       | 28.4         |
| time_remaining     | 4.73         |
| total_timesteps    | 72000        |
| true_eprew         | 115          |
| value_loss         | 116.40614    |
-------------------------------------
Current reward shaping 0.928
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6069564819335938 seconds
Total simulation time for 400 steps: 3.5295193195343018 	 Other agent action time: 0 	 113.32987973353197 steps/s
Curr learning rate 0.0009393939393939395 	 Curr reward per step 0.5378346666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 179.08it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 185.27it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 186.24it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.56it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 185.74it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.52it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.74it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.88it/s]
-------------------------------------
| approxkl           | 0.0036004416 |
| clipfrac           | 0.42248964   |
| eplenmean          | 400          |
| eprewmean          | 210          |
| explained_variance | 0.25         |
| fps                | 3007         |
| nupdates           | 7            |
| policy_entropy     | 1.3562856    |
| policy_loss        | 0.0032578944 |
| serial_timesteps   | 2800         |
| time_elapsed       | 32.4         |
| time_remaining     | 4.55         |
| total_timesteps    | 84000        |
| true_eprew         | 116          |
| value_loss         | 125.331894   |
-------------------------------------
Current reward shaping 0.916
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6134018898010254 seconds
Total simulation time for 400 steps: 3.5667476654052734 	 Other agent action time: 0 	 112.14698586045049 steps/s
Curr learning rate 0.0009292929292929292 	 Curr reward per step 0.5302466666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 180.67it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.63it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 190.04it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.55it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.45it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.02it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 186.45it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 176.61it/s]
-------------------------------------
| approxkl           | 0.002295543  |
| clipfrac           | 0.33407292   |
| eplenmean          | 400          |
| eprewmean          | 212          |
| explained_variance | 0.27         |
| fps                | 2978         |
| nupdates           | 8            |
| policy_entropy     | 1.3536506    |
| policy_loss        | 0.0003857889 |
| serial_timesteps   | 3200         |
| time_elapsed       | 36.4         |
| time_remaining     | 4.4          |
| total_timesteps    | 96000        |
| true_eprew         | 118          |
| value_loss         | 118.212265   |
-------------------------------------
Current reward shaping 0.904
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6109001636505127 seconds
Total simulation time for 400 steps: 3.5320980548858643 	 Other agent action time: 0 	 113.24713917460187 steps/s
Curr learning rate 0.0009191919191919192 	 Curr reward per step 0.5260473333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.63it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.08it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.52it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 173.41it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 171.46it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 166.75it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.67it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.20it/s]
-------------------------------------
| approxkl           | 0.002243063  |
| clipfrac           | 0.31751046   |
| eplenmean          | 400          |
| eprewmean          | 213          |
| explained_variance | 0.27         |
| fps                | 2977         |
| nupdates           | 9            |
| policy_entropy     | 1.3397962    |
| policy_loss        | 0.0022183114 |
| serial_timesteps   | 3600         |
| time_elapsed       | 40.4         |
| time_remaining     | 4.27         |
| total_timesteps    | 108000       |
| true_eprew         | 118          |
| value_loss         | 117.53203    |
-------------------------------------
Current reward shaping 0.892
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6176648139953613 seconds
Total simulation time for 400 steps: 3.5502851009368896 	 Other agent action time: 0 	 112.66700803674709 steps/s
Curr learning rate 0.0009090909090909091 	 Curr reward per step 0.5125853333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 180.98it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 189.35it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 187.72it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.76it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 188.59it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 189.88it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.38it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 188.18it/s]
-------------------------------------
| approxkl           | 0.0026869955 |
| clipfrac           | 0.34347913   |
| eplenmean          | 400          |
| eprewmean          | 209          |
| explained_variance | 0.308        |
| fps                | 2996         |
| nupdates           | 10           |
| policy_entropy     | 1.3388848    |
| policy_loss        | 0.001235567  |
| serial_timesteps   | 4000         |
| time_elapsed       | 44.4         |
| time_remaining     | 4.15         |
| total_timesteps    | 120000       |
| true_eprew         | 118          |
| value_loss         | 110.174255   |
-------------------------------------
Current reward shaping 0.88
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6173825263977051 seconds
Total simulation time for 400 steps: 3.5548627376556396 	 Other agent action time: 0 	 112.5219254636515 steps/s
Curr learning rate 0.000898989898989899 	 Curr reward per step 0.5509133333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.17it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 171.51it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.41it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 171.11it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 173.97it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.95it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 173.27it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 173.99it/s]
-------------------------------------
| approxkl           | 0.0023062176 |
| clipfrac           | 0.3115       |
| eplenmean          | 400          |
| eprewmean          | 212          |
| explained_variance | 0.27         |
| fps                | 2965         |
| nupdates           | 11           |
| policy_entropy     | 1.3209037    |
| policy_loss        | 0.0014944149 |
| serial_timesteps   | 4400         |
| time_elapsed       | 48.5         |
| time_remaining     | 4.04         |
| total_timesteps    | 132000       |
| true_eprew         | 120          |
| value_loss         | 121.41113    |
-------------------------------------
Current reward shaping 0.868
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6150088310241699 seconds
Total simulation time for 400 steps: 3.5698509216308594 	 Other agent action time: 0 	 112.04949696254067 steps/s
Curr learning rate 0.0008888888888888889 	 Curr reward per step 0.5333593333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.06it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 178.14it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 184.32it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 183.68it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 185.47it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.60it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 186.79it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.46it/s]
---------------------------------------
| approxkl           | 0.0017381081   |
| clipfrac           | 0.27339584     |
| eplenmean          | 400            |
| eprewmean          | 212            |
| explained_variance | 0.304          |
| fps                | 2973           |
| nupdates           | 12             |
| policy_entropy     | 1.2980044      |
| policy_loss        | -0.00014956128 |
| serial_timesteps   | 4800           |
| time_elapsed       | 52.5           |
| time_remaining     | 3.94           |
| total_timesteps    | 144000         |
| true_eprew         | 121            |
| value_loss         | 112.37675      |
---------------------------------------
Current reward shaping 0.856
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6121218204498291 seconds
Total simulation time for 400 steps: 3.541304111480713 	 Other agent action time: 0 	 112.95273927568718 steps/s
Curr learning rate 0.0008787878787878789 	 Curr reward per step 0.5459060000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.06it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 179.29it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.39it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.61it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.88it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.77it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 183.99it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 179.75it/s]
--------------------------------------
| approxkl           | 0.002211197   |
| clipfrac           | 0.31482288    |
| eplenmean          | 400           |
| eprewmean          | 217           |
| explained_variance | 0.326         |
| fps                | 2992          |
| nupdates           | 13            |
| policy_entropy     | 1.2998558     |
| policy_loss        | 0.00057139125 |
| serial_timesteps   | 5200          |
| time_elapsed       | 56.5          |
| time_remaining     | 3.84          |
| total_timesteps    | 156000        |
| true_eprew         | 125           |
| value_loss         | 118.501144    |
--------------------------------------
Current reward shaping 0.844
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5992882251739502 seconds
Total simulation time for 400 steps: 3.55936336517334 	 Other agent action time: 0 	 112.37964741498656 steps/s
Curr learning rate 0.0008686868686868688 	 Curr reward per step 0.5409596666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 171.61it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 182.55it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 185.05it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.53it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 185.13it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.52it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 186.40it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 185.80it/s]
---------------------------------------
| approxkl           | 0.0018618588   |
| clipfrac           | 0.25351045     |
| eplenmean          | 400            |
| eprewmean          | 216            |
| explained_variance | 0.305          |
| fps                | 2982           |
| nupdates           | 14             |
| policy_entropy     | 1.2891871      |
| policy_loss        | -1.7317769e-05 |
| serial_timesteps   | 5600           |
| time_elapsed       | 60.6           |
| time_remaining     | 3.75           |
| total_timesteps    | 168000         |
| true_eprew         | 125            |
| value_loss         | 114.09005      |
---------------------------------------
Current reward shaping 0.832
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5914525985717773 seconds
Total simulation time for 400 steps: 3.4940121173858643 	 Other agent action time: 0 	 114.48157206142444 steps/s
Curr learning rate 0.0008585858585858587 	 Curr reward per step 0.5490053333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.49it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 182.11it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.31it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.41it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 179.30it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.61it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 179.97it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.99it/s]
-------------------------------------
| approxkl           | 0.0026862095 |
| clipfrac           | 0.31284377   |
| eplenmean          | 400          |
| eprewmean          | 218          |
| explained_variance | 0.274        |
| fps                | 3025         |
| nupdates           | 15           |
| policy_entropy     | 1.2496737    |
| policy_loss        | 0.0033473806 |
| serial_timesteps   | 6000         |
| time_elapsed       | 64.5         |
| time_remaining     | 3.66         |
| total_timesteps    | 180000       |
| true_eprew         | 126          |
| value_loss         | 112.10509    |
-------------------------------------
Current reward shaping 0.8200000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6093857288360596 seconds
Total simulation time for 400 steps: 3.5248353481292725 	 Other agent action time: 0 	 113.48047794978312 steps/s
Curr learning rate 0.0008484848484848486 	 Curr reward per step 0.5823783333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.90it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.36it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.31it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 176.70it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.62it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 180.80it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 170.60it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 180.87it/s]
-------------------------------------
| approxkl           | 0.0022652657 |
| clipfrac           | 0.315625     |
| eplenmean          | 400          |
| eprewmean          | 223          |
| explained_variance | 0.292        |
| fps                | 2995         |
| nupdates           | 16           |
| policy_entropy     | 1.2486585    |
| policy_loss        | 0.0023970788 |
| serial_timesteps   | 6400         |
| time_elapsed       | 68.5         |
| time_remaining     | 3.57         |
| total_timesteps    | 192000       |
| true_eprew         | 130          |
| value_loss         | 123.58994    |
-------------------------------------
Current reward shaping 0.808
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6177194118499756 seconds
Total simulation time for 400 steps: 3.5709121227264404 	 Other agent action time: 0 	 112.01619817364605 steps/s
Curr learning rate 0.0008383838383838385 	 Curr reward per step 0.5757846666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.69it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 183.37it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.91it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.74it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.68it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.26it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.39it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 180.41it/s]
-------------------------------------
| approxkl           | 0.0023333214 |
| clipfrac           | 0.32905215   |
| eplenmean          | 400          |
| eprewmean          | 228          |
| explained_variance | 0.333        |
| fps                | 2969         |
| nupdates           | 17           |
| policy_entropy     | 1.2442966    |
| policy_loss        | 0.0019941684 |
| serial_timesteps   | 6800         |
| time_elapsed       | 72.6         |
| time_remaining     | 3.49         |
| total_timesteps    | 204000       |
| true_eprew         | 133          |
| value_loss         | 119.58331    |
-------------------------------------
Current reward shaping 0.796
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6121015548706055 seconds
Total simulation time for 400 steps: 3.520150899887085 	 Other agent action time: 0 	 113.63149233540832 steps/s
Curr learning rate 0.0008282828282828282 	 Curr reward per step 0.5959890000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.01it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 185.91it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.00it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 183.39it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 182.90it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 181.84it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.54it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.70it/s]
-------------------------------------
| approxkl           | 0.0025281194 |
| clipfrac           | 0.32292706   |
| eplenmean          | 400          |
| eprewmean          | 232          |
| explained_variance | 0.292        |
| fps                | 3009         |
| nupdates           | 18           |
| policy_entropy     | 1.1967661    |
| policy_loss        | 0.0036089954 |
| serial_timesteps   | 7200         |
| time_elapsed       | 76.6         |
| time_remaining     | 3.4          |
| total_timesteps    | 216000       |
| true_eprew         | 137          |
| value_loss         | 122.47475    |
-------------------------------------
Current reward shaping 0.784
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6109282970428467 seconds
Total simulation time for 400 steps: 3.5434536933898926 	 Other agent action time: 0 	 112.88421822646555 steps/s
Curr learning rate 0.0008181818181818183 	 Curr reward per step 0.5739933333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 181.19it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 188.72it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 184.78it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 183.61it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.69it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.60it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.97it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.44it/s]
-------------------------------------
| approxkl           | 0.0023476263 |
| clipfrac           | 0.3100625    |
| eplenmean          | 400          |
| eprewmean          | 233          |
| explained_variance | 0.322        |
| fps                | 2997         |
| nupdates           | 19           |
| policy_entropy     | 1.1929163    |
| policy_loss        | 0.0029542863 |
| serial_timesteps   | 7600         |
| time_elapsed       | 80.6         |
| time_remaining     | 3.32         |
| total_timesteps    | 228000       |
| true_eprew         | 139          |
| value_loss         | 113.135574   |
-------------------------------------
Current reward shaping 0.772
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.610623836517334 seconds
Total simulation time for 400 steps: 3.502173662185669 	 Other agent action time: 0 	 114.2147816137605 steps/s
Curr learning rate 0.0008080808080808081 	 Curr reward per step 0.5745413333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.45it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 179.30it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.56it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.97it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 182.61it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.25it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.13it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.57it/s]
-------------------------------------
| approxkl           | 0.0016592478 |
| clipfrac           | 0.2696458    |
| eplenmean          | 400          |
| eprewmean          | 231          |
| explained_variance | 0.336        |
| fps                | 3019         |
| nupdates           | 20           |
| policy_entropy     | 1.2222288    |
| policy_loss        | 0.0009383196 |
| serial_timesteps   | 8000         |
| time_elapsed       | 84.5         |
| time_remaining     | 3.24         |
| total_timesteps    | 240000       |
| true_eprew         | 139          |
| value_loss         | 112.90521    |
-------------------------------------
Current reward shaping 0.76
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6098752021789551 seconds
Total simulation time for 400 steps: 3.552760601043701 	 Other agent action time: 0 	 112.58850367865801 steps/s
Curr learning rate 0.000797979797979798 	 Curr reward per step 0.5694966666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.77it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.44it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.56it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 182.88it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 182.77it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 180.87it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.13it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.26it/s]
-------------------------------------
| approxkl           | 0.0025050517 |
| clipfrac           | 0.30882293   |
| eplenmean          | 400          |
| eprewmean          | 232          |
| explained_variance | 0.343        |
| fps                | 2983         |
| nupdates           | 21           |
| policy_entropy     | 1.1997902    |
| policy_loss        | 0.003147519  |
| serial_timesteps   | 8400         |
| time_elapsed       | 88.6         |
| time_remaining     | 3.16         |
| total_timesteps    | 252000       |
| true_eprew         | 139          |
| value_loss         | 110.70586    |
-------------------------------------
Current reward shaping 0.748
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5998544692993164 seconds
Total simulation time for 400 steps: 3.503838300704956 	 Other agent action time: 0 	 114.16051931378279 steps/s
Curr learning rate 0.0007878787878787879 	 Curr reward per step 0.5950753333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.63it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.67it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.64it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 179.68it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.51it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.55it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 177.00it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 177.75it/s]
-------------------------------------
| approxkl           | 0.0019592145 |
| clipfrac           | 0.27655208   |
| eplenmean          | 400          |
| eprewmean          | 233          |
| explained_variance | 0.271        |
| fps                | 3017         |
| nupdates           | 22           |
| policy_entropy     | 1.15762      |
| policy_loss        | 0.0020578643 |
| serial_timesteps   | 8800         |
| time_elapsed       | 92.5         |
| time_remaining     | 3.08         |
| total_timesteps    | 264000       |
| true_eprew         | 140          |
| value_loss         | 118.13621    |
-------------------------------------
Current reward shaping 0.736
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.603463888168335 seconds
Total simulation time for 400 steps: 3.536573886871338 	 Other agent action time: 0 	 113.10381538610059 steps/s
Curr learning rate 0.0007777777777777778 	 Curr reward per step 0.5814026666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 161.48it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.98it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 169.00it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.98it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 171.57it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.63it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.44it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 170.07it/s]
-------------------------------------
| approxkl           | 0.0024115972 |
| clipfrac           | 0.31557295   |
| eplenmean          | 400          |
| eprewmean          | 234          |
| explained_variance | 0.306        |
| fps                | 2970         |
| nupdates           | 23           |
| policy_entropy     | 1.1680875    |
| policy_loss        | 0.0029974652 |
| serial_timesteps   | 9200         |
| time_elapsed       | 96.6         |
| time_remaining     | 3.01         |
| total_timesteps    | 276000       |
| true_eprew         | 142          |
| value_loss         | 110.94281    |
-------------------------------------
Current reward shaping 0.724
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.607480525970459 seconds
Total simulation time for 400 steps: 3.5550172328948975 	 Other agent action time: 0 	 112.51703544465654 steps/s
Curr learning rate 0.0007676767676767678 	 Curr reward per step 0.5964763333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 176.82it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.79it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.84it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.76it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.13it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.77it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 186.44it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 185.28it/s]
-------------------------------------
| approxkl           | 0.002980106  |
| clipfrac           | 0.32956243   |
| eplenmean          | 400          |
| eprewmean          | 236          |
| explained_variance | 0.314        |
| fps                | 2987         |
| nupdates           | 24           |
| policy_entropy     | 1.1561797    |
| policy_loss        | 0.0042685396 |
| serial_timesteps   | 9600         |
| time_elapsed       | 101          |
| time_remaining     | 2.93         |
| total_timesteps    | 288000       |
| true_eprew         | 144          |
| value_loss         | 115.083145   |
-------------------------------------
Current reward shaping 0.712
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6036107540130615 seconds
Total simulation time for 400 steps: 3.5411462783813477 	 Other agent action time: 0 	 112.95777371355565 steps/s
Curr learning rate 0.0007575757575757577 	 Curr reward per step 0.5971926666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 176.11it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.80it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 184.39it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 182.46it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 185.27it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.34it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.41it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.90it/s]
-------------------------------------
| approxkl           | 0.0016764237 |
| clipfrac           | 0.25190622   |
| eplenmean          | 400          |
| eprewmean          | 236          |
| explained_variance | 0.338        |
| fps                | 2995         |
| nupdates           | 25           |
| policy_entropy     | 1.1604183    |
| policy_loss        | 0.0013363517 |
| serial_timesteps   | 10000        |
| time_elapsed       | 105          |
| time_remaining     | 2.86         |
| total_timesteps    | 300000       |
| true_eprew         | 146          |
| value_loss         | 112.1198     |
-------------------------------------
Current reward shaping 0.7
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.604111909866333 seconds
Total simulation time for 400 steps: 3.5642151832580566 	 Other agent action time: 0 	 112.22666966879343 steps/s
Curr learning rate 0.0007474747474747475 	 Curr reward per step 0.5999916666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.52it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.59it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.52it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 182.97it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 179.17it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.70it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 181.95it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 179.32it/s]
-------------------------------------
| approxkl           | 0.0016859913 |
| clipfrac           | 0.23489586   |
| eplenmean          | 400          |
| eprewmean          | 238          |
| explained_variance | 0.324        |
| fps                | 2975         |
| nupdates           | 26           |
| policy_entropy     | 1.1275433    |
| policy_loss        | 0.0015763374 |
| serial_timesteps   | 10400        |
| time_elapsed       | 109          |
| time_remaining     | 2.79         |
| total_timesteps    | 312000       |
| true_eprew         | 148          |
| value_loss         | 110.13568    |
-------------------------------------
Current reward shaping 0.688
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6012992858886719 seconds
Total simulation time for 400 steps: 3.522080659866333 	 Other agent action time: 0 	 113.56923325406763 steps/s
Curr learning rate 0.0007373737373737374 	 Curr reward per step 0.6115813333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.56it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 170.71it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.51it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 175.76it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 179.78it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 180.73it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 180.34it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 179.95it/s]
--------------------------------------
| approxkl           | 0.0013485696  |
| clipfrac           | 0.21527085    |
| eplenmean          | 400           |
| eprewmean          | 241           |
| explained_variance | 0.323         |
| fps                | 2997          |
| nupdates           | 27            |
| policy_entropy     | 1.1324607     |
| policy_loss        | 0.00029631035 |
| serial_timesteps   | 10800         |
| time_elapsed       | 113           |
| time_remaining     | 2.71          |
| total_timesteps    | 324000        |
| true_eprew         | 151           |
| value_loss         | 109.51499     |
--------------------------------------
Current reward shaping 0.6759999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5992615222930908 seconds
Total simulation time for 400 steps: 3.503697633743286 	 Other agent action time: 0 	 114.16510264689917 steps/s
Curr learning rate 0.0007272727272727272 	 Curr reward per step 0.5845969999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.23it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 182.47it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 181.88it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.41it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.14it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.10it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.27it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.69it/s]
-------------------------------------
| approxkl           | 0.0016648769 |
| clipfrac           | 0.24194786   |
| eplenmean          | 400          |
| eprewmean          | 239          |
| explained_variance | 0.344        |
| fps                | 3023         |
| nupdates           | 28           |
| policy_entropy     | 1.1222199    |
| policy_loss        | 0.0018091401 |
| serial_timesteps   | 11200        |
| time_elapsed       | 117          |
| time_remaining     | 2.64         |
| total_timesteps    | 336000       |
| true_eprew         | 150          |
| value_loss         | 105.54067    |
-------------------------------------
Current reward shaping 0.6639999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6006309986114502 seconds
Total simulation time for 400 steps: 3.511629104614258 	 Other agent action time: 0 	 113.90724592024898 steps/s
Curr learning rate 0.0007171717171717171 	 Curr reward per step 0.5878673333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.25it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 175.09it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 171.33it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.07it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.47it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 176.80it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.69it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 180.98it/s]
-------------------------------------
| approxkl           | 0.0020276993 |
| clipfrac           | 0.2737083    |
| eplenmean          | 400          |
| eprewmean          | 238          |
| explained_variance | 0.337        |
| fps                | 2998         |
| nupdates           | 29           |
| policy_entropy     | 1.1253835    |
| policy_loss        | 0.002424874  |
| serial_timesteps   | 11600        |
| time_elapsed       | 121          |
| time_remaining     | 2.57         |
| total_timesteps    | 348000       |
| true_eprew         | 150          |
| value_loss         | 99.469864    |
-------------------------------------
Current reward shaping 0.652
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6043801307678223 seconds
Total simulation time for 400 steps: 3.5362744331359863 	 Other agent action time: 0 	 113.11339308167832 steps/s
Curr learning rate 0.0007070707070707071 	 Curr reward per step 0.5980043333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.58it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 176.56it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 181.02it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.30it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.39it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 178.06it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 176.38it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 179.57it/s]
-------------------------------------
| approxkl           | 0.0017413714 |
| clipfrac           | 0.24759372   |
| eplenmean          | 400          |
| eprewmean          | 238          |
| explained_variance | 0.357        |
| fps                | 2989         |
| nupdates           | 30           |
| policy_entropy     | 1.0937827    |
| policy_loss        | 0.0016171715 |
| serial_timesteps   | 12000        |
| time_elapsed       | 125          |
| time_remaining     | 2.49         |
| total_timesteps    | 360000       |
| true_eprew         | 151          |
| value_loss         | 101.19629    |
-------------------------------------
Current reward shaping 0.64
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.603104829788208 seconds
Total simulation time for 400 steps: 3.5273778438568115 	 Other agent action time: 0 	 113.39868245094 steps/s
Curr learning rate 0.000696969696969697 	 Curr reward per step 0.5487066666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.76it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.06it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 185.80it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 189.57it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.82it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.54it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.36it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 185.36it/s]
-------------------------------------
| approxkl           | 0.0019489279 |
| clipfrac           | 0.26333335   |
| eplenmean          | 400          |
| eprewmean          | 233          |
| explained_variance | 0.386        |
| fps                | 3007         |
| nupdates           | 31           |
| policy_entropy     | 1.127702     |
| policy_loss        | 0.0029446257 |
| serial_timesteps   | 12400        |
| time_elapsed       | 129          |
| time_remaining     | 2.42         |
| total_timesteps    | 372000       |
| true_eprew         | 149          |
| value_loss         | 84.880516    |
-------------------------------------
Current reward shaping 0.628
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6076650619506836 seconds
Total simulation time for 400 steps: 3.518787384033203 	 Other agent action time: 0 	 113.67552407827594 steps/s
Curr learning rate 0.0006868686868686869 	 Curr reward per step 0.5942890000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.65it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 178.78it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 179.40it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 179.16it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 179.84it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.78it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 179.13it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.49it/s]
-------------------------------------
| approxkl           | 0.0018470905 |
| clipfrac           | 0.2548334    |
| eplenmean          | 400          |
| eprewmean          | 233          |
| explained_variance | 0.319        |
| fps                | 3006         |
| nupdates           | 32           |
| policy_entropy     | 1.0891978    |
| policy_loss        | 0.0025305029 |
| serial_timesteps   | 12800        |
| time_elapsed       | 133          |
| time_remaining     | 2.35         |
| total_timesteps    | 384000       |
| true_eprew         | 150          |
| value_loss         | 99.193436    |
-------------------------------------
Current reward shaping 0.616
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6078214645385742 seconds
Total simulation time for 400 steps: 3.5457801818847656 	 Other agent action time: 0 	 112.81015163985131 steps/s
Curr learning rate 0.0006767676767676768 	 Curr reward per step 0.589666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.03it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.78it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.66it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 173.55it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 170.72it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.54it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.47it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 173.82it/s]
-------------------------------------
| approxkl           | 0.0013018972 |
| clipfrac           | 0.22203128   |
| eplenmean          | 400          |
| eprewmean          | 232          |
| explained_variance | 0.372        |
| fps                | 2972         |
| nupdates           | 33           |
| policy_entropy     | 1.0949116    |
| policy_loss        | 0.0012377474 |
| serial_timesteps   | 13200        |
| time_elapsed       | 137          |
| time_remaining     | 2.28         |
| total_timesteps    | 396000       |
| true_eprew         | 150          |
| value_loss         | 94.78544     |
-------------------------------------
Current reward shaping 0.604
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6043038368225098 seconds
Total simulation time for 400 steps: 3.5323963165283203 	 Other agent action time: 0 	 113.23757703187863 steps/s
Curr learning rate 0.0006666666666666668 	 Curr reward per step 0.6015233333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.91it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 188.32it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 189.64it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.97it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 186.48it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 188.58it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.72it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 185.72it/s]
--------------------------------------
| approxkl           | 0.0011284658  |
| clipfrac           | 0.20161459    |
| eplenmean          | 400           |
| eprewmean          | 236           |
| explained_variance | 0.329         |
| fps                | 3007          |
| nupdates           | 34            |
| policy_entropy     | 1.0791304     |
| policy_loss        | 0.00024806106 |
| serial_timesteps   | 13600         |
| time_elapsed       | 141           |
| time_remaining     | 2.21          |
| total_timesteps    | 408000        |
| true_eprew         | 154           |
| value_loss         | 95.96656      |
--------------------------------------
Current reward shaping 0.5920000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6152172088623047 seconds
Total simulation time for 400 steps: 3.525575876235962 	 Other agent action time: 0 	 113.45664199037326 steps/s
Curr learning rate 0.0006565656565656567 	 Curr reward per step 0.5911813333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.06it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.21it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 174.58it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 177.26it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.96it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 176.50it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.50it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 177.90it/s]
---------------------------------------
| approxkl           | 0.0010406735   |
| clipfrac           | 0.19079165     |
| eplenmean          | 400            |
| eprewmean          | 238            |
| explained_variance | 0.35           |
| fps                | 2993           |
| nupdates           | 35             |
| policy_entropy     | 1.0632687      |
| policy_loss        | -0.00025974546 |
| serial_timesteps   | 14000          |
| time_elapsed       | 145            |
| time_remaining     | 2.14           |
| total_timesteps    | 420000         |
| true_eprew         | 156            |
| value_loss         | 92.75739       |
---------------------------------------
Current reward shaping 0.5800000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6107332706451416 seconds
Total simulation time for 400 steps: 3.5232434272766113 	 Other agent action time: 0 	 113.53175227781269 steps/s
Curr learning rate 0.0006464646464646465 	 Curr reward per step 0.6050283333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.40it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 176.24it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.10it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 179.34it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.64it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 178.84it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.03it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 179.79it/s]
-------------------------------------
| approxkl           | 0.0016865261 |
| clipfrac           | 0.25026047   |
| eplenmean          | 400          |
| eprewmean          | 240          |
| explained_variance | 0.334        |
| fps                | 2997         |
| nupdates           | 36           |
| policy_entropy     | 1.05444      |
| policy_loss        | 0.0018918465 |
| serial_timesteps   | 14400        |
| time_elapsed       | 149          |
| time_remaining     | 2.06         |
| total_timesteps    | 432000       |
| true_eprew         | 158          |
| value_loss         | 95.79569     |
-------------------------------------
Current reward shaping 0.5680000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6129744052886963 seconds
Total simulation time for 400 steps: 3.5398173332214355 	 Other agent action time: 0 	 113.00018118052922 steps/s
Curr learning rate 0.0006363636363636364 	 Curr reward per step 0.5904493333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.28it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 169.56it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.75it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 175.74it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.80it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 180.77it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 180.87it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.34it/s]
--------------------------------------
| approxkl           | 0.0011879376  |
| clipfrac           | 0.20036454    |
| eplenmean          | 400           |
| eprewmean          | 239           |
| explained_variance | 0.334         |
| fps                | 2980          |
| nupdates           | 37            |
| policy_entropy     | 1.0582286     |
| policy_loss        | 0.00048832345 |
| serial_timesteps   | 14800         |
| time_elapsed       | 153           |
| time_remaining     | 1.99          |
| total_timesteps    | 444000        |
| true_eprew         | 159           |
| value_loss         | 88.41006      |
--------------------------------------
Current reward shaping 0.556
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5979058742523193 seconds
Total simulation time for 400 steps: 3.498751163482666 	 Other agent action time: 0 	 114.32650717630315 steps/s
Curr learning rate 0.0006262626262626263 	 Curr reward per step 0.5835303333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 176.55it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.24it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.35it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 176.63it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.09it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 177.57it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 177.65it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.10it/s]
-------------------------------------
| approxkl           | 0.0016679779 |
| clipfrac           | 0.23886457   |
| eplenmean          | 400          |
| eprewmean          | 238          |
| explained_variance | 0.333        |
| fps                | 3015         |
| nupdates           | 38           |
| policy_entropy     | 1.0509975    |
| policy_loss        | 0.0020375187 |
| serial_timesteps   | 15200        |
| time_elapsed       | 157          |
| time_remaining     | 1.92         |
| total_timesteps    | 456000       |
| true_eprew         | 160          |
| value_loss         | 84.21452     |
-------------------------------------
Current reward shaping 0.544
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6105468273162842 seconds
Total simulation time for 400 steps: 3.535332202911377 	 Other agent action time: 0 	 113.14353985478267 steps/s
Curr learning rate 0.0006161616161616161 	 Curr reward per step 0.590264

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.39it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.63it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.48it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 179.23it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 179.54it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 181.61it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 181.35it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.07it/s]
-------------------------------------
| approxkl           | 0.0017924656 |
| clipfrac           | 0.24499997   |
| eplenmean          | 400          |
| eprewmean          | 236          |
| explained_variance | 0.334        |
| fps                | 2994         |
| nupdates           | 39           |
| policy_entropy     | 1.0221384    |
| policy_loss        | 0.0025149777 |
| serial_timesteps   | 15600        |
| time_elapsed       | 161          |
| time_remaining     | 1.85         |
| total_timesteps    | 468000       |
| true_eprew         | 160          |
| value_loss         | 86.64973     |
-------------------------------------
Current reward shaping 0.532
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6104202270507812 seconds
Total simulation time for 400 steps: 3.4948372840881348 	 Other agent action time: 0 	 114.45454179546076 steps/s
Curr learning rate 0.0006060606060606061 	 Curr reward per step 0.58377

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.42it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.24it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 181.58it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.65it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 185.02it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.16it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.04it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.18it/s]
--------------------------------------
| approxkl           | 0.0011289933  |
| clipfrac           | 0.18228124    |
| eplenmean          | 400           |
| eprewmean          | 235           |
| explained_variance | 0.331         |
| fps                | 3029          |
| nupdates           | 40            |
| policy_entropy     | 1.0272665     |
| policy_loss        | 0.00038076582 |
| serial_timesteps   | 16000         |
| time_elapsed       | 165           |
| time_remaining     | 1.78          |
| total_timesteps    | 480000        |
| true_eprew         | 160           |
| value_loss         | 79.58894      |
--------------------------------------
Current reward shaping 0.52
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6154944896697998 seconds
Total simulation time for 400 steps: 3.5220069885253906 	 Other agent action time: 0 	 113.57160883075753 steps/s
Curr learning rate 0.000595959595959596 	 Curr reward per step 0.5882766666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 171.78it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.14it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.03it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 181.20it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.61it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.13it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 186.76it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.84it/s]
--------------------------------------
| approxkl           | 0.0012001308  |
| clipfrac           | 0.19953124    |
| eplenmean          | 400           |
| eprewmean          | 235           |
| explained_variance | 0.313         |
| fps                | 3006          |
| nupdates           | 41            |
| policy_entropy     | 1.0319556     |
| policy_loss        | 0.00078098563 |
| serial_timesteps   | 16400         |
| time_elapsed       | 169           |
| time_remaining     | 1.71          |
| total_timesteps    | 492000        |
| true_eprew         | 161           |
| value_loss         | 83.13076      |
--------------------------------------
Current reward shaping 0.508
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6129951477050781 seconds
Total simulation time for 400 steps: 3.538877487182617 	 Other agent action time: 0 	 113.03019147985519 steps/s
Curr learning rate 0.0005858585858585859 	 Curr reward per step 0.5882766666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.10it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.39it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.91it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 181.56it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.95it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 180.09it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 179.84it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 180.89it/s]
------------------------------------
| approxkl           | 0.001281007 |
| clipfrac           | 0.20134373  |
| eplenmean          | 400         |
| eprewmean          | 236         |
| explained_variance | 0.301       |
| fps                | 2990        |
| nupdates           | 42          |
| policy_entropy     | 1.0025622   |
| policy_loss        | 0.001211579 |
| serial_timesteps   | 16800       |
| time_elapsed       | 173         |
| time_remaining     | 1.64        |
| total_timesteps    | 504000      |
| true_eprew         | 162         |
| value_loss         | 80.900246   |
------------------------------------
Current reward shaping 0.496
Current self-play randomization 0.9984
SP envs: 30/30
Other agent actions took 0.6055676937103271 seconds
Total simulation time for 400 steps: 3.5201244354248047 	 Other agent action time: 0 	 113.63234662235128 steps/s
Curr learning rate 0.0005757575757575758 	 Curr reward per step 0.5850333333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.06it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 185.30it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.58it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 183.97it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.15it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.62it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.42it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.38it/s]
--------------------------------------
| approxkl           | 0.001230003   |
| clipfrac           | 0.19907291    |
| eplenmean          | 400           |
| eprewmean          | 234           |
| explained_variance | 0.326         |
| fps                | 3011          |
| nupdates           | 43            |
| policy_entropy     | 1.0181164     |
| policy_loss        | 0.00052582775 |
| serial_timesteps   | 17200         |
| time_elapsed       | 177           |
| time_remaining     | 1.57          |
| total_timesteps    | 516000        |
| true_eprew         | 162           |
| value_loss         | 78.079605     |
--------------------------------------
Current reward shaping 0.484
Current self-play randomization 0.9936
SP envs: 30/30
Other agent actions took 0.6133608818054199 seconds
Total simulation time for 400 steps: 3.5251221656799316 	 Other agent action time: 0 	 113.47124473992444 steps/s
Curr learning rate 0.0005656565656565657 	 Curr reward per step 0.6097526666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.03it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.34it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.54it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 175.83it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.00it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.57it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 177.09it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 176.29it/s]
-------------------------------------
| approxkl           | 0.0008609517 |
| clipfrac           | 0.15107295   |
| eplenmean          | 400          |
| eprewmean          | 238          |
| explained_variance | 0.314        |
| fps                | 2992         |
| nupdates           | 44           |
| policy_entropy     | 0.9779323    |
| policy_loss        | 0.0001271792 |
| serial_timesteps   | 17600        |
| time_elapsed       | 181          |
| time_remaining     | 1.51         |
| total_timesteps    | 528000       |
| true_eprew         | 166          |
| value_loss         | 82.796394    |
-------------------------------------
Current reward shaping 0.472
Current self-play randomization 0.9888
SP envs: 30/30
Other agent actions took 0.6037657260894775 seconds
Total simulation time for 400 steps: 3.538684606552124 	 Other agent action time: 0 	 113.03635233820268 steps/s
Curr learning rate 0.0005555555555555557 	 Curr reward per step 0.59906

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.02it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 186.42it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 184.77it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.29it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.76it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 187.77it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 189.14it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 189.41it/s]
--------------------------------------
| approxkl           | 0.0008532049  |
| clipfrac           | 0.14979164    |
| eplenmean          | 400           |
| eprewmean          | 239           |
| explained_variance | 0.332         |
| fps                | 3002          |
| nupdates           | 45            |
| policy_entropy     | 0.9827404     |
| policy_loss        | 0.00066288275 |
| serial_timesteps   | 18000         |
| time_elapsed       | 185           |
| time_remaining     | 1.44          |
| total_timesteps    | 540000        |
| true_eprew         | 167           |
| value_loss         | 82.29137      |
--------------------------------------
Current reward shaping 0.45999999999999996
Current self-play randomization 0.984
SP envs: 29/30
Other agent actions took 5.1904871463775635 seconds
Total simulation time for 400 steps: 7.964057445526123 	 Other agent action time: 0 	 50.225654791666955 steps/s
Curr learning rate 0.0005454545454545455 	 Curr reward per step 0.5942416666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.89it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.69it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.71it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 182.59it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.12it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 181.21it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 181.31it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.73it/s]
--------------------------------------
| approxkl           | 0.0012066144  |
| clipfrac           | 0.2020416     |
| eplenmean          | 400           |
| eprewmean          | 240           |
| explained_variance | 0.336         |
| fps                | 1422          |
| nupdates           | 46            |
| policy_entropy     | 0.99861366    |
| policy_loss        | 0.00056744204 |
| serial_timesteps   | 18400         |
| time_elapsed       | 193           |
| time_remaining     | 1.4           |
| total_timesteps    | 552000        |
| true_eprew         | 170           |
| value_loss         | 77.05099      |
--------------------------------------
Current reward shaping 0.44799999999999995
Current self-play randomization 0.9792
SP envs: 30/30
Other agent actions took 0.6113896369934082 seconds
Total simulation time for 400 steps: 3.5528640747070312 	 Other agent action time: 0 	 112.58522464949182 steps/s
Curr learning rate 0.0005353535353535353 	 Curr reward per step 0.592128

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.74it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 186.16it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 187.83it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 189.20it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 188.48it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 189.70it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.66it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.54it/s]
--------------------------------------
| approxkl           | 0.0011897299  |
| clipfrac           | 0.1933229     |
| eplenmean          | 400           |
| eprewmean          | 239           |
| explained_variance | 0.34          |
| fps                | 2993          |
| nupdates           | 47            |
| policy_entropy     | 0.9975084     |
| policy_loss        | 0.00051513175 |
| serial_timesteps   | 18800         |
| time_elapsed       | 197           |
| time_remaining     | 1.33          |
| total_timesteps    | 564000        |
| true_eprew         | 170           |
| value_loss         | 74.32721      |
--------------------------------------
Current reward shaping 0.43600000000000005
Current self-play randomization 0.9744
SP envs: 29/30
Other agent actions took 5.151520252227783 seconds
Total simulation time for 400 steps: 7.913481712341309 	 Other agent action time: 0 	 50.5466512137367 steps/s
Curr learning rate 0.0005252525252525252 	 Curr reward per step 0.5802346666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.24it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 178.18it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 177.02it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 178.43it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 179.01it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.10it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.93it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 187.10it/s]
-------------------------------------
| approxkl           | 0.0013588851 |
| clipfrac           | 0.21495834   |
| eplenmean          | 400          |
| eprewmean          | 237          |
| explained_variance | 0.302        |
| fps                | 1430         |
| nupdates           | 48           |
| policy_entropy     | 0.95137465   |
| policy_loss        | 0.0020266064 |
| serial_timesteps   | 19200        |
| time_elapsed       | 205          |
| time_remaining     | 1.28         |
| total_timesteps    | 576000       |
| true_eprew         | 170          |
| value_loss         | 74.5729      |
-------------------------------------
Current reward shaping 0.42400000000000004
Current self-play randomization 0.9696
SP envs: 29/30
Other agent actions took 5.128988742828369 seconds
Total simulation time for 400 steps: 7.912500619888306 	 Other agent action time: 0 	 50.552918630374336 steps/s
Curr learning rate 0.0005151515151515151 	 Curr reward per step 0.571356

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 179.06it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 183.81it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 181.96it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.74it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.07it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 188.01it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.26it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 188.22it/s]
--------------------------------------
| approxkl           | 0.0007868358  |
| clipfrac           | 0.1381354     |
| eplenmean          | 400           |
| eprewmean          | 235           |
| explained_variance | 0.289         |
| fps                | 1432          |
| nupdates           | 49            |
| policy_entropy     | 0.9795971     |
| policy_loss        | -0.0002012999 |
| serial_timesteps   | 19600         |
| time_elapsed       | 214           |
| time_remaining     | 1.24          |
| total_timesteps    | 588000        |
| true_eprew         | 169           |
| value_loss         | 71.02206      |
--------------------------------------
Current reward shaping 0.41200000000000003
Current self-play randomization 0.9648
SP envs: 30/30
Other agent actions took 0.5970065593719482 seconds
Total simulation time for 400 steps: 3.528540849685669 	 Other agent action time: 0 	 113.36130628490045 steps/s
Curr learning rate 0.000505050505050505 	 Curr reward per step 0.5810636666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.43it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.94it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.78it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 179.10it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.00it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 180.66it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.34it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 178.02it/s]
--------------------------------------
| approxkl           | 0.0009322879  |
| clipfrac           | 0.16370833    |
| eplenmean          | 400           |
| eprewmean          | 231           |
| explained_variance | 0.342         |
| fps                | 2997          |
| nupdates           | 50            |
| policy_entropy     | 0.96726143    |
| policy_loss        | 0.00032418806 |
| serial_timesteps   | 20000         |
| time_elapsed       | 218           |
| time_remaining     | 1.16          |
| total_timesteps    | 600000        |
| true_eprew         | 168           |
| value_loss         | 68.95189      |
--------------------------------------
Current reward shaping 0.4
Current self-play randomization 0.96
../../thesis_data/dr_ppo/ppo_bc_train_simple/
PPO agent on index 0:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑1O 
X ←0    X 
X D X S X 


Timestep: 2
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 3
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →0↑1O 
X       X 
X D X S X 


Timestep: 4
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O ←0  ↑1O 
X       X 
X D X S X 


Timestep: 5
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O ←o  →1O 
X       X 
X D X S X 


Timestep: 6
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →o→1O 
X       X 
X D X S X 


Timestep: 7
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o→1O 
X       X 
X D X S X 


Timestep: 8
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑0  O 
X     ↓1X 
X D X S X 


Timestep: 9
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →0O 
X     ↓1X 
X D X S X 


Timestep: 10
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X     ↓1X 
X D X S X 


Timestep: 11
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X   ←1  X 
X D X S X 


Timestep: 12
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X   ←1  X 
X D X S X 


Timestep: 13
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ←1    X 
X D X S X 


Timestep: 14
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑0  O 
X ←1    X 
X D X S X 


Timestep: 15
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O       O 
X ←1↓0  X 
X D X S X 


Timestep: 16
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑0  O 
X ←1    X 
X D X S X 


Timestep: 17
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←0  O 
X ↑1    X 
X D X S X 


Timestep: 18
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑0  O 
X ↑1    X 
X D X S X 


Timestep: 19
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →0O 
X ↑1    X 
X D X S X 


Timestep: 20
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
X ↓1    X 
X D X S X 


Timestep: 21
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
X ↓1    X 
X D X S X 


Timestep: 22
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ↓1    X 
X D X S X 


Timestep: 23
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 24
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 3 
X X ø1X X 
O   ↑0  O 
X   →d  X 
X D X S X 


Timestep: 25
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø2X X 
O   ↑0  O 
X   ↑d  X 
X D X S X 


Timestep: 26
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø3X X 
O ←0    O 
X   ↑d  X 
X D X S X 


Timestep: 27
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø4X X 
O ↑0    O 
X   ↑d  X 
X D X S X 


Timestep: 28
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø5X X 
O   →0  O 
X   ↑d  X 
X D X S X 


Timestep: 29
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X ø6X X 
O   ↓0  O 
X   ↑d  X 
X D X S X 


Timestep: 30
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X ø7X X 
O       O 
X ←d↓0  X 
X D X S X 


Timestep: 31
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X ø8X X 
O       O 
X →d↓0  X 
X D X S X 


Timestep: 32
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø9X X 
O       O 
X →d←0  X 
X D X S X 


Timestep: 33
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø10X X 
O       O 
X →d↓0  X 
X D X S X 


Timestep: 34
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø11X X 
O       O 
X →d↓0  X 
X D X S X 


Timestep: 35
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X ø12X X 
O ↑d    O 
X     →0X 
X D X S X 


Timestep: 36
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø13X X 
O ↑d    O 
X     →0X 
X D X S X 


Timestep: 37
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø14X X 
O ↑d    O 
X     →0X 
X D X S X 


Timestep: 38
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø15X X 
O   →d  O 
X     →0X 
X D X S X 


Timestep: 39
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø16X X 
O   →d  O 
X     →0X 
X D X S X 


Timestep: 40
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø17X X 
O   →d  O 
X     →0X 
X D X S X 


Timestep: 41
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø18X X 
O   →d  O 
X     →0X 
X D X S X 


Timestep: 42
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø19X X 
O   ↑d  O 
X     →0X 
X D X S X 


Timestep: 43
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d  O 
X     →0X 
X D X S X 


Timestep: 44
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d  O 
X     →0X 
X D X S X 


Timestep: 45
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d  O 
X     →0X 
X D X S X 


Timestep: 46
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d  O 
X   ←0  X 
X D X S X 


Timestep: 47
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 5 
X X P X X 
O   ↑s  O 
X   ←0  X 
X D X S X 


Timestep: 48
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →sO 
X   ←0  X 
X D X S X 


Timestep: 49
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X P X X 
O       O 
X   ←0↓sX 
X D X S X 


Timestep: 50
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑0  O 
X     ↓sX 
X D X S X 


Timestep: 51
Joint action taken: ('stay', 'interact') 	 Reward: 20 + shape * 0 
X X P X X 
O   ↑0  O 
X     ↓1X 
X D X S X 


Timestep: 52
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     →0O 
X     ↓1X 
X D X S X 


Timestep: 53
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X   ←1  X 
X D X S X 


Timestep: 54
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑oO 
X ←1    X 
X D X S X 


Timestep: 55
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X ↓1    X 
X D X S X 


Timestep: 56
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   →1  X 
X D X S X 


Timestep: 57
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   →1  X 
X D X S X 


Timestep: 58
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑1  X 
X D X S X 


Timestep: 59
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑0  O 
X   ↑1  X 
X D X S X 


Timestep: 60
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↓0  O 
X   ↑1  X 
X D X S X 


Timestep: 61
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →0O 
X   ↑1  X 
X D X S X 


Timestep: 62
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X   ↓1  X 
X D X S X 


Timestep: 63
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X ←1    X 
X D X S X 


Timestep: 64
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ↑1↑o  O 
X       X 
X D X S X 


Timestep: 65
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O ↑1↑0  O 
X       X 
X D X S X 


Timestep: 66
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑1  →0O 
X       X 
X D X S X 


Timestep: 67
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑1  →oO 
X       X 
X D X S X 


Timestep: 68
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ↓1    X 
X D X S X 


Timestep: 69
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ↓1    X 
X D X S X 


Timestep: 70
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 71
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑d↑o  O 
X       X 
X D X S X 


Timestep: 72
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø1X X 
O ↑d↑0  O 
X       X 
X D X S X 


Timestep: 73
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø2X X 
O ↑d←0  O 
X       X 
X D X S X 


Timestep: 74
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø3X X 
O ↑d←0  O 
X       X 
X D X S X 


Timestep: 75
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø4X X 
O →d←0  O 
X       X 
X D X S X 


Timestep: 76
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X ø5X X 
O ↑d  →0O 
X       X 
X D X S X 


Timestep: 77
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø6X X 
O ↑d  →oO 
X       X 
X D X S X 


Timestep: 78
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø7X X 
O ↑d←o  O 
X       X 
X D X S X 


Timestep: 79
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø8X X 
O ↑d    O 
X   ↓o  X 
X D X S X 


Timestep: 80
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø9X X 
O   →d  O 
X     →oX 
X D X S X 


Timestep: 81
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø10X X 
O   →d  O 
X     ↓oX 
X D X S X 


Timestep: 82
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø11X X 
O   →d  O 
X     →oX 
X D X S X 


Timestep: 83
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø12X X 
O   →d  O 
X   ←o  X 
X D X S X 


Timestep: 84
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø13X X 
O   ↑d  O 
X   ←o  X 
X D X S X 


Timestep: 85
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø14X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 86
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø15X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 87
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø16X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 88
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø17X X 
O     →dO 
X   ↑o  X 
X D X S X 


Timestep: 89
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø18X X 
O   ↑o→dO 
X       X 
X D X S X 


Timestep: 90
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø19X X 
O   ↑o→dO 
X       X 
X D X S X 


Timestep: 91
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
X   ↓o  X 
X D X S X 


Timestep: 92
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
X   ↓o  X 
X D X S X 


Timestep: 93
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
X   ↓o  X 
X D X S X 


Timestep: 94
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
X   ↓o  X 
X D X S X 


Timestep: 95
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
X   ↓o  X 
X D X S X 


Timestep: 96
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
X     →oX 
X D X S X 


Timestep: 97
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
X     →oX 
X D X S X 


Timestep: 98
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑1O 
X     →oX 
X D X S X 


Timestep: 99
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↓1O 
X     ↓oX 
X D X S X 


tot rew 100 tot rew shaped 85
PPO agent on index 1:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X   →0  X 
X D X S X 


Timestep: 2
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X   →0  X 
X D X S X 


Timestep: 3
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X ←0    X 
X D X S X 


Timestep: 4
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X ←0    X 
X D X S X 


Timestep: 5
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   →0  X 
X D X S X 


Timestep: 6
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X ←0    X 
X D X S X 


Timestep: 7
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑1  O 
X ←0    X 
X D X S X 


Timestep: 8
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X ←0    X 
X D X S X 


Timestep: 9
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X ←0    X 
X D X S X 


Timestep: 10
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X   →0  X 
X D X S X 


Timestep: 11
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ←0    X 
X D X S X 


Timestep: 12
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑1  O 
X ←0    X 
X D X S X 


Timestep: 13
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0  →1O 
X       X 
X D X S X 


Timestep: 14
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0  →oO 
X       X 
X D X S X 


Timestep: 15
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0  →oO 
X       X 
X D X S X 


Timestep: 16
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0←o  O 
X       X 
X D X S X 


Timestep: 17
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O →0↑o  O 
X       X 
X D X S X 


Timestep: 18
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø1X X 
O →0↑1  O 
X       X 
X D X S X 


Timestep: 19
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø2X X 
O →0  →1O 
X       X 
X D X S X 


Timestep: 20
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø3X X 
O →0  →oO 
X       X 
X D X S X 


Timestep: 21
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X ø4X X 
O   ←o  O 
X ↓0    X 
X D X S X 


Timestep: 22
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 3 
X X ø5X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 23
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X ø6X X 
O ↑d↑o  O 
X       X 
X D X S X 


Timestep: 24
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø7X X 
O ↑d↑o  O 
X       X 
X D X S X 


Timestep: 25
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø8X X 
O ↑d↑o  O 
X       X 
X D X S X 


Timestep: 26
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø9X X 
O ↑d↑o  O 
X       X 
X D X S X 


Timestep: 27
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X ø10X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 28
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X ø11X X 
O   ↓o  O 
X →d    X 
X D X S X 


Timestep: 29
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø12X X 
O ↑d  →oO 
X       X 
X D X S X 


Timestep: 30
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø13X X 
O ↑d←o  O 
X       X 
X D X S X 


Timestep: 31
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X ø14X X 
O   →d  O 
X   ↓o  X 
X D X S X 


Timestep: 32
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø15X X 
O   →d  O 
X   ↓o  X 
X D X S X 


Timestep: 33
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø16X X 
O   →d  O 
X   ↓o  X 
X D X S X 


Timestep: 34
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø17X X 
O   →d  O 
X   ↓o  X 
X D X S X 


Timestep: 35
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø18X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 36
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø19X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 37
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 38
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 5 
X X P X X 
O   ↑s  O 
X   ↑o  X 
X D X S X 


Timestep: 39
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o→sO 
X       X 
X D X S X 


Timestep: 40
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X     ↓sX 
X D X S X 


Timestep: 41
Joint action taken: ('interact', 'stay') 	 Reward: 20 + shape * 0 
X X P X X 
O   ↑o  O 
X     ↓0X 
X D X S X 


Timestep: 42
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X     ↓0X 
X D X S X 


Timestep: 43
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ←0  X 
X D X S X 


Timestep: 44
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑1  O 
X   ←0  X 
X D X S X 


Timestep: 45
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←1    O 
X   ←0  X 
X D X S X 


Timestep: 46
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o    O 
X ←0    X 
X D X S X 


Timestep: 47
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ↑0→o  O 
X       X 
X D X S X 


Timestep: 48
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ↑0↑o  O 
X       X 
X D X S X 


Timestep: 49
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O ↑0↑1  O 
X       X 
X D X S X 


Timestep: 50
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0←1  O 
X       X 
X D X S X 


Timestep: 51
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←0←1  O 
X       X 
X D X S X 


Timestep: 52
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←0←1  O 
X       X 
X D X S X 


Timestep: 53
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←0  →1O 
X       X 
X D X S X 


Timestep: 54
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←0  →oO 
X       X 
X D X S X 


Timestep: 55
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
X ↓0    X 
X D X S X 


Timestep: 56
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ↓0    X 
X D X S X 


Timestep: 57
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o  O 
X ↓0    X 
X D X S X 


Timestep: 58
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø1X X 
O   ↑1  O 
X ↓0    X 
X D X S X 


Timestep: 59
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 3 
X X ø2X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 60
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X ø3X X 
O ↑d  →oO 
X       X 
X D X S X 


Timestep: 61
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø4X X 
O ↑d←o  O 
X       X 
X D X S X 


Timestep: 62
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø5X X 
O ↑d↑o  O 
X       X 
X D X S X 


Timestep: 63
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø6X X 
O ↑d↑o  O 
X       X 
X D X S X 


Timestep: 64
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø7X X 
O →d↑o  O 
X       X 
X D X S X 


Timestep: 65
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø8X X 
O →d    O 
X   ↓o  X 
X D X S X 


Timestep: 66
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø9X X 
O   →d  O 
X     →oX 
X D X S X 


Timestep: 67
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø10X X 
O   →d  O 
X     →oX 
X D X S X 


Timestep: 68
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø11X X 
O   →d  O 
X     ↓oX 
X D X S X 


Timestep: 69
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø12X X 
O   →d  O 
X     ↓oX 
X D X S X 


Timestep: 70
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø13X X 
O   ↑d  O 
X   ←o  X 
X D X S X 


Timestep: 71
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø14X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 72
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø15X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 73
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø16X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 74
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø17X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 75
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø18X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 76
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø19X X 
O     →dO 
X   ↑o  X 
X D X S X 


Timestep: 77
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
X   ↑o  X 
X D X S X 


Timestep: 78
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o→dO 
X       X 
X D X S X 


Timestep: 79
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o→dO 
X       X 
X D X S X 


Timestep: 80
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
X   ↓o  X 
X D X S X 


Timestep: 81
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
X   ↓o  X 
X D X S X 


Timestep: 82
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
X ←o    X 
X D X S X 


Timestep: 83
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
X   →o  X 
X D X S X 


Timestep: 84
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑0O 
X   ↓o  X 
X D X S X 


Timestep: 85
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →0O 
X   ↓1  X 
X D XoS X 


Timestep: 86
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →0O 
X   ↓1  X 
X D XoS X 


Timestep: 87
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →0O 
X   ↓o  X 
X D X S X 


Timestep: 88
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →0O 
X   ↓1  X 
X D XoS X 


Timestep: 89
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →oO 
X   ↓1  X 
X D XoS X 


Timestep: 90
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ←o  O 
X   ↓o  X 
X D X S X 


Timestep: 91
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ←o  O 
X   ↓1  X 
X D XoS X 


Timestep: 92
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ←o  O 
X   ↓1  X 
X D XoS X 


Timestep: 93
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ←o  O 
X ←1    X 
X D XoS X 


Timestep: 94
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ←o  O 
X ↓1    X 
X D XoS X 


Timestep: 95
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑o  O 
X ↓d    X 
X D XoS X 


Timestep: 96
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑o  O 
X   →d  X 
X D XoS X 


Timestep: 97
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑o  O 
X   ↑d  X 
X D XoS X 


Timestep: 98
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑d→oO 
X       X 
X D XoS X 


Timestep: 99
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 5 
X X P XdX 
O   ↑s→oO 
X       X 
X D XoS X 


tot rew 120 tot rew shaped 99
../../thesis_data/dr_ppo/ppo_bc_train_simple/
SP envs: 29/30
Other agent actions took 5.125068426132202 seconds
Total simulation time for 400 steps: 7.887337923049927 	 Other agent action time: 0 	 50.71419582912018 steps/s
Curr learning rate 0.000494949494949495 	 Curr reward per step 0.5600666666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.50it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 182.42it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.98it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 181.30it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 182.32it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.81it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 181.35it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 180.80it/s]
-------------------------------------
| approxkl           | 0.0014058409 |
| clipfrac           | 0.20277083   |
| eplenmean          | 400          |
| eprewmean          | 230          |
| explained_variance | 0.343        |
| fps                | 1436         |
| nupdates           | 51           |
| policy_entropy     | 0.98430556   |
| policy_loss        | 0.00185881   |
| serial_timesteps   | 20400        |
| time_elapsed       | 228          |
| time_remaining     | 1.12         |
| total_timesteps    | 612000       |
| true_eprew         | 169          |
| value_loss         | 65.08083     |
-------------------------------------
Current reward shaping 0.388
Current self-play randomization 0.9552
SP envs: 30/30
Other agent actions took 0.6005518436431885 seconds
Total simulation time for 400 steps: 3.5350232124328613 	 Other agent action time: 0 	 113.15342954274787 steps/s
Curr learning rate 0.0004848484848484849 	 Curr reward per step 0.5535806666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.98it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 178.53it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 179.39it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 176.72it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.83it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 177.19it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 176.86it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 179.45it/s]
---------------------------------------
| approxkl           | 0.0007749117   |
| clipfrac           | 0.14764588     |
| eplenmean          | 400            |
| eprewmean          | 227            |
| explained_variance | 0.331          |
| fps                | 2989           |
| nupdates           | 52             |
| policy_entropy     | 0.97354585     |
| policy_loss        | 0.000111360496 |
| serial_timesteps   | 20800          |
| time_elapsed       | 232            |
| time_remaining     | 1.04           |
| total_timesteps    | 624000         |
| true_eprew         | 168            |
| value_loss         | 62.886993      |
---------------------------------------
Current reward shaping 0.376
Current self-play randomization 0.9504
SP envs: 29/30
Other agent actions took 5.167299270629883 seconds
Total simulation time for 400 steps: 7.953524351119995 	 Other agent action time: 0 	 50.292170155193276 steps/s
Curr learning rate 0.0004747474747474748 	 Curr reward per step 0.5553226666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.82it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 178.71it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.92it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 177.81it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.78it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.54it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 189.90it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 189.18it/s]
--------------------------------------
| approxkl           | 0.0009627033  |
| clipfrac           | 0.16266668    |
| eplenmean          | 400           |
| eprewmean          | 224           |
| explained_variance | 0.358         |
| fps                | 1424          |
| nupdates           | 53            |
| policy_entropy     | 0.9567496     |
| policy_loss        | 0.00039905956 |
| serial_timesteps   | 21200         |
| time_elapsed       | 241           |
| time_remaining     | 0.985         |
| total_timesteps    | 636000        |
| true_eprew         | 167           |
| value_loss         | 60.839558     |
--------------------------------------
Current reward shaping 0.364
Current self-play randomization 0.9456
SP envs: 29/30
Other agent actions took 5.1400065422058105 seconds
Total simulation time for 400 steps: 7.92636775970459 	 Other agent action time: 0 	 50.464476558037944 steps/s
Curr learning rate 0.0004646464646464647 	 Curr reward per step 0.5398890000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.30it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.80it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.85it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 173.52it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.78it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 178.06it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.85it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 178.51it/s]
-------------------------------------
| approxkl           | 0.0009981138 |
| clipfrac           | 0.16688542   |
| eplenmean          | 400          |
| eprewmean          | 220          |
| explained_variance | 0.24         |
| fps                | 1426         |
| nupdates           | 54           |
| policy_entropy     | 0.9338915    |
| policy_loss        | 0.0011647053 |
| serial_timesteps   | 21600        |
| time_elapsed       | 249          |
| time_remaining     | 0.923        |
| total_timesteps    | 648000       |
| true_eprew         | 166          |
| value_loss         | 63.99761     |
-------------------------------------
Current reward shaping 0.352
Current self-play randomization 0.9408
SP envs: 29/30
Other agent actions took 5.174924850463867 seconds
Total simulation time for 400 steps: 7.9534149169921875 	 Other agent action time: 0 	 50.29286214471399 steps/s
Curr learning rate 0.00045454545454545455 	 Curr reward per step 0.5406399999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.54it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 175.03it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.75it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 178.20it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.53it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.07it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.48it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 187.66it/s]
-------------------------------------
| approxkl           | 0.0012281856 |
| clipfrac           | 0.16774999   |
| eplenmean          | 400          |
| eprewmean          | 217          |
| explained_variance | 0.173        |
| fps                | 1423         |
| nupdates           | 55           |
| policy_entropy     | 0.93321055   |
| policy_loss        | 0.002339458  |
| serial_timesteps   | 22000        |
| time_elapsed       | 258          |
| time_remaining     | 0.859        |
| total_timesteps    | 660000       |
| true_eprew         | 165          |
| value_loss         | 63.294422    |
-------------------------------------
Current reward shaping 0.33999999999999997
Current self-play randomization 0.9359999999999999
SP envs: 29/30
Other agent actions took 5.151054382324219 seconds
Total simulation time for 400 steps: 7.903059244155884 	 Other agent action time: 0 	 50.61331158510422 steps/s
Curr learning rate 0.0004444444444444444 	 Curr reward per step 0.5608099999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.44it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.57it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 179.14it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 169.31it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.47it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.52it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.74it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.35it/s]
-------------------------------------
| approxkl           | 0.0012518575 |
| clipfrac           | 0.19043753   |
| eplenmean          | 400          |
| eprewmean          | 219          |
| explained_variance | 0.281        |
| fps                | 1432         |
| nupdates           | 56           |
| policy_entropy     | 0.9270121    |
| policy_loss        | 0.001190923  |
| serial_timesteps   | 22400        |
| time_elapsed       | 266          |
| time_remaining     | 0.792        |
| total_timesteps    | 672000       |
| true_eprew         | 167          |
| value_loss         | 59.61964     |
-------------------------------------
Current reward shaping 0.32799999999999996
Current self-play randomization 0.9312
SP envs: 25/30
Other agent actions took 5.163754940032959 seconds
Total simulation time for 400 steps: 7.956362962722778 	 Other agent action time: 0 	 50.27422729129924 steps/s
Curr learning rate 0.00043434343434343433 	 Curr reward per step 0.489388

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 181.14it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 191.91it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 189.37it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 190.53it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 187.85it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 187.61it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 192.10it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 190.47it/s]
-------------------------------------
| approxkl           | 0.001093444  |
| clipfrac           | 0.17767708   |
| eplenmean          | 400          |
| eprewmean          | 212          |
| explained_variance | 0.236        |
| fps                | 1427         |
| nupdates           | 57           |
| policy_entropy     | 0.9500521    |
| policy_loss        | 0.0011587339 |
| serial_timesteps   | 22800        |
| time_elapsed       | 274          |
| time_remaining     | 0.722        |
| total_timesteps    | 684000       |
| true_eprew         | 163          |
| value_loss         | 58.091484    |
-------------------------------------
Current reward shaping 0.31599999999999995
Current self-play randomization 0.9264
SP envs: 29/30
Other agent actions took 5.180355548858643 seconds
Total simulation time for 400 steps: 7.951494216918945 	 Other agent action time: 0 	 50.305010490844886 steps/s
Curr learning rate 0.00042424242424242425 	 Curr reward per step 0.5314536666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 180.47it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 185.35it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 184.44it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.77it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.29it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.61it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.89it/s]
---------------------------------------
| approxkl           | 0.00078635116  |
| clipfrac           | 0.14565626     |
| eplenmean          | 400            |
| eprewmean          | 212            |
| explained_variance | 0.336          |
| fps                | 1426           |
| nupdates           | 58             |
| policy_entropy     | 0.92944324     |
| policy_loss        | -0.00011906129 |
| serial_timesteps   | 23200          |
| time_elapsed       | 283            |
| time_remaining     | 0.65           |
| total_timesteps    | 696000         |
| true_eprew         | 164            |
| value_loss         | 54.069378      |
---------------------------------------
Current reward shaping 0.30400000000000005
Current self-play randomization 0.9216
SP envs: 29/30
Other agent actions took 5.171833276748657 seconds
Total simulation time for 400 steps: 7.990328550338745 	 Other agent action time: 0 	 50.060519724566554 steps/s
Curr learning rate 0.0004141414141414141 	 Curr reward per step 0.5475466666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.00it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 186.17it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 185.89it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.53it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.47it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 180.37it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.01it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.09it/s]
--------------------------------------
| approxkl           | 0.0008209819  |
| clipfrac           | 0.1518125     |
| eplenmean          | 400           |
| eprewmean          | 212           |
| explained_variance | 0.164         |
| fps                | 1419          |
| nupdates           | 59            |
| policy_entropy     | 0.8750558     |
| policy_loss        | 0.00017251697 |
| serial_timesteps   | 23600         |
| time_elapsed       | 291           |
| time_remaining     | 0.576         |
| total_timesteps    | 708000        |
| true_eprew         | 165           |
| value_loss         | 61.861443     |
--------------------------------------
Current reward shaping 0.29200000000000004
Current self-play randomization 0.9168000000000001
SP envs: 26/30
Other agent actions took 5.182764053344727 seconds
Total simulation time for 400 steps: 7.956956148147583 	 Other agent action time: 0 	 50.270479383391084 steps/s
Curr learning rate 0.00040404040404040404 	 Curr reward per step 0.5058376666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.94it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 175.78it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 179.02it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 176.39it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.41it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 176.74it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 177.97it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 177.58it/s]
-------------------------------------
| approxkl           | 0.0008575573 |
| clipfrac           | 0.14471878   |
| eplenmean          | 400          |
| eprewmean          | 213          |
| explained_variance | 0.295        |
| fps                | 1421         |
| nupdates           | 60           |
| policy_entropy     | 0.929005     |
| policy_loss        | 0.0006702103 |
| serial_timesteps   | 24000        |
| time_elapsed       | 300          |
| time_remaining     | 0.5          |
| total_timesteps    | 720000       |
| true_eprew         | 167          |
| value_loss         | 53.89599     |
-------------------------------------
Current reward shaping 0.28
Current self-play randomization 0.912
SP envs: 29/30
Other agent actions took 5.133175373077393 seconds
Total simulation time for 400 steps: 7.910060882568359 	 Other agent action time: 0 	 50.56851090507939 steps/s
Curr learning rate 0.00039393939393939396 	 Curr reward per step 0.5573466666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.08it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 182.51it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.16it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 183.89it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.86it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 178.46it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 179.05it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.12it/s]
--------------------------------------
| approxkl           | 0.00074979896 |
| clipfrac           | 0.13103123    |
| eplenmean          | 400           |
| eprewmean          | 215           |
| explained_variance | 0.282         |
| fps                | 1431          |
| nupdates           | 61            |
| policy_entropy     | 0.891376      |
| policy_loss        | -9.276615e-06 |
| serial_timesteps   | 24400         |
| time_elapsed       | 308           |
| time_remaining     | 0.421         |
| total_timesteps    | 732000        |
| true_eprew         | 171           |
| value_loss         | 55.309437     |
--------------------------------------
Current reward shaping 0.268
Current self-play randomization 0.9072
SP envs: 28/30
Other agent actions took 5.166693210601807 seconds
Total simulation time for 400 steps: 7.979644060134888 	 Other agent action time: 0 	 50.12754917206651 steps/s
Curr learning rate 0.0003838383838383839 	 Curr reward per step 0.5296903333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.85it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 189.19it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 185.65it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 189.18it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 187.12it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 188.66it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.28it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 188.39it/s]
---------------------------------------
| approxkl           | 0.0007277067   |
| clipfrac           | 0.13566664     |
| eplenmean          | 400            |
| eprewmean          | 214            |
| explained_variance | 0.232          |
| fps                | 1421           |
| nupdates           | 62             |
| policy_entropy     | 0.89528674     |
| policy_loss        | -0.00021784726 |
| serial_timesteps   | 24800          |
| time_elapsed       | 317            |
| time_remaining     | 0.34           |
| total_timesteps    | 744000         |
| true_eprew         | 171            |
| value_loss         | 55.43759       |
---------------------------------------
Current reward shaping 0.256
Current self-play randomization 0.9024
SP envs: 28/30
Other agent actions took 5.172126054763794 seconds
Total simulation time for 400 steps: 7.937456369400024 	 Other agent action time: 0 	 50.39397779143133 steps/s
Curr learning rate 0.0003737373737373737 	 Curr reward per step 0.5459919999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 181.30it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 186.00it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 185.61it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 190.66it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 189.55it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 190.06it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 189.55it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 189.52it/s]
--------------------------------------
| approxkl           | 0.0006750827  |
| clipfrac           | 0.12822917    |
| eplenmean          | 400           |
| eprewmean          | 215           |
| explained_variance | 0.247         |
| fps                | 1430          |
| nupdates           | 63            |
| policy_entropy     | 0.886934      |
| policy_loss        | 0.00019464841 |
| serial_timesteps   | 25200         |
| time_elapsed       | 325           |
| time_remaining     | 0.258         |
| total_timesteps    | 756000        |
| true_eprew         | 174           |
| value_loss         | 54.919952     |
--------------------------------------
Current reward shaping 0.244
Current self-play randomization 0.8976
SP envs: 28/30
Other agent actions took 5.21705174446106 seconds
Total simulation time for 400 steps: 8.017143487930298 	 Other agent action time: 0 	 49.893082318184106 steps/s
Curr learning rate 0.0003636363636363636 	 Curr reward per step 0.5126176666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.21it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.94it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.22it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 183.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.13it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.34it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.89it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.47it/s]
---------------------------------------
| approxkl           | 0.0005955181   |
| clipfrac           | 0.10944794     |
| eplenmean          | 400            |
| eprewmean          | 211            |
| explained_variance | 0.289          |
| fps                | 1414           |
| nupdates           | 64             |
| policy_entropy     | 0.8884595      |
| policy_loss        | -1.7827386e-05 |
| serial_timesteps   | 25600          |
| time_elapsed       | 333            |
| time_remaining     | 0.174          |
| total_timesteps    | 768000         |
| true_eprew         | 172            |
| value_loss         | 50.41396       |
---------------------------------------
Current reward shaping 0.23199999999999998
Current self-play randomization 0.8928
SP envs: 29/30
Other agent actions took 5.169922351837158 seconds
Total simulation time for 400 steps: 7.968356609344482 	 Other agent action time: 0 	 50.198556566973984 steps/s
Curr learning rate 0.00035353535353535354 	 Curr reward per step 0.5244939999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.59it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 174.06it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 174.51it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.13it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 172.83it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.41it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.14it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.71it/s]
-------------------------------------
| approxkl           | 0.0005920602 |
| clipfrac           | 0.112708345  |
| eplenmean          | 400          |
| eprewmean          | 211          |
| explained_variance | 0.282        |
| fps                | 1418         |
| nupdates           | 65           |
| policy_entropy     | 0.8906719    |
| policy_loss        | 3.911796e-05 |
| serial_timesteps   | 26000        |
| time_elapsed       | 342          |
| time_remaining     | 0.0877       |
| total_timesteps    | 780000       |
| true_eprew         | 173          |
| value_loss         | 51.15535     |
-------------------------------------
Current reward shaping 0.21999999999999997
Current self-play randomization 0.888
SP envs: 27/30
Other agent actions took 5.157237768173218 seconds
Total simulation time for 400 steps: 7.888925075531006 	 Other agent action time: 0 	 50.7039927709132 steps/s
Curr learning rate 0.0003434343434343434 	 Curr reward per step 0.501015

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.20it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 176.14it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 177.33it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 172.48it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.50it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.82it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 180.58it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.64it/s]
---------------------------------------
| approxkl           | 0.0006855939   |
| clipfrac           | 0.13756248     |
| eplenmean          | 400            |
| eprewmean          | 206            |
| explained_variance | 0.362          |
| fps                | 1434           |
| nupdates           | 66             |
| policy_entropy     | 0.909042       |
| policy_loss        | -0.00013181898 |
| serial_timesteps   | 26400          |
| time_elapsed       | 350            |
| time_remaining     | 0              |
| total_timesteps    | 792000         |
| true_eprew         | 171            |
| value_loss         | 46.61767       |
---------------------------------------
Current reward shaping 0.20799999999999996
Current self-play randomization 0.8832
LOADING BC MODEL FROM: seed0/worker12
Loading a model without an environment, this model cannot be trained until it has a valid environment.
Loaded MediumLevelPlanner from /home/ubuntu/human_aware_rl/overcooked_ai/overcooked_ai_py/data/planners/simple_am.pkl
TOT NUM UPDATES 66
SP envs: 26/30
Other agent actions took 5.156816720962524 seconds
Total simulation time for 400 steps: 7.900912046432495 	 Other agent action time: 0 	 50.62706655247634 steps/s
Curr learning rate 0.001 	 Curr reward per step 0.4953453333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.42it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 186.59it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 186.21it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.48it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 186.66it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 188.43it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.18it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 188.84it/s]
------------------------------------
| approxkl           | 0.004116147 |
| clipfrac           | 0.37159368  |
| eplenmean          | 400         |
| eprewmean          | 198         |
| explained_variance | 0.254       |
| fps                | 1434        |
| nupdates           | 1           |
| policy_entropy     | 0.89701253  |
| policy_loss        | 0.008593818 |
| serial_timesteps   | 400         |
| time_elapsed       | 8.37        |
| time_remaining     | 9.06        |
| total_timesteps    | 12000       |
| true_eprew         | 167         |
| value_loss         | 51.97276    |
------------------------------------
Current reward shaping 0.988
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6020674705505371 seconds
Total simulation time for 400 steps: 3.50504207611084 	 Other agent action time: 0 	 114.12131190271931 steps/s
Curr learning rate 0.00098989898989899 	 Curr reward per step 0.8314326666666669

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 181.68it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 191.44it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 192.26it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 192.02it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 191.78it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 189.80it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 190.53it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 189.91it/s]
------------------------------------
| approxkl           | 0.005764849 |
| clipfrac           | 0.37865618  |
| eplenmean          | 400         |
| eprewmean          | 265         |
| explained_variance | 0.238       |
| fps                | 3029        |
| nupdates           | 2           |
| policy_entropy     | 0.89999217  |
| policy_loss        | 0.010427038 |
| serial_timesteps   | 800         |
| time_elapsed       | 12.3        |
| time_remaining     | 6.57        |
| total_timesteps    | 24000       |
| true_eprew         | 172         |
| value_loss         | 165.04323   |
------------------------------------
Current reward shaping 0.976
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5957024097442627 seconds
Total simulation time for 400 steps: 3.4957082271575928 	 Other agent action time: 0 	 114.42602586007168 steps/s
Curr learning rate 0.0009797979797979799 	 Curr reward per step 0.823281333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 181.02it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 189.16it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 190.52it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 190.26it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 189.87it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 188.67it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.44it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 191.46it/s]
------------------------------------
| approxkl           | 0.005580397 |
| clipfrac           | 0.40498963  |
| eplenmean          | 400         |
| eprewmean          | 287         |
| explained_variance | 0.26        |
| fps                | 3040        |
| nupdates           | 3           |
| policy_entropy     | 0.90833884  |
| policy_loss        | 0.010098855 |
| serial_timesteps   | 1200        |
| time_elapsed       | 16.3        |
| time_remaining     | 5.7         |
| total_timesteps    | 36000       |
| true_eprew         | 174         |
| value_loss         | 151.17676   |
------------------------------------
Current reward shaping 0.964
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6039543151855469 seconds
Total simulation time for 400 steps: 3.527880907058716 	 Other agent action time: 0 	 113.38251220432784 steps/s
Curr learning rate 0.0009696969696969698 	 Curr reward per step 0.8329083333333331

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.57it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.51it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.49it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 173.26it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.80it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 175.39it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 174.77it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.30it/s]
------------------------------------
| approxkl           | 0.007008534 |
| clipfrac           | 0.42857295  |
| eplenmean          | 400         |
| eprewmean          | 320         |
| explained_variance | 0.289       |
| fps                | 2990        |
| nupdates           | 4           |
| policy_entropy     | 0.8823166   |
| policy_loss        | 0.011610481 |
| serial_timesteps   | 1600        |
| time_elapsed       | 20.3        |
| time_remaining     | 5.24        |
| total_timesteps    | 48000       |
| true_eprew         | 179         |
| value_loss         | 153.7419    |
------------------------------------
Current reward shaping 0.952
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.61037278175354 seconds
Total simulation time for 400 steps: 3.4891903400421143 	 Other agent action time: 0 	 114.63977628551271 steps/s
Curr learning rate 0.0009595959595959597 	 Curr reward per step 0.8453213333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.90it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 190.13it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 188.57it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 182.51it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 188.40it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 191.80it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 189.55it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 192.07it/s]
------------------------------------
| approxkl           | 0.005130467 |
| clipfrac           | 0.37148958  |
| eplenmean          | 400         |
| eprewmean          | 334         |
| explained_variance | 0.26        |
| fps                | 3043        |
| nupdates           | 5           |
| policy_entropy     | 0.8779335   |
| policy_loss        | 0.007820132 |
| serial_timesteps   | 2000        |
| time_elapsed       | 24.2        |
| time_remaining     | 4.93        |
| total_timesteps    | 60000       |
| true_eprew         | 181         |
| value_loss         | 151.2889    |
------------------------------------
Current reward shaping 0.94
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6121206283569336 seconds
Total simulation time for 400 steps: 3.789407253265381 	 Other agent action time: 0 	 105.55740601787123 steps/s
Curr learning rate 0.0009494949494949496 	 Curr reward per step 0.8199466666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.21it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 179.36it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.12it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.22it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 185.72it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.36it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 186.17it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 185.27it/s]
-------------------------------------
| approxkl           | 0.0047469847 |
| clipfrac           | 0.35302088   |
| eplenmean          | 400          |
| eprewmean          | 334          |
| explained_variance | 0.257        |
| fps                | 2818         |
| nupdates           | 6            |
| policy_entropy     | 0.89110625   |
| policy_loss        | 0.0072005773 |
| serial_timesteps   | 2400         |
| time_elapsed       | 28.5         |
| time_remaining     | 4.75         |
| total_timesteps    | 72000        |
| true_eprew         | 181          |
| value_loss         | 139.42265    |
-------------------------------------
Current reward shaping 0.928
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6040432453155518 seconds
Total simulation time for 400 steps: 3.5295002460479736 	 Other agent action time: 0 	 113.33049217035332 steps/s
Curr learning rate 0.0009393939393939395 	 Curr reward per step 0.8242933333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 164.11it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 169.81it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.65it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 169.80it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 172.42it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.69it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 175.63it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 176.30it/s]
-------------------------------------
| approxkl           | 0.0034180996 |
| clipfrac           | 0.33785415   |
| eplenmean          | 400          |
| eprewmean          | 333          |
| explained_variance | 0.234        |
| fps                | 2980         |
| nupdates           | 7            |
| policy_entropy     | 0.8942379    |
| policy_loss        | 0.0060789553 |
| serial_timesteps   | 2800         |
| time_elapsed       | 32.5         |
| time_remaining     | 4.57         |
| total_timesteps    | 84000        |
| true_eprew         | 181          |
| value_loss         | 144.84232    |
-------------------------------------
Current reward shaping 0.916
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6124613285064697 seconds
Total simulation time for 400 steps: 3.521148920059204 	 Other agent action time: 0 	 113.5992850859811 steps/s
Curr learning rate 0.0009292929292929292 	 Curr reward per step 0.8321123333333336

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.95it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 182.26it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.30it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.86it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 185.67it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.31it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 183.56it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.44it/s]
-------------------------------------
| approxkl           | 0.0023634976 |
| clipfrac           | 0.28168756   |
| eplenmean          | 400          |
| eprewmean          | 331          |
| explained_variance | 0.266        |
| fps                | 3010         |
| nupdates           | 8            |
| policy_entropy     | 0.863849     |
| policy_loss        | 0.00382218   |
| serial_timesteps   | 3200         |
| time_elapsed       | 36.5         |
| time_remaining     | 4.41         |
| total_timesteps    | 96000        |
| true_eprew         | 182          |
| value_loss         | 142.20381    |
-------------------------------------
Current reward shaping 0.904
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.602015495300293 seconds
Total simulation time for 400 steps: 3.515141725540161 	 Other agent action time: 0 	 113.79342036018 steps/s
Curr learning rate 0.0009191919191919192 	 Curr reward per step 0.8259693333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 179.87it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.03it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.90it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 182.60it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.50it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.56it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 183.81it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.96it/s]
------------------------------------
| approxkl           | 0.006769234 |
| clipfrac           | 0.41184378  |
| eplenmean          | 400         |
| eprewmean          | 330         |
| explained_variance | 0.278       |
| fps                | 3013        |
| nupdates           | 9           |
| policy_entropy     | 0.85982215  |
| policy_loss        | 0.012148132 |
| serial_timesteps   | 3600        |
| time_elapsed       | 40.5        |
| time_remaining     | 4.27        |
| total_timesteps    | 108000      |
| true_eprew         | 182         |
| value_loss         | 136.14127   |
------------------------------------
Current reward shaping 0.892
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5981807708740234 seconds
Total simulation time for 400 steps: 3.513368844985962 	 Other agent action time: 0 	 113.8508416418767 steps/s
Curr learning rate 0.0009090909090909091 	 Curr reward per step 0.8067256666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 164.97it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.13it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.56it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.78it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.08it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 175.12it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 173.43it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 170.82it/s]
-------------------------------------
| approxkl           | 0.0020514436 |
| clipfrac           | 0.25660416   |
| eplenmean          | 400          |
| eprewmean          | 329          |
| explained_variance | 0.323        |
| fps                | 2989         |
| nupdates           | 10           |
| policy_entropy     | 0.88196784   |
| policy_loss        | 0.0036996629 |
| serial_timesteps   | 4000         |
| time_elapsed       | 44.5         |
| time_remaining     | 4.15         |
| total_timesteps    | 120000       |
| true_eprew         | 183          |
| value_loss         | 132.62398    |
-------------------------------------
Current reward shaping 0.88
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6169922351837158 seconds
Total simulation time for 400 steps: 3.540487766265869 	 Other agent action time: 0 	 112.97878326575255 steps/s
Curr learning rate 0.000898989898989899 	 Curr reward per step 0.8230333333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.98it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.35it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 185.69it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.74it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 185.73it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 177.66it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 177.14it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 179.14it/s]
-------------------------------------
| approxkl           | 0.0024731886 |
| clipfrac           | 0.26358333   |
| eplenmean          | 400          |
| eprewmean          | 328          |
| explained_variance | 0.279        |
| fps                | 2993         |
| nupdates           | 11           |
| policy_entropy     | 0.8623314    |
| policy_loss        | 0.0037955218 |
| serial_timesteps   | 4400         |
| time_elapsed       | 48.5         |
| time_remaining     | 4.04         |
| total_timesteps    | 132000       |
| true_eprew         | 184          |
| value_loss         | 130.89352    |
-------------------------------------
Current reward shaping 0.868
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6167464256286621 seconds
Total simulation time for 400 steps: 3.5311992168426514 	 Other agent action time: 0 	 113.27596531289778 steps/s
Curr learning rate 0.0008888888888888889 	 Curr reward per step 0.8300753333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.18it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.43it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 177.60it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 178.51it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.75it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 173.46it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 174.88it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.99it/s]
-------------------------------------
| approxkl           | 0.0032635122 |
| clipfrac           | 0.3044271    |
| eplenmean          | 400          |
| eprewmean          | 329          |
| explained_variance | 0.253        |
| fps                | 2989         |
| nupdates           | 12           |
| policy_entropy     | 0.8350965    |
| policy_loss        | 0.005743497  |
| serial_timesteps   | 4800         |
| time_elapsed       | 52.5         |
| time_remaining     | 3.94         |
| total_timesteps    | 144000       |
| true_eprew         | 185          |
| value_loss         | 131.64468    |
-------------------------------------
Current reward shaping 0.856
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6095097064971924 seconds
Total simulation time for 400 steps: 3.608783006668091 	 Other agent action time: 0 	 110.8406904102863 steps/s
Curr learning rate 0.0008787878787878789 	 Curr reward per step 0.77539

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.90it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.52it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.69it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.60it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.44it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.78it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.69it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.58it/s]
-------------------------------------
| approxkl           | 0.0034450404 |
| clipfrac           | 0.3307187    |
| eplenmean          | 400          |
| eprewmean          | 324          |
| explained_variance | 0.321        |
| fps                | 2940         |
| nupdates           | 13           |
| policy_entropy     | 0.88211024   |
| policy_loss        | 0.0068474645 |
| serial_timesteps   | 5200         |
| time_elapsed       | 56.6         |
| time_remaining     | 3.85         |
| total_timesteps    | 156000       |
| true_eprew         | 183          |
| value_loss         | 120.13628    |
-------------------------------------
Current reward shaping 0.844
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6169991493225098 seconds
Total simulation time for 400 steps: 3.5687830448150635 	 Other agent action time: 0 	 112.08302521531628 steps/s
Curr learning rate 0.0008686868686868688 	 Curr reward per step 0.784013

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.60it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 176.89it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 174.04it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 173.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.94it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.90it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 174.05it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 173.87it/s]
-------------------------------------
| approxkl           | 0.0031274948 |
| clipfrac           | 0.31727082   |
| eplenmean          | 400          |
| eprewmean          | 319          |
| explained_variance | 0.287        |
| fps                | 2956         |
| nupdates           | 14           |
| policy_entropy     | 0.866583     |
| policy_loss        | 0.0054953275 |
| serial_timesteps   | 5600         |
| time_elapsed       | 60.7         |
| time_remaining     | 3.76         |
| total_timesteps    | 168000       |
| true_eprew         | 182          |
| value_loss         | 115.10631    |
-------------------------------------
Current reward shaping 0.832
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.616398811340332 seconds
Total simulation time for 400 steps: 3.5483572483062744 	 Other agent action time: 0 	 112.7282209791392 steps/s
Curr learning rate 0.0008585858585858587 	 Curr reward per step 0.7894640000000002

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 179.53it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 185.07it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.60it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 183.39it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 186.15it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.59it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.16it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.38it/s]
-------------------------------------
| approxkl           | 0.0022185608 |
| clipfrac           | 0.27931255   |
| eplenmean          | 400          |
| eprewmean          | 314          |
| explained_variance | 0.284        |
| fps                | 2993         |
| nupdates           | 15           |
| policy_entropy     | 0.8755841    |
| policy_loss        | 0.004295882  |
| serial_timesteps   | 6000         |
| time_elapsed       | 64.7         |
| time_remaining     | 3.67         |
| total_timesteps    | 180000       |
| true_eprew         | 180          |
| value_loss         | 118.986855   |
-------------------------------------
Current reward shaping 0.8200000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6144089698791504 seconds
Total simulation time for 400 steps: 3.5557637214660645 	 Other agent action time: 0 	 112.49341388608279 steps/s
Curr learning rate 0.0008484848484848486 	 Curr reward per step 0.7887249999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.34it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 176.17it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 174.41it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 176.50it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.00it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 177.24it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 175.47it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.35it/s]
-------------------------------------
| approxkl           | 0.0054019853 |
| clipfrac           | 0.36718747   |
| eplenmean          | 400          |
| eprewmean          | 315          |
| explained_variance | 0.302        |
| fps                | 2970         |
| nupdates           | 16           |
| policy_entropy     | 0.8557212    |
| policy_loss        | 0.00906615   |
| serial_timesteps   | 6400         |
| time_elapsed       | 68.7         |
| time_remaining     | 3.58         |
| total_timesteps    | 192000       |
| true_eprew         | 181          |
| value_loss         | 111.142105   |
-------------------------------------
Current reward shaping 0.808
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6062514781951904 seconds
Total simulation time for 400 steps: 3.5275766849517822 	 Other agent action time: 0 	 113.39229043732823 steps/s
Curr learning rate 0.0008383838383838385 	 Curr reward per step 0.7634866666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.47it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.43it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.56it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 172.99it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 173.05it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.01it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 171.09it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 171.74it/s]
------------------------------------
| approxkl           | 0.002161048 |
| clipfrac           | 0.27778125  |
| eplenmean          | 400         |
| eprewmean          | 312         |
| explained_variance | 0.326       |
| fps                | 2983        |
| nupdates           | 17          |
| policy_entropy     | 0.86983836  |
| policy_loss        | 0.004158917 |
| serial_timesteps   | 6800        |
| time_elapsed       | 72.7        |
| time_remaining     | 3.49        |
| total_timesteps    | 204000      |
| true_eprew         | 181         |
| value_loss         | 109.548996  |
------------------------------------
Current reward shaping 0.796
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6179976463317871 seconds
Total simulation time for 400 steps: 3.569240093231201 	 Other agent action time: 0 	 112.06867275714242 steps/s
Curr learning rate 0.0008282828282828282 	 Curr reward per step 0.7509873333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.14it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.78it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.37it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 169.38it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 168.20it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.97it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 164.73it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.44it/s]
-------------------------------------
| approxkl           | 0.0024161788 |
| clipfrac           | 0.30237502   |
| eplenmean          | 400          |
| eprewmean          | 307          |
| explained_variance | 0.3          |
| fps                | 2940         |
| nupdates           | 18           |
| policy_entropy     | 0.86184967   |
| policy_loss        | 0.0044979947 |
| serial_timesteps   | 7200         |
| time_elapsed       | 76.8         |
| time_remaining     | 3.41         |
| total_timesteps    | 216000       |
| true_eprew         | 179          |
| value_loss         | 96.27574     |
-------------------------------------
Current reward shaping 0.784
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6202657222747803 seconds
Total simulation time for 400 steps: 3.5711770057678223 	 Other agent action time: 0 	 112.00788965485565 steps/s
Curr learning rate 0.0008181818181818183 	 Curr reward per step 0.7569026666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.74it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 161.81it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 163.80it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.09it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 168.78it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 170.76it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 170.03it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.20it/s]
-------------------------------------
| approxkl           | 0.004283579  |
| clipfrac           | 0.3323229    |
| eplenmean          | 400          |
| eprewmean          | 304          |
| explained_variance | 0.28         |
| fps                | 2940         |
| nupdates           | 19           |
| policy_entropy     | 0.8738055    |
| policy_loss        | 0.0076811523 |
| serial_timesteps   | 7600         |
| time_elapsed       | 80.9         |
| time_remaining     | 3.34         |
| total_timesteps    | 228000       |
| true_eprew         | 179          |
| value_loss         | 101.860886   |
-------------------------------------
Current reward shaping 0.772
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.612302303314209 seconds
Total simulation time for 400 steps: 3.5334951877593994 	 Other agent action time: 0 	 113.20236161228262 steps/s
Curr learning rate 0.0008080808080808081 	 Curr reward per step 0.7781043333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.36it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 176.30it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.80it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 175.83it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 175.85it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.10it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 175.91it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.36it/s]
-------------------------------------
| approxkl           | 0.0021452005 |
| clipfrac           | 0.26661456   |
| eplenmean          | 400          |
| eprewmean          | 304          |
| explained_variance | 0.317        |
| fps                | 2985         |
| nupdates           | 20           |
| policy_entropy     | 0.83792955   |
| policy_loss        | 0.0037779156 |
| serial_timesteps   | 8000         |
| time_elapsed       | 84.9         |
| time_remaining     | 3.26         |
| total_timesteps    | 240000       |
| true_eprew         | 180          |
| value_loss         | 98.25851     |
-------------------------------------
Current reward shaping 0.76
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6125061511993408 seconds
Total simulation time for 400 steps: 3.530344247817993 	 Other agent action time: 0 	 113.30339817348656 steps/s
Curr learning rate 0.000797979797979798 	 Curr reward per step 0.7451133333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 164.91it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 169.54it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 167.29it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.89it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 169.35it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.61it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.20it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 169.20it/s]
-------------------------------------
| approxkl           | 0.0017594036 |
| clipfrac           | 0.22091663   |
| eplenmean          | 400          |
| eprewmean          | 304          |
| explained_variance | 0.302        |
| fps                | 2974         |
| nupdates           | 21           |
| policy_entropy     | 0.8486804    |
| policy_loss        | 0.0029886318 |
| serial_timesteps   | 8400         |
| time_elapsed       | 89           |
| time_remaining     | 3.18         |
| total_timesteps    | 252000       |
| true_eprew         | 182          |
| value_loss         | 96.27549     |
-------------------------------------
Current reward shaping 0.748
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6180963516235352 seconds
Total simulation time for 400 steps: 3.5551772117614746 	 Other agent action time: 0 	 112.51197230807323 steps/s
Curr learning rate 0.0007878787878787879 	 Curr reward per step 0.773783

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.65it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.95it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.89it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 178.40it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.90it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 180.85it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 179.69it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.51it/s]
-------------------------------------
| approxkl           | 0.0023905435 |
| clipfrac           | 0.25506246   |
| eplenmean          | 400          |
| eprewmean          | 306          |
| explained_variance | 0.24         |
| fps                | 2974         |
| nupdates           | 22           |
| policy_entropy     | 0.81701773   |
| policy_loss        | 0.004737686  |
| serial_timesteps   | 8800         |
| time_elapsed       | 93           |
| time_remaining     | 3.1          |
| total_timesteps    | 264000       |
| true_eprew         | 184          |
| value_loss         | 102.838234   |
-------------------------------------
Current reward shaping 0.736
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6094658374786377 seconds
Total simulation time for 400 steps: 3.5488691329956055 	 Other agent action time: 0 	 112.71196119377878 steps/s
Curr learning rate 0.0007777777777777778 	 Curr reward per step 0.7601146666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.25it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 182.63it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 186.66it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.16it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 186.17it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.15it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 183.11it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.42it/s]
-------------------------------------
| approxkl           | 0.0019709966 |
| clipfrac           | 0.2560834    |
| eplenmean          | 400          |
| eprewmean          | 306          |
| explained_variance | 0.263        |
| fps                | 2989         |
| nupdates           | 23           |
| policy_entropy     | 0.8132936    |
| policy_loss        | 0.0035477628 |
| serial_timesteps   | 9200         |
| time_elapsed       | 97           |
| time_remaining     | 3.02         |
| total_timesteps    | 276000       |
| true_eprew         | 184          |
| value_loss         | 96.32445     |
-------------------------------------
Current reward shaping 0.724
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6087243556976318 seconds
Total simulation time for 400 steps: 3.5441205501556396 	 Other agent action time: 0 	 112.86297808984914 steps/s
Curr learning rate 0.0007676767676767678 	 Curr reward per step 0.7495850000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 171.34it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 175.19it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.81it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.06it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.82it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 187.76it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.44it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 188.49it/s]
-------------------------------------
| approxkl           | 0.0023801392 |
| clipfrac           | 0.2500729    |
| eplenmean          | 400          |
| eprewmean          | 303          |
| explained_variance | 0.318        |
| fps                | 2993         |
| nupdates           | 24           |
| policy_entropy     | 0.8027633    |
| policy_loss        | 0.0040920177 |
| serial_timesteps   | 9600         |
| time_elapsed       | 101          |
| time_remaining     | 2.95         |
| total_timesteps    | 288000       |
| true_eprew         | 184          |
| value_loss         | 97.57893     |
-------------------------------------
Current reward shaping 0.712
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6091146469116211 seconds
Total simulation time for 400 steps: 3.527116060256958 	 Other agent action time: 0 	 113.40709893477651 steps/s
Curr learning rate 0.0007575757575757577 	 Curr reward per step 0.7707713333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 179.78it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 183.31it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 184.72it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 182.33it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.79it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.71it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.66it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.99it/s]
------------------------------------
| approxkl           | 0.008903345 |
| clipfrac           | 0.3665208   |
| eplenmean          | 400         |
| eprewmean          | 303         |
| explained_variance | 0.26        |
| fps                | 3005        |
| nupdates           | 25          |
| policy_entropy     | 0.78984654  |
| policy_loss        | 0.014691068 |
| serial_timesteps   | 10000       |
| time_elapsed       | 105         |
| time_remaining     | 2.87        |
| total_timesteps    | 300000      |
| true_eprew         | 185         |
| value_loss         | 99.666885   |
------------------------------------
Current reward shaping 0.7
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6039738655090332 seconds
Total simulation time for 400 steps: 3.5123724937438965 	 Other agent action time: 0 	 113.88313759786716 steps/s
Curr learning rate 0.0007474747474747475 	 Curr reward per step 0.7482583333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.40it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 171.90it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.30it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 173.98it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.24it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 176.49it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 174.75it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.37it/s]
-------------------------------------
| approxkl           | 0.0046727685 |
| clipfrac           | 0.32220834   |
| eplenmean          | 400          |
| eprewmean          | 303          |
| explained_variance | 0.264        |
| fps                | 2999         |
| nupdates           | 26           |
| policy_entropy     | 0.80165976   |
| policy_loss        | 0.00822949   |
| serial_timesteps   | 10400        |
| time_elapsed       | 109          |
| time_remaining     | 2.8          |
| total_timesteps    | 312000       |
| true_eprew         | 185          |
| value_loss         | 93.51049     |
-------------------------------------
Current reward shaping 0.688
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6047883033752441 seconds
Total simulation time for 400 steps: 3.5613884925842285 	 Other agent action time: 0 	 112.31574450046882 steps/s
Curr learning rate 0.0007373737373737374 	 Curr reward per step 0.7708480000000002

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 180.02it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.32it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 185.58it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 184.70it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.66it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.18it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.00it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 185.24it/s]
-------------------------------------
| approxkl           | 0.0029875843 |
| clipfrac           | 0.27916664   |
| eplenmean          | 400          |
| eprewmean          | 305          |
| explained_variance | 0.25         |
| fps                | 2982         |
| nupdates           | 27           |
| policy_entropy     | 0.80376655   |
| policy_loss        | 0.0053628823 |
| serial_timesteps   | 10800        |
| time_elapsed       | 113          |
| time_remaining     | 2.72         |
| total_timesteps    | 324000       |
| true_eprew         | 188          |
| value_loss         | 94.577805    |
-------------------------------------
Current reward shaping 0.6759999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6002538204193115 seconds
Total simulation time for 400 steps: 3.507133722305298 	 Other agent action time: 0 	 114.05325022425244 steps/s
Curr learning rate 0.0007272727272727272 	 Curr reward per step 0.759291

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.46it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 174.94it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 175.37it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 175.70it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 175.60it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 176.60it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 176.16it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.74it/s]
-------------------------------------
| approxkl           | 0.001874359  |
| clipfrac           | 0.22719786   |
| eplenmean          | 400          |
| eprewmean          | 305          |
| explained_variance | 0.255        |
| fps                | 3006         |
| nupdates           | 28           |
| policy_entropy     | 0.79861605   |
| policy_loss        | 0.0031550757 |
| serial_timesteps   | 11200        |
| time_elapsed       | 117          |
| time_remaining     | 2.65         |
| total_timesteps    | 336000       |
| true_eprew         | 189          |
| value_loss         | 87.16911     |
-------------------------------------
Current reward shaping 0.6639999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.605499267578125 seconds
Total simulation time for 400 steps: 3.517401695251465 	 Other agent action time: 0 	 113.72030682193758 steps/s
Curr learning rate 0.0007171717171717171 	 Curr reward per step 0.7307459999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 179.80it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.91it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 186.72it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.11it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.68it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.69it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 186.25it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.67it/s]
-------------------------------------
| approxkl           | 0.0018239664 |
| clipfrac           | 0.2498229    |
| eplenmean          | 400          |
| eprewmean          | 302          |
| explained_variance | 0.327        |
| fps                | 3016         |
| nupdates           | 29           |
| policy_entropy     | 0.80687153   |
| policy_loss        | 0.0036328423 |
| serial_timesteps   | 11600        |
| time_elapsed       | 121          |
| time_remaining     | 2.57         |
| total_timesteps    | 348000       |
| true_eprew         | 188          |
| value_loss         | 85.285255    |
-------------------------------------
Current reward shaping 0.652
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6087701320648193 seconds
Total simulation time for 400 steps: 3.5213143825531006 	 Other agent action time: 0 	 113.59394718684085 steps/s
Curr learning rate 0.0007070707070707071 	 Curr reward per step 0.7478253333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.59it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 183.16it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.93it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 181.24it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 182.66it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.72it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.22it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.64it/s]
-------------------------------------
| approxkl           | 0.0030199587 |
| clipfrac           | 0.30504164   |
| eplenmean          | 400          |
| eprewmean          | 300          |
| explained_variance | 0.249        |
| fps                | 3009         |
| nupdates           | 30           |
| policy_entropy     | 0.7746332    |
| policy_loss        | 0.006075365  |
| serial_timesteps   | 12000        |
| time_elapsed       | 125          |
| time_remaining     | 2.5          |
| total_timesteps    | 360000       |
| true_eprew         | 188          |
| value_loss         | 88.54344     |
-------------------------------------
Current reward shaping 0.64
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5942986011505127 seconds
Total simulation time for 400 steps: 3.518735647201538 	 Other agent action time: 0 	 113.6771954773361 steps/s
Curr learning rate 0.000696969696969697 	 Curr reward per step 0.7393066666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.49it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.14it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.04it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.04it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.40it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.06it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 186.15it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 185.68it/s]
-------------------------------------
| approxkl           | 0.0020520617 |
| clipfrac           | 0.24931245   |
| eplenmean          | 400          |
| eprewmean          | 296          |
| explained_variance | 0.239        |
| fps                | 3011         |
| nupdates           | 31           |
| policy_entropy     | 0.7903166    |
| policy_loss        | 0.0030983319 |
| serial_timesteps   | 12400        |
| time_elapsed       | 129          |
| time_remaining     | 2.43         |
| total_timesteps    | 372000       |
| true_eprew         | 188          |
| value_loss         | 83.58179     |
-------------------------------------
Current reward shaping 0.628
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6096527576446533 seconds
Total simulation time for 400 steps: 3.5358612537384033 	 Other agent action time: 0 	 113.12661082984721 steps/s
Curr learning rate 0.0006868686868686869 	 Curr reward per step 0.7274436666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 179.21it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 183.14it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.74it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 184.95it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.24it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.67it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.38it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.14it/s]
-------------------------------------
| approxkl           | 0.0026843431 |
| clipfrac           | 0.2891979    |
| eplenmean          | 400          |
| eprewmean          | 296          |
| explained_variance | 0.266        |
| fps                | 2999         |
| nupdates           | 32           |
| policy_entropy     | 0.80464524   |
| policy_loss        | 0.005323353  |
| serial_timesteps   | 12800        |
| time_elapsed       | 133          |
| time_remaining     | 2.36         |
| total_timesteps    | 384000       |
| true_eprew         | 188          |
| value_loss         | 79.34447     |
-------------------------------------
Current reward shaping 0.616
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6130096912384033 seconds
Total simulation time for 400 steps: 3.579970359802246 	 Other agent action time: 0 	 111.73276865401075 steps/s
Curr learning rate 0.0006767676767676768 	 Curr reward per step 0.7479053333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.40it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 174.88it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.41it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 172.36it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 170.80it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.86it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 174.18it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 176.40it/s]
-------------------------------------
| approxkl           | 0.0014066531 |
| clipfrac           | 0.19268748   |
| eplenmean          | 400          |
| eprewmean          | 296          |
| explained_variance | 0.21         |
| fps                | 2947         |
| nupdates           | 33           |
| policy_entropy     | 0.772519     |
| policy_loss        | 0.0021851694 |
| serial_timesteps   | 13200        |
| time_elapsed       | 137          |
| time_remaining     | 2.28         |
| total_timesteps    | 396000       |
| true_eprew         | 190          |
| value_loss         | 85.18268     |
-------------------------------------
Current reward shaping 0.604
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6116492748260498 seconds
Total simulation time for 400 steps: 3.5777902603149414 	 Other agent action time: 0 	 111.80085217314814 steps/s
Curr learning rate 0.0006666666666666668 	 Curr reward per step 0.7344293333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 163.30it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 169.72it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.16it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.05it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 169.75it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 173.22it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 175.30it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 172.73it/s]
--------------------------------------
| approxkl           | 0.00094000297 |
| clipfrac           | 0.15598957    |
| eplenmean          | 400           |
| eprewmean          | 295           |
| explained_variance | 0.246         |
| fps                | 2943          |
| nupdates           | 34            |
| policy_entropy     | 0.78913987    |
| policy_loss        | 0.00041717308 |
| serial_timesteps   | 13600         |
| time_elapsed       | 141           |
| time_remaining     | 2.21          |
| total_timesteps    | 408000        |
| true_eprew         | 190           |
| value_loss         | 75.957306     |
--------------------------------------
Current reward shaping 0.5920000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6170980930328369 seconds
Total simulation time for 400 steps: 3.542168140411377 	 Other agent action time: 0 	 112.92518710123828 steps/s
Curr learning rate 0.0006565656565656567 	 Curr reward per step 0.7357826666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 171.29it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.21it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 175.74it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 176.31it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.80it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 177.71it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 176.55it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.56it/s]
-------------------------------------
| approxkl           | 0.0016023197 |
| clipfrac           | 0.20447919   |
| eplenmean          | 400          |
| eprewmean          | 296          |
| explained_variance | 0.243        |
| fps                | 2980         |
| nupdates           | 35           |
| policy_entropy     | 0.77176046   |
| policy_loss        | 0.0027322336 |
| serial_timesteps   | 14000        |
| time_elapsed       | 145          |
| time_remaining     | 2.14         |
| total_timesteps    | 420000       |
| true_eprew         | 192          |
| value_loss         | 80.460434    |
-------------------------------------
Current reward shaping 0.5800000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.611030101776123 seconds
Total simulation time for 400 steps: 3.5671300888061523 	 Other agent action time: 0 	 112.13496285297295 steps/s
Curr learning rate 0.0006464646464646465 	 Curr reward per step 0.7246883333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 176.95it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.73it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.86it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.28it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 186.31it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.32it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.18it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 185.91it/s]
-------------------------------------
| approxkl           | 0.0017166212 |
| clipfrac           | 0.21006253   |
| eplenmean          | 400          |
| eprewmean          | 294          |
| explained_variance | 0.269        |
| fps                | 2978         |
| nupdates           | 36           |
| policy_entropy     | 0.78213775   |
| policy_loss        | 0.0025116499 |
| serial_timesteps   | 14400        |
| time_elapsed       | 149          |
| time_remaining     | 2.07         |
| total_timesteps    | 432000       |
| true_eprew         | 192          |
| value_loss         | 76.20803     |
-------------------------------------
Current reward shaping 0.5680000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6052560806274414 seconds
Total simulation time for 400 steps: 3.4942400455474854 	 Other agent action time: 0 	 114.47410446506034 steps/s
Curr learning rate 0.0006363636363636364 	 Curr reward per step 0.7064400000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 179.02it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.63it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 185.28it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.14it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 186.28it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.04it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.59it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.21it/s]
-------------------------------------
| approxkl           | 0.0011051081 |
| clipfrac           | 0.17309377   |
| eplenmean          | 400          |
| eprewmean          | 290          |
| explained_variance | 0.253        |
| fps                | 3034         |
| nupdates           | 37           |
| policy_entropy     | 0.7990326    |
| policy_loss        | 0.0011512168 |
| serial_timesteps   | 14800        |
| time_elapsed       | 153          |
| time_remaining     | 2            |
| total_timesteps    | 444000       |
| true_eprew         | 191          |
| value_loss         | 75.01995     |
-------------------------------------
Current reward shaping 0.556
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.60953688621521 seconds
Total simulation time for 400 steps: 3.5731639862060547 	 Other agent action time: 0 	 111.94560382455761 steps/s
Curr learning rate 0.0006262626262626263 	 Curr reward per step 0.7117620000000002

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.00it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.69it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 181.45it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 183.15it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.53it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.76it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 183.47it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.87it/s]
--------------------------------------
| approxkl           | 0.00090672245 |
| clipfrac           | 0.15410414    |
| eplenmean          | 400           |
| eprewmean          | 286           |
| explained_variance | 0.231         |
| fps                | 2970          |
| nupdates           | 38            |
| policy_entropy     | 0.78775835    |
| policy_loss        | 0.00086661836 |
| serial_timesteps   | 15200         |
| time_elapsed       | 157           |
| time_remaining     | 1.93          |
| total_timesteps    | 456000        |
| true_eprew         | 190           |
| value_loss         | 78.14564      |
--------------------------------------
Current reward shaping 0.544
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6017920970916748 seconds
Total simulation time for 400 steps: 3.515810012817383 	 Other agent action time: 0 	 113.77179043854571 steps/s
Curr learning rate 0.0006161616161616161 	 Curr reward per step 0.6953066666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.90it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 170.82it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.10it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 170.04it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 172.98it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 175.44it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.40it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 176.14it/s]
--------------------------------------
| approxkl           | 0.000796434   |
| clipfrac           | 0.13645832    |
| eplenmean          | 400           |
| eprewmean          | 283           |
| explained_variance | 0.257         |
| fps                | 2992          |
| nupdates           | 39            |
| policy_entropy     | 0.77114856    |
| policy_loss        | 0.00067864073 |
| serial_timesteps   | 15600         |
| time_elapsed       | 161           |
| time_remaining     | 1.86          |
| total_timesteps    | 468000        |
| true_eprew         | 189           |
| value_loss         | 73.508255     |
--------------------------------------
Current reward shaping 0.532
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6147894859313965 seconds
Total simulation time for 400 steps: 3.529355764389038 	 Other agent action time: 0 	 113.33513159426235 steps/s
Curr learning rate 0.0006060606060606061 	 Curr reward per step 0.7079883333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.95it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 174.91it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 177.07it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.03it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.92it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.39it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 176.74it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.85it/s]
-------------------------------------
| approxkl           | 0.0021959317 |
| clipfrac           | 0.24194793   |
| eplenmean          | 400          |
| eprewmean          | 282          |
| explained_variance | 0.271        |
| fps                | 2989         |
| nupdates           | 40           |
| policy_entropy     | 0.79367775   |
| policy_loss        | 0.0038369936 |
| serial_timesteps   | 16000        |
| time_elapsed       | 165          |
| time_remaining     | 1.79         |
| total_timesteps    | 480000       |
| true_eprew         | 190          |
| value_loss         | 68.89252     |
-------------------------------------
Current reward shaping 0.52
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6125586032867432 seconds
Total simulation time for 400 steps: 3.540379762649536 	 Other agent action time: 0 	 112.98222982176621 steps/s
Curr learning rate 0.000595959595959596 	 Curr reward per step 0.6591666666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.30it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 176.70it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.63it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 177.13it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.22it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 173.75it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 173.58it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 177.63it/s]
-------------------------------------
| approxkl           | 0.0023541772 |
| clipfrac           | 0.25566673   |
| eplenmean          | 400          |
| eprewmean          | 276          |
| explained_variance | 0.344        |
| fps                | 2981         |
| nupdates           | 41           |
| policy_entropy     | 0.7876492    |
| policy_loss        | 0.0038442202 |
| serial_timesteps   | 16400        |
| time_elapsed       | 169          |
| time_remaining     | 1.72         |
| total_timesteps    | 492000       |
| true_eprew         | 188          |
| value_loss         | 71.101875    |
-------------------------------------
Current reward shaping 0.508
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6085295677185059 seconds
Total simulation time for 400 steps: 3.531548500061035 	 Other agent action time: 0 	 113.26476190064695 steps/s
Curr learning rate 0.0005858585858585859 	 Curr reward per step 0.7113343333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.80it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 174.27it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 174.35it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 176.08it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.18it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 177.06it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 175.90it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.28it/s]
--------------------------------------
| approxkl           | 0.0007240776  |
| clipfrac           | 0.14302084    |
| eplenmean          | 400           |
| eprewmean          | 277           |
| explained_variance | 0.296         |
| fps                | 2987          |
| nupdates           | 42            |
| policy_entropy     | 0.7619921     |
| policy_loss        | 0.00055070326 |
| serial_timesteps   | 16800         |
| time_elapsed       | 173           |
| time_remaining     | 1.65          |
| total_timesteps    | 504000        |
| true_eprew         | 190           |
| value_loss         | 73.56184      |
--------------------------------------
Current reward shaping 0.496
Current self-play randomization 0.9984
SP envs: 30/30
Other agent actions took 0.616389274597168 seconds
Total simulation time for 400 steps: 3.550757884979248 	 Other agent action time: 0 	 112.65200640463769 steps/s
Curr learning rate 0.0005757575757575758 	 Curr reward per step 0.6974533333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.48it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 178.44it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.61it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 179.77it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.95it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.20it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 179.17it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.02it/s]
-------------------------------------
| approxkl           | 0.0011401295 |
| clipfrac           | 0.17078128   |
| eplenmean          | 400          |
| eprewmean          | 277          |
| explained_variance | 0.295        |
| fps                | 2981         |
| nupdates           | 43           |
| policy_entropy     | 0.76638037   |
| policy_loss        | 0.001131495  |
| serial_timesteps   | 17200        |
| time_elapsed       | 177          |
| time_remaining     | 1.58         |
| total_timesteps    | 516000       |
| true_eprew         | 191          |
| value_loss         | 67.8093      |
-------------------------------------
Current reward shaping 0.484
Current self-play randomization 0.9936
SP envs: 30/30
Other agent actions took 0.6152918338775635 seconds
Total simulation time for 400 steps: 3.6144192218780518 	 Other agent action time: 0 	 110.66784881477032 steps/s
Curr learning rate 0.0005656565656565657 	 Curr reward per step 0.6724056666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.99it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 174.23it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 174.58it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 176.52it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.03it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 176.82it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 176.28it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 177.31it/s]
-------------------------------------
| approxkl           | 0.0010961907 |
| clipfrac           | 0.17102084   |
| eplenmean          | 400          |
| eprewmean          | 277          |
| explained_variance | 0.363        |
| fps                | 2927         |
| nupdates           | 44           |
| policy_entropy     | 0.7723657    |
| policy_loss        | 0.0013599412 |
| serial_timesteps   | 17600        |
| time_elapsed       | 181          |
| time_remaining     | 1.51         |
| total_timesteps    | 528000       |
| true_eprew         | 192          |
| value_loss         | 67.10047     |
-------------------------------------
Current reward shaping 0.472
Current self-play randomization 0.9888
SP envs: 30/30
Other agent actions took 0.6138710975646973 seconds
Total simulation time for 400 steps: 3.5527005195617676 	 Other agent action time: 0 	 112.59040771872907 steps/s
Curr learning rate 0.0005555555555555557 	 Curr reward per step 0.682542

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.72it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.03it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.47it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.44it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.22it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 175.14it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 175.86it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 177.77it/s]
-------------------------------------
| approxkl           | 0.0012491377 |
| clipfrac           | 0.17680213   |
| eplenmean          | 400          |
| eprewmean          | 275          |
| explained_variance | 0.294        |
| fps                | 2977         |
| nupdates           | 45           |
| policy_entropy     | 0.7654956    |
| policy_loss        | 0.0010497839 |
| serial_timesteps   | 18000        |
| time_elapsed       | 185          |
| time_remaining     | 1.44         |
| total_timesteps    | 540000       |
| true_eprew         | 193          |
| value_loss         | 68.29622     |
-------------------------------------
Current reward shaping 0.45999999999999996
Current self-play randomization 0.984
SP envs: 30/30
Other agent actions took 0.6035811901092529 seconds
Total simulation time for 400 steps: 3.517406702041626 	 Other agent action time: 0 	 113.72014494878457 steps/s
Curr learning rate 0.0005454545454545455 	 Curr reward per step 0.6694249999999998

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.66it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.11it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.54it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.72it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.26it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.89it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.26it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.23it/s]
-------------------------------------
| approxkl           | 0.0021687152 |
| clipfrac           | 0.23330212   |
| eplenmean          | 400          |
| eprewmean          | 271          |
| explained_variance | 0.277        |
| fps                | 3010         |
| nupdates           | 46           |
| policy_entropy     | 0.75902665   |
| policy_loss        | 0.0035577104 |
| serial_timesteps   | 18400        |
| time_elapsed       | 189          |
| time_remaining     | 1.37         |
| total_timesteps    | 552000       |
| true_eprew         | 191          |
| value_loss         | 67.44092     |
-------------------------------------
Current reward shaping 0.44799999999999995
Current self-play randomization 0.9792
SP envs: 30/30
Other agent actions took 0.6127455234527588 seconds
Total simulation time for 400 steps: 3.511517286300659 	 Other agent action time: 0 	 113.91087310334592 steps/s
Curr learning rate 0.0005353535353535353 	 Curr reward per step 0.6901040000000002

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 181.43it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 187.13it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 187.17it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.67it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 187.81it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 187.39it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.38it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.40it/s]
--------------------------------------
| approxkl           | 0.0007801244  |
| clipfrac           | 0.1276875     |
| eplenmean          | 400           |
| eprewmean          | 271           |
| explained_variance | 0.294         |
| fps                | 3022          |
| nupdates           | 47            |
| policy_entropy     | 0.7391645     |
| policy_loss        | 0.00037931107 |
| serial_timesteps   | 18800         |
| time_elapsed       | 193           |
| time_remaining     | 1.3           |
| total_timesteps    | 564000        |
| true_eprew         | 193           |
| value_loss         | 65.3445       |
--------------------------------------
Current reward shaping 0.43600000000000005
Current self-play randomization 0.9744
SP envs: 29/30
Other agent actions took 5.142074823379517 seconds
Total simulation time for 400 steps: 7.9249420166015625 	 Other agent action time: 0 	 50.47355541050775 steps/s
Curr learning rate 0.0005252525252525252 	 Curr reward per step 0.6511896666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 182.12it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 187.53it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 186.00it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 184.31it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 187.31it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 188.54it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.18it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 187.50it/s]
-------------------------------------
| approxkl           | 0.0017514419 |
| clipfrac           | 0.22652082   |
| eplenmean          | 400          |
| eprewmean          | 269          |
| explained_variance | 0.241        |
| fps                | 1431         |
| nupdates           | 48           |
| policy_entropy     | 0.7688835    |
| policy_loss        | 0.0027447247 |
| serial_timesteps   | 19200        |
| time_elapsed       | 202          |
| time_remaining     | 1.26         |
| total_timesteps    | 576000       |
| true_eprew         | 193          |
| value_loss         | 70.26143     |
-------------------------------------
Current reward shaping 0.42400000000000004
Current self-play randomization 0.9696
SP envs: 29/30
Other agent actions took 5.162242889404297 seconds
Total simulation time for 400 steps: 7.9784886837005615 	 Other agent action time: 0 	 50.1348082146396 steps/s
Curr learning rate 0.0005151515151515151 	 Curr reward per step 0.63109

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 179.33it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.79it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 184.36it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.39it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 186.38it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 187.75it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.69it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 187.28it/s]
-------------------------------------
| approxkl           | 0.0012271763 |
| clipfrac           | 0.17206252   |
| eplenmean          | 400          |
| eprewmean          | 264          |
| explained_variance | 0.319        |
| fps                | 1422         |
| nupdates           | 49           |
| policy_entropy     | 0.738955     |
| policy_loss        | 0.0016133578 |
| serial_timesteps   | 19600        |
| time_elapsed       | 210          |
| time_remaining     | 1.22         |
| total_timesteps    | 588000       |
| true_eprew         | 191          |
| value_loss         | 67.21866     |
-------------------------------------
Current reward shaping 0.41200000000000003
Current self-play randomization 0.9648
SP envs: 28/30
Other agent actions took 5.140411376953125 seconds
Total simulation time for 400 steps: 7.921768426895142 	 Other agent action time: 0 	 50.493775940478486 steps/s
Curr learning rate 0.000505050505050505 	 Curr reward per step 0.6345379999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 180.39it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 183.11it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.89it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 181.60it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.71it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.33it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.78it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.21it/s]
-------------------------------------
| approxkl           | 0.0014979532 |
| clipfrac           | 0.21371874   |
| eplenmean          | 400          |
| eprewmean          | 258          |
| explained_variance | 0.248        |
| fps                | 1430         |
| nupdates           | 50           |
| policy_entropy     | 0.7684866    |
| policy_loss        | 0.002413787  |
| serial_timesteps   | 20000        |
| time_elapsed       | 219          |
| time_remaining     | 1.17         |
| total_timesteps    | 600000       |
| true_eprew         | 188          |
| value_loss         | 68.92961     |
-------------------------------------
Current reward shaping 0.4
Current self-play randomization 0.96
../../thesis_data/dr_ppo/ppo_bc_train_simple/
PPO agent on index 0:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 


Timestep: 2
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑1O 
X   →0  X 
X D X S X 


Timestep: 3
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←1  O 
X   →0  X 
X D X S X 


Timestep: 4
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←1  O 
X   →0  X 
X D X S X 


Timestep: 5
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←1  O 
X     →0X 
X D X S X 


Timestep: 6
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←1  O 
X   ←0  X 
X D X S X 


Timestep: 7
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑1  O 
X   ←0  X 
X D X S X 


Timestep: 8
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑0→1O 
X       X 
X D X S X 


Timestep: 9
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →0→1O 
X       X 
X D X S X 


Timestep: 10
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →0→1O 
X       X 
X D X S X 


Timestep: 11
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →0→1O 
X       X 
X D X S X 


Timestep: 12
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   →0←1O 
X       X 
X D X S X 


Timestep: 13
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →0←1O 
X       X 
X D X S X 


Timestep: 14
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →0←1O 
X       X 
X D X S X 


Timestep: 15
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ←0  ←1O 
X       X 
X D X S X 


Timestep: 16
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O ←o  →1O 
X       X 
X D X S X 


Timestep: 17
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →o→1O 
X       X 
X D X S X 


Timestep: 18
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o←1O 
X       X 
X D X S X 


Timestep: 19
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑0←1O 
X       X 
X D X S X 


Timestep: 20
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →0←1O 
X       X 
X D X S X 


Timestep: 21
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←0  ←1O 
X       X 
X D X S X 


Timestep: 22
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o  ←1O 
X       X 
X D X S X 


Timestep: 23
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →o↑1O 
X       X 
X D X S X 


Timestep: 24
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o←1O 
X       X 
X D X S X 


Timestep: 25
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑0←1O 
X       X 
X D X S X 


Timestep: 26
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←0  ←1O 
X       X 
X D X S X 


Timestep: 27
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←0  ←1O 
X       X 
X D X S X 


Timestep: 28
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←o←1  O 
X       X 
X D X S X 


Timestep: 29
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O →o←1  O 
X       X 
X D X S X 


Timestep: 30
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O →o←1  O 
X       X 
X D X S X 


Timestep: 31
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←1  O 
X ↓o    X 
X D X S X 


Timestep: 32
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X   →o  X 
X D X S X 


Timestep: 33
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o→1O 
X       X 
X D X S X 


Timestep: 34
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø1X X 
O   ↑0→1O 
X       X 
X D X S X 


Timestep: 35
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø2X X 
O   →0→1O 
X       X 
X D X S X 


Timestep: 36
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø3X X 
O ←0  ↑1O 
X       X 
X D X S X 


Timestep: 37
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø4X X 
O   →0↑1O 
X       X 
X D X S X 


Timestep: 38
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø5X X 
O ←0  ↑1O 
X       X 
X D X S X 


Timestep: 39
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø6X X 
O     ↑1O 
X ↓0    X 
X D X S X 


Timestep: 40
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 3 
X X ø7X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 41
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø8X X 
O     →1O 
X   →d  X 
X D X S X 


Timestep: 42
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø9X X 
O   ↑d→1O 
X       X 
X D X S X 


Timestep: 43
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø10X X 
O   ↑d→1O 
X       X 
X D X S X 


Timestep: 44
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø11X X 
O   ↑d→oO 
X       X 
X D X S X 


Timestep: 45
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø12X X 
O   ↑d→oO 
X       X 
X D X S X 


Timestep: 46
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø13X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 47
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø14X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 48
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø15X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 49
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø16X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 50
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø17X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 51
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø18X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 52
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø19X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 53
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 54
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 5 
X X P X X 
O   ↑s↑oO 
X       X 
X D X S X 


Timestep: 55
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O ←s←o  O 
X       X 
X D X S X 


Timestep: 56
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O →s←o  O 
X       X 
X D X S X 


Timestep: 57
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X ↓s    X 
X D X S X 


Timestep: 58
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   →s  X 
X D X S X 


Timestep: 59
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑s  X 
X D X S X 


Timestep: 60
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑s  X 
X D X S X 


Timestep: 61
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s→oO 
X       X 
X D X S X 


Timestep: 62
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s→oO 
X       X 
X D X S X 


Timestep: 63
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ←s  →oO 
X       X 
X D X S X 


Timestep: 64
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X ↓s    X 
X D X S X 


Timestep: 65
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   →s  X 
X D X S X 


Timestep: 66
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   ↑s  X 
X D X S X 


Timestep: 67
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   ↑s  X 
X D X S X 


Timestep: 68
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑s  X 
X D X S X 


Timestep: 69
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑s  X 
X D X S X 


Timestep: 70
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑s  X 
X D X S X 


Timestep: 71
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑s  X 
X D X S X 


Timestep: 72
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑1  O 
X   ↑s  X 
X D X S X 


Timestep: 73
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑s→1O 
X       X 
X D X S X 


Timestep: 74
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s→1O 
X       X 
X D X S X 


Timestep: 75
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s→1O 
X       X 
X D X S X 


Timestep: 76
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s→1O 
X       X 
X D X S X 


Timestep: 77
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s←1O 
X       X 
X D X S X 


Timestep: 78
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←s  ←1O 
X       X 
X D X S X 


Timestep: 79
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ↑s  ←1O 
X       X 
X D X S X 


Timestep: 80
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s←1O 
X       X 
X D X S X 


Timestep: 81
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s→1O 
X       X 
X D X S X 


Timestep: 82
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑s←1O 
X       X 
X D X S X 


Timestep: 83
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s←1O 
X       X 
X D X S X 


Timestep: 84
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑s←1O 
X       X 
X D X S X 


Timestep: 85
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s←1O 
X       X 
X D X S X 


Timestep: 86
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s→1O 
X       X 
X D X S X 


Timestep: 87
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s→1O 
X       X 
X D X S X 


Timestep: 88
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s→1O 
X       X 
X D X S X 


Timestep: 89
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s→1O 
X       X 
X D X S X 


Timestep: 90
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s→oO 
X       X 
X D X S X 


Timestep: 91
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 92
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 93
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 94
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 95
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X   ↓s  X 
X D X S X 


Timestep: 96
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X   ↑s  X 
X D X S X 


Timestep: 97
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X   ↑s  X 
X D X S X 


Timestep: 98
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X   ↑s  X 
X D X S X 


Timestep: 99
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X     →sX 
X D X S X 


tot rew 60 tot rew shaped 76
PPO agent on index 1:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←1  O 
X ↑0    X 
X D X S X 


Timestep: 2
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O ←1    O 
X   →0  X 
X D X S X 


Timestep: 3
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O ←o    O 
X   →0  X 
X D X S X 


Timestep: 4
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   →o  O 
X   →0  X 
X D X S X 


Timestep: 5
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →o  O 
X   ↑0  X 
X D X S X 


Timestep: 6
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →o  O 
X   ↑0  X 
X D X S X 


Timestep: 7
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑0  X 
X D X S X 


Timestep: 8
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑1  O 
X ←0    X 
X D X S X 


Timestep: 9
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X ←0    X 
X D X S X 


Timestep: 10
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X ←0    X 
X D X S X 


Timestep: 11
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X ←0    X 
X D X S X 


Timestep: 12
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X ←0    X 
X D X S X 


Timestep: 13
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ←0    X 
X D X S X 


Timestep: 14
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O ↑0↑1  O 
X       X 
X D X S X 


Timestep: 15
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←0  →1O 
X       X 
X D X S X 


Timestep: 16
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←0  →oO 
X       X 
X D X S X 


Timestep: 17
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←0  →oO 
X       X 
X D X S X 


Timestep: 18
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←0←o  O 
X       X 
X D X S X 


Timestep: 19
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←o↑o  O 
X       X 
X D X S X 


Timestep: 20
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 3 
X X ø1X X 
O ←o↑1  O 
X       X 
X D X S X 


Timestep: 21
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø2X X 
O   →o→1O 
X       X 
X D X S X 


Timestep: 22
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X ø3X X 
O   ↑o→oO 
X       X 
X D X S X 


Timestep: 23
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø4X X 
O   ↑o←oO 
X       X 
X D X S X 


Timestep: 24
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø5X X 
O   ↑o←oO 
X       X 
X D X S X 


Timestep: 25
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø6X X 
O   ↑o  O 
X     ↓oX 
X D X S X 


Timestep: 26
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X ø7X X 
O       O 
X   ↓o↓oX 
X D X S X 


Timestep: 27
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø8X X 
O       O 
X   ↓0↓oX 
X D XoS X 


Timestep: 28
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø9X X 
O       O 
X   ↓0←oX 
X D XoS X 


Timestep: 29
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X ø10X X 
O       O 
X   ↓0←oX 
X D XoS X 


Timestep: 30
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X ø11X X 
O       O 
X   →0↓oX 
X D XoS X 


Timestep: 31
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø12X X 
O       O 
X   →0←oX 
X D XoS X 


Timestep: 32
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø13X X 
O       O 
X   →0←oX 
X D XoS X 


Timestep: 33
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø14X X 
O   ↑0  O 
X   ←o  X 
X D XoS X 


Timestep: 34
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø15X X 
O   ↑0  O 
X   ↓o  X 
X D XoS X 


Timestep: 35
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø16X X 
O   ↑0  O 
X   ↓o  X 
X D XoS X 


Timestep: 36
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø17X X 
O   ↑0  O 
X   ↓o  X 
X D XoS X 


Timestep: 37
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X ø18X X 
O       O 
X ←o↓0  X 
X D XoS X 


Timestep: 38
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø19X X 
O ↑o    O 
X   ↓0  X 
X D XoS X 


Timestep: 39
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑o    O 
X   ↓0  X 
X D XoS X 


Timestep: 40
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ↓o↓0  X 
X D XoS X 


Timestep: 41
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ↓o↓0  X 
X D XoS X 


Timestep: 42
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑0  O 
X ↓o    X 
X D XoS X 


Timestep: 43
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑0  O 
X ←o    X 
X D XoS X 


Timestep: 44
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑o↑0  O 
X       X 
X D XoS X 


Timestep: 45
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑o    O 
X   ↓0  X 
X D XoS X 


Timestep: 46
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑o↑0  O 
X       X 
X D XoS X 


Timestep: 47
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑o↑0  O 
X       X 
X D XoS X 


Timestep: 48
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑0  O 
X ↓o    X 
X D XoS X 


Timestep: 49
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑0  O 
X ←o    X 
X D XoS X 


Timestep: 50
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
Xo←1↓0  X 
X D XoS X 


Timestep: 51
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ←o↓0  X 
X D XoS X 


Timestep: 52
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ←o↓0  X 
X D XoS X 


Timestep: 53
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
Xo←1↓0  X 
X D XoS X 


Timestep: 54
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑0  O 
Xo  →1  X 
X D XoS X 


Timestep: 55
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑0  O 
Xo  ↓1  X 
X D XoS X 


Timestep: 56
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑0  O 
Xo←1    X 
X D XoS X 


Timestep: 57
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑0  O 
X ←o    X 
X D XoS X 


Timestep: 58
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑0  O 
X   →o  X 
X D XoS X 


Timestep: 59
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ←0    O 
X   ↓o  X 
X D XoS X 


Timestep: 60
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ←0    O 
X   ↓o  X 
X D XoS X 


Timestep: 61
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ←0    O 
X   ↓o  X 
X D XoS X 


Timestep: 62
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ←0    O 
X   ↓o  X 
X D XoS X 


Timestep: 63
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ←o    O 
X   ↓o  X 
X D XoS X 


Timestep: 64
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ↓o↓o  X 
X D XoS X 


Timestep: 65
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ↓o↓o  X 
X D XoS X 


Timestep: 66
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ↓o↓o  X 
X D XoS X 


Timestep: 67
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ↓o↓o  X 
X D XoS X 


Timestep: 68
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ↓o↓o  X 
X D XoS X 


Timestep: 69
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ↓o↓o  X 
X D XoS X 


Timestep: 70
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑o    O 
X   ↓o  X 
X D XoS X 


Timestep: 71
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑o    O 
X   ↓o  X 
X D XoS X 


Timestep: 72
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑o    O 
X   ↓o  X 
X D XoS X 


Timestep: 73
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑o    O 
X   ↓o  X 
X D XoS X 


Timestep: 74
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O ↑0    O 
X   ↓o  X 
X D XoS X 


Timestep: 75
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O       O 
X ↓0↓o  X 
X D XoS X 


Timestep: 76
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O       O 
X ↓0↓o  X 
X D XoS X 


Timestep: 77
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O ↑0    O 
X   ↓o  X 
X D XoS X 


Timestep: 78
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O ↑0↑o  O 
X       X 
X D XoS X 


Timestep: 79
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O ↑0    O 
X   ↓o  X 
X D XoS X 


Timestep: 80
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O ↑0    O 
X   ↓o  X 
X D XoS X 


Timestep: 81
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑o    O 
X   ↓o  X 
X D XoS X 


Timestep: 82
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ↓o↓o  X 
X D XoS X 


Timestep: 83
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ↓o↓o  X 
X D XoS X 


Timestep: 84
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ↓o↓o  X 
X D XoS X 


Timestep: 85
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ↓o↓o  X 
X D XoS X 


Timestep: 86
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑o    O 
X   ↓o  X 
X D XoS X 


Timestep: 87
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑o    O 
X   ↓o  X 
X D XoS X 


Timestep: 88
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑o    O 
X   ↓o  X 
X D XoS X 


Timestep: 89
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑o    O 
X   ↓o  X 
X D XoS X 


Timestep: 90
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O ↑0    O 
X   ↓o  X 
X D XoS X 


Timestep: 91
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O   →0  O 
X   ↓o  X 
X D XoS X 


Timestep: 92
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O   →0  O 
X   ↓o  X 
X D XoS X 


Timestep: 93
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O   →0  O 
X   ↓o  X 
X D XoS X 


Timestep: 94
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O   →0  O 
X   ↑o  X 
X D XoS X 


Timestep: 95
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O   ↑0  O 
X   ↓o  X 
X D XoS X 


Timestep: 96
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O   ↑0  O 
X   ↓o  X 
X D XoS X 


Timestep: 97
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O   ↑0  O 
X   ↓o  X 
X D XoS X 


Timestep: 98
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O   ↑0  O 
X   ↓o  X 
X D XoS X 


Timestep: 99
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X Xoø20X X 
O       O 
X ←o↓0  X 
X D XoS X 


tot rew 40 tot rew shaped 54
../../thesis_data/dr_ppo/ppo_bc_train_simple/
SP envs: 28/30
Other agent actions took 5.1327738761901855 seconds
Total simulation time for 400 steps: 7.885074853897095 	 Other agent action time: 0 	 50.72875114207765 steps/s
Curr learning rate 0.000494949494949495 	 Curr reward per step 0.6418666666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.26it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 178.60it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 177.45it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 177.44it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.74it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 178.65it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 177.20it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 176.57it/s]
-------------------------------------
| approxkl           | 0.0021147332 |
| clipfrac           | 0.24957287   |
| eplenmean          | 400          |
| eprewmean          | 254          |
| explained_variance | 0.217        |
| fps                | 1434         |
| nupdates           | 51           |
| policy_entropy     | 0.7499527    |
| policy_loss        | 0.004048979  |
| serial_timesteps   | 20400        |
| time_elapsed       | 229          |
| time_remaining     | 1.12         |
| total_timesteps    | 612000       |
| true_eprew         | 186          |
| value_loss         | 65.71287     |
-------------------------------------
Current reward shaping 0.388
Current self-play randomization 0.9552
SP envs: 29/30
Other agent actions took 5.160009384155273 seconds
Total simulation time for 400 steps: 7.943337917327881 	 Other agent action time: 0 	 50.35666418363314 steps/s
Curr learning rate 0.0004848484848484849 	 Curr reward per step 0.6306820000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 171.32it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 171.98it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.86it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 178.40it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.64it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.89it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.84it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 180.13it/s]
-------------------------------------
| approxkl           | 0.0020721864 |
| clipfrac           | 0.2400521    |
| eplenmean          | 400          |
| eprewmean          | 255          |
| explained_variance | 0.337        |
| fps                | 1425         |
| nupdates           | 52           |
| policy_entropy     | 0.7590467    |
| policy_loss        | 0.003400601  |
| serial_timesteps   | 20800        |
| time_elapsed       | 238          |
| time_remaining     | 1.07         |
| total_timesteps    | 624000       |
| true_eprew         | 189          |
| value_loss         | 60.908943    |
-------------------------------------
Current reward shaping 0.376
Current self-play randomization 0.9504
SP envs: 30/30
Other agent actions took 0.6028127670288086 seconds
Total simulation time for 400 steps: 3.5107405185699463 	 Other agent action time: 0 	 113.93607641584822 steps/s
Curr learning rate 0.0004747474747474748 	 Curr reward per step 0.6660039999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.57it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.57it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.67it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 176.05it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.85it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.90it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 173.98it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.55it/s]
-------------------------------------
| approxkl           | 0.001126786  |
| clipfrac           | 0.18004167   |
| eplenmean          | 400          |
| eprewmean          | 257          |
| explained_variance | 0.287        |
| fps                | 3001         |
| nupdates           | 53           |
| policy_entropy     | 0.7485671    |
| policy_loss        | 0.0013146631 |
| serial_timesteps   | 21200        |
| time_elapsed       | 242          |
| time_remaining     | 0.988        |
| total_timesteps    | 636000       |
| true_eprew         | 192          |
| value_loss         | 60.98684     |
-------------------------------------
Current reward shaping 0.364
Current self-play randomization 0.9456
SP envs: 30/30
Other agent actions took 0.6026034355163574 seconds
Total simulation time for 400 steps: 3.512169122695923 	 Other agent action time: 0 	 113.88973196511735 steps/s
Curr learning rate 0.0004646464646464647 	 Curr reward per step 0.6617676666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.73it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.47it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 185.00it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 184.67it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.55it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.34it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.52it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.85it/s]
--------------------------------------
| approxkl           | 0.00084615184 |
| clipfrac           | 0.13826041    |
| eplenmean          | 400           |
| eprewmean          | 263           |
| explained_variance | 0.248         |
| fps                | 3017          |
| nupdates           | 54            |
| policy_entropy     | 0.74310815    |
| policy_loss        | 0.0009804895  |
| serial_timesteps   | 21600         |
| time_elapsed       | 246           |
| time_remaining     | 0.91          |
| total_timesteps    | 648000        |
| true_eprew         | 197           |
| value_loss         | 67.32087      |
--------------------------------------
Current reward shaping 0.352
Current self-play randomization 0.9408
SP envs: 26/30
Other agent actions took 5.137601613998413 seconds
Total simulation time for 400 steps: 7.901330947875977 	 Other agent action time: 0 	 50.62438247919832 steps/s
Curr learning rate 0.00045454545454545455 	 Curr reward per step 0.569704

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.71it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.15it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 175.72it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.83it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 175.21it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.15it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 171.72it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.51it/s]
-------------------------------------
| approxkl           | 0.0009102797 |
| clipfrac           | 0.15409376   |
| eplenmean          | 400          |
| eprewmean          | 252          |
| explained_variance | 0.282        |
| fps                | 1430         |
| nupdates           | 55           |
| policy_entropy     | 0.76833045   |
| policy_loss        | 0.0011002065 |
| serial_timesteps   | 22000        |
| time_elapsed       | 254          |
| time_remaining     | 0.847        |
| total_timesteps    | 660000       |
| true_eprew         | 190          |
| value_loss         | 73.774155    |
-------------------------------------
Current reward shaping 0.33999999999999997
Current self-play randomization 0.9359999999999999
SP envs: 27/30
Other agent actions took 5.163723468780518 seconds
Total simulation time for 400 steps: 7.924828290939331 	 Other agent action time: 0 	 50.474279733900445 steps/s
Curr learning rate 0.0004444444444444444 	 Curr reward per step 0.5857166666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.93it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 178.52it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 181.09it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 182.57it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.90it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.51it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.17it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.61it/s]
-------------------------------------
| approxkl           | 0.0016059146 |
| clipfrac           | 0.22030203   |
| eplenmean          | 400          |
| eprewmean          | 245          |
| explained_variance | 0.184        |
| fps                | 1429         |
| nupdates           | 56           |
| policy_entropy     | 0.76680434   |
| policy_loss        | 0.0029717474 |
| serial_timesteps   | 22400        |
| time_elapsed       | 262          |
| time_remaining     | 0.781        |
| total_timesteps    | 672000       |
| true_eprew         | 186          |
| value_loss         | 70.66399     |
-------------------------------------
Current reward shaping 0.32799999999999996
Current self-play randomization 0.9312
SP envs: 22/30
Other agent actions took 5.175920486450195 seconds
Total simulation time for 400 steps: 7.927743911743164 	 Other agent action time: 0 	 50.45571658886335 steps/s
Curr learning rate 0.00043434343434343433 	 Curr reward per step 0.5024226666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.47it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 176.87it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.80it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.70it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.69it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.69it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.98it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 179.72it/s]
-------------------------------------
| approxkl           | 0.0013594719 |
| clipfrac           | 0.19480208   |
| eplenmean          | 400          |
| eprewmean          | 225          |
| explained_variance | 0.186        |
| fps                | 1428         |
| nupdates           | 57           |
| policy_entropy     | 0.7889052    |
| policy_loss        | 0.002397722  |
| serial_timesteps   | 22800        |
| time_elapsed       | 271          |
| time_remaining     | 0.713        |
| total_timesteps    | 684000       |
| true_eprew         | 172          |
| value_loss         | 86.94188     |
-------------------------------------
Current reward shaping 0.31599999999999995
Current self-play randomization 0.9264
SP envs: 28/30
Other agent actions took 5.171598434448242 seconds
Total simulation time for 400 steps: 7.921577215194702 	 Other agent action time: 0 	 50.49499476351043 steps/s
Curr learning rate 0.00042424242424242425 	 Curr reward per step 0.6107823333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 179.16it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 182.03it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 185.29it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.11it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 187.79it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.82it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.42it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.49it/s]
-------------------------------------
| approxkl           | 0.0011264232 |
| clipfrac           | 0.1528125    |
| eplenmean          | 400          |
| eprewmean          | 227          |
| explained_variance | 0.258        |
| fps                | 1431         |
| nupdates           | 58           |
| policy_entropy     | 0.7298492    |
| policy_loss        | 0.0018283778 |
| serial_timesteps   | 23200        |
| time_elapsed       | 279          |
| time_remaining     | 0.642        |
| total_timesteps    | 696000       |
| true_eprew         | 176          |
| value_loss         | 65.12874     |
-------------------------------------
Current reward shaping 0.30400000000000005
Current self-play randomization 0.9216
SP envs: 25/30
Other agent actions took 5.208387851715088 seconds
Total simulation time for 400 steps: 8.008158206939697 	 Other agent action time: 0 	 49.949063150796476 steps/s
Curr learning rate 0.0004141414141414141 	 Curr reward per step 0.5468066666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.81it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.40it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.63it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 173.79it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.72it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 181.66it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 181.02it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.04it/s]
-------------------------------------
| approxkl           | 0.0011004482 |
| clipfrac           | 0.16634373   |
| eplenmean          | 400          |
| eprewmean          | 223          |
| explained_variance | 0.29         |
| fps                | 1414         |
| nupdates           | 59           |
| policy_entropy     | 0.76845586   |
| policy_loss        | 0.0016631087 |
| serial_timesteps   | 23600        |
| time_elapsed       | 288          |
| time_remaining     | 0.569        |
| total_timesteps    | 708000       |
| true_eprew         | 174          |
| value_loss         | 68.035995    |
-------------------------------------
Current reward shaping 0.29200000000000004
Current self-play randomization 0.9168000000000001
SP envs: 28/30
Other agent actions took 5.161366701126099 seconds
Total simulation time for 400 steps: 7.947421073913574 	 Other agent action time: 0 	 50.3307923765296 steps/s
Curr learning rate 0.00040404040404040404 	 Curr reward per step 0.5832480000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 180.63it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.90it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 185.79it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.52it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 185.42it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.99it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.53it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.90it/s]
-------------------------------------
| approxkl           | 0.0013434883 |
| clipfrac           | 0.18273959   |
| eplenmean          | 400          |
| eprewmean          | 229          |
| explained_variance | 0.256        |
| fps                | 1427         |
| nupdates           | 60           |
| policy_entropy     | 0.7414483    |
| policy_loss        | 0.0018265139 |
| serial_timesteps   | 24000        |
| time_elapsed       | 296          |
| time_remaining     | 0.493        |
| total_timesteps    | 720000       |
| true_eprew         | 180          |
| value_loss         | 66.296936    |
-------------------------------------
Current reward shaping 0.28
Current self-play randomization 0.912
SP envs: 26/30
Other agent actions took 5.190763473510742 seconds
Total simulation time for 400 steps: 7.986037254333496 	 Other agent action time: 0 	 50.087419737861396 steps/s
Curr learning rate 0.00039393939393939396 	 Curr reward per step 0.5452700000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 181.32it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 187.88it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 187.25it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.89it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 188.27it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.67it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.47it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 187.44it/s]
-------------------------------------
| approxkl           | 0.0014965183 |
| clipfrac           | 0.20764585   |
| eplenmean          | 400          |
| eprewmean          | 227          |
| explained_variance | 0.285        |
| fps                | 1421         |
| nupdates           | 61           |
| policy_entropy     | 0.77524936   |
| policy_loss        | 0.0027201066 |
| serial_timesteps   | 24400        |
| time_elapsed       | 304          |
| time_remaining     | 0.416        |
| total_timesteps    | 732000       |
| true_eprew         | 180          |
| value_loss         | 67.20949     |
-------------------------------------
Current reward shaping 0.268
Current self-play randomization 0.9072
SP envs: 27/30
Other agent actions took 5.138028144836426 seconds
Total simulation time for 400 steps: 7.8915088176727295 	 Other agent action time: 0 	 50.68739188432704 steps/s
Curr learning rate 0.0003838383838383839 	 Curr reward per step 0.5501676666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.20it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 183.85it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.87it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 182.22it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.70it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 180.34it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 180.58it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.91it/s]
-------------------------------------
| approxkl           | 0.0006915448 |
| clipfrac           | 0.12614585   |
| eplenmean          | 400          |
| eprewmean          | 218          |
| explained_variance | 0.337        |
| fps                | 1435         |
| nupdates           | 62           |
| policy_entropy     | 0.74188554   |
| policy_loss        | 0.0005050215 |
| serial_timesteps   | 24800        |
| time_elapsed       | 313          |
| time_remaining     | 0.336        |
| total_timesteps    | 744000       |
| true_eprew         | 175          |
| value_loss         | 65.16076     |
-------------------------------------
Current reward shaping 0.256
Current self-play randomization 0.9024
SP envs: 27/30
Other agent actions took 5.186274290084839 seconds
Total simulation time for 400 steps: 7.968332529067993 	 Other agent action time: 0 	 50.19870826685813 steps/s
Curr learning rate 0.0003737373737373737 	 Curr reward per step 0.5456106666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.49it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 182.80it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.50it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 183.52it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 182.96it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 180.00it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 181.43it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.12it/s]
--------------------------------------
| approxkl           | 0.0006429507  |
| clipfrac           | 0.12184373    |
| eplenmean          | 400           |
| eprewmean          | 222           |
| explained_variance | 0.287         |
| fps                | 1422          |
| nupdates           | 63            |
| policy_entropy     | 0.7600632     |
| policy_loss        | 0.00044901003 |
| serial_timesteps   | 25200         |
| time_elapsed       | 321           |
| time_remaining     | 0.255         |
| total_timesteps    | 756000        |
| true_eprew         | 180           |
| value_loss         | 59.967274     |
--------------------------------------
Current reward shaping 0.244
Current self-play randomization 0.8976
SP envs: 25/30
Other agent actions took 5.191071510314941 seconds
Total simulation time for 400 steps: 7.973127365112305 	 Other agent action time: 0 	 50.16852004023716 steps/s
Curr learning rate 0.0003636363636363636 	 Curr reward per step 0.5122523333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.09it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.00it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.76it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 177.14it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.18it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.36it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 183.49it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 176.30it/s]
-------------------------------------
| approxkl           | 0.0013787359 |
| clipfrac           | 0.1900729    |
| eplenmean          | 400          |
| eprewmean          | 213          |
| explained_variance | 0.285        |
| fps                | 1419         |
| nupdates           | 64           |
| policy_entropy     | 0.7631537    |
| policy_loss        | 0.0024064041 |
| serial_timesteps   | 25600        |
| time_elapsed       | 330          |
| time_remaining     | 0.172        |
| total_timesteps    | 768000       |
| true_eprew         | 173          |
| value_loss         | 71.571556    |
-------------------------------------
Current reward shaping 0.23199999999999998
Current self-play randomization 0.8928
SP envs: 22/30
Other agent actions took 5.188241958618164 seconds
Total simulation time for 400 steps: 7.95635461807251 	 Other agent action time: 0 	 50.27428001907024 steps/s
Curr learning rate 0.00035353535353535354 	 Curr reward per step 0.46228666666666657

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.73it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.91it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.85it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 181.22it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 182.97it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.14it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.81it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.22it/s]
-------------------------------------
| approxkl           | 0.0007281153 |
| clipfrac           | 0.14147915   |
| eplenmean          | 400          |
| eprewmean          | 205          |
| explained_variance | 0.28         |
| fps                | 1424         |
| nupdates           | 65           |
| policy_entropy     | 0.75589883   |
| policy_loss        | 0.0009545046 |
| serial_timesteps   | 26000        |
| time_elapsed       | 338          |
| time_remaining     | 0.0867       |
| total_timesteps    | 780000       |
| true_eprew         | 168          |
| value_loss         | 79.98705     |
-------------------------------------
Current reward shaping 0.21999999999999997
Current self-play randomization 0.888
SP envs: 28/30
Other agent actions took 5.160611867904663 seconds
Total simulation time for 400 steps: 7.942438125610352 	 Other agent action time: 0 	 50.362369045117525 steps/s
Curr learning rate 0.0003434343434343434 	 Curr reward per step 0.5553266666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 161.96it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.74it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.37it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.21it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 165.77it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 176.59it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.31it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 178.90it/s]
-------------------------------------
| approxkl           | 0.0008812081 |
| clipfrac           | 0.14653124   |
| eplenmean          | 400          |
| eprewmean          | 207          |
| explained_variance | 0.344        |
| fps                | 1421         |
| nupdates           | 66           |
| policy_entropy     | 0.7421475    |
| policy_loss        | 0.0011141361 |
| serial_timesteps   | 26400        |
| time_elapsed       | 347          |
| time_remaining     | 0            |
| total_timesteps    | 792000       |
| true_eprew         | 172          |
| value_loss         | 59.90099     |
-------------------------------------
Current reward shaping 0.20799999999999996
Current self-play randomization 0.8832
LOADING BC MODEL FROM: seed1/worker12
Loading a model without an environment, this model cannot be trained until it has a valid environment.
Loaded MediumLevelPlanner from /home/ubuntu/human_aware_rl/overcooked_ai/overcooked_ai_py/data/planners/simple_am.pkl
TOT NUM UPDATES 66
SP envs: 28/30
Other agent actions took 5.118930101394653 seconds
Total simulation time for 400 steps: 7.866269111633301 	 Other agent action time: 0 	 50.85002741750169 steps/s
Curr learning rate 0.001 	 Curr reward per step 0.5484373333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 179.17it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.93it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 187.84it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 189.78it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 188.34it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 188.48it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 189.36it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.30it/s]
-------------------------------------
| approxkl           | 0.0017732049 |
| clipfrac           | 0.21493752   |
| eplenmean          | 400          |
| eprewmean          | 219          |
| explained_variance | 0.246        |
| fps                | 1440         |
| nupdates           | 1            |
| policy_entropy     | 0.72744495   |
| policy_loss        | 0.0028777854 |
| serial_timesteps   | 400          |
| time_elapsed       | 8.33         |
| time_remaining     | 9.02         |
| total_timesteps    | 12000        |
| true_eprew         | 185          |
| value_loss         | 65.42852     |
-------------------------------------
Current reward shaping 0.988
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.603386640548706 seconds
Total simulation time for 400 steps: 3.5385677814483643 	 Other agent action time: 0 	 113.04008421064546 steps/s
Curr learning rate 0.00098989898989899 	 Curr reward per step 0.9161373333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.83it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 176.29it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 174.38it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 176.42it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 175.32it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 178.60it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.05it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.01it/s]
-------------------------------------
| approxkl           | 0.0035152372 |
| clipfrac           | 0.29505205   |
| eplenmean          | 400          |
| eprewmean          | 293          |
| explained_variance | 0.242        |
| fps                | 2982         |
| nupdates           | 2            |
| policy_entropy     | 0.7265245    |
| policy_loss        | 0.006267724  |
| serial_timesteps   | 800          |
| time_elapsed       | 12.4         |
| time_remaining     | 6.59         |
| total_timesteps    | 24000        |
| true_eprew         | 190          |
| value_loss         | 134.58656    |
-------------------------------------
Current reward shaping 0.976
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6144177913665771 seconds
Total simulation time for 400 steps: 3.5164144039154053 	 Other agent action time: 0 	 113.7522356735355 steps/s
Curr learning rate 0.0009797979797979799 	 Curr reward per step 0.8803666666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.62it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 176.11it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.14it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 178.32it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 175.23it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.63it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 177.26it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 177.49it/s]
-------------------------------------
| approxkl           | 0.0059546167 |
| clipfrac           | 0.37167713   |
| eplenmean          | 400          |
| eprewmean          | 313          |
| explained_variance | 0.246        |
| fps                | 3000         |
| nupdates           | 3            |
| policy_entropy     | 0.75289786   |
| policy_loss        | 0.011205911  |
| serial_timesteps   | 1200         |
| time_elapsed       | 16.4         |
| time_remaining     | 5.72         |
| total_timesteps    | 36000        |
| true_eprew         | 190          |
| value_loss         | 128.49545    |
-------------------------------------
Current reward shaping 0.964
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6045212745666504 seconds
Total simulation time for 400 steps: 3.526327610015869 	 Other agent action time: 0 	 113.4324555846358 steps/s
Curr learning rate 0.0009696969696969698 	 Curr reward per step 0.8740359999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.58it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 175.66it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.84it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.40it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 175.94it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.63it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 177.11it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.65it/s]
-------------------------------------
| approxkl           | 0.0052755857 |
| clipfrac           | 0.367802     |
| eplenmean          | 400          |
| eprewmean          | 340          |
| explained_variance | 0.315        |
| fps                | 2991         |
| nupdates           | 4            |
| policy_entropy     | 0.76904154   |
| policy_loss        | 0.009316599  |
| serial_timesteps   | 1600         |
| time_elapsed       | 20.4         |
| time_remaining     | 5.26         |
| total_timesteps    | 48000        |
| true_eprew         | 188          |
| value_loss         | 110.0201     |
-------------------------------------
Current reward shaping 0.952
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6054308414459229 seconds
Total simulation time for 400 steps: 3.544585943222046 	 Other agent action time: 0 	 112.8481595332396 steps/s
Curr learning rate 0.0009595959595959597 	 Curr reward per step 0.8646053333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 180.01it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 186.52it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 186.59it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 187.55it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 188.07it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 188.69it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.71it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 189.46it/s]
-------------------------------------
| approxkl           | 0.0045594396 |
| clipfrac           | 0.36490628   |
| eplenmean          | 400          |
| eprewmean          | 351          |
| explained_variance | 0.279        |
| fps                | 3000         |
| nupdates           | 5            |
| policy_entropy     | 0.7926737    |
| policy_loss        | 0.0083592255 |
| serial_timesteps   | 2000         |
| time_elapsed       | 24.4         |
| time_remaining     | 4.95         |
| total_timesteps    | 60000        |
| true_eprew         | 190          |
| value_loss         | 110.33041    |
-------------------------------------
Current reward shaping 0.94
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6060678958892822 seconds
Total simulation time for 400 steps: 3.515455722808838 	 Other agent action time: 0 	 113.78325643663669 steps/s
Curr learning rate 0.0009494949494949496 	 Curr reward per step 0.8159083333333337

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 173.66it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 178.55it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.63it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 179.63it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 173.33it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.18it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 177.23it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 163.68it/s]
-------------------------------------
| approxkl           | 0.0055380976 |
| clipfrac           | 0.37574995   |
| eplenmean          | 400          |
| eprewmean          | 341          |
| explained_variance | 0.222        |
| fps                | 2999         |
| nupdates           | 6            |
| policy_entropy     | 0.82807195   |
| policy_loss        | 0.009210622  |
| serial_timesteps   | 2400         |
| time_elapsed       | 28.4         |
| time_remaining     | 4.73         |
| total_timesteps    | 72000        |
| true_eprew         | 185          |
| value_loss         | 103.07528    |
-------------------------------------
Current reward shaping 0.928
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6043493747711182 seconds
Total simulation time for 400 steps: 3.5217301845550537 	 Other agent action time: 0 	 113.58053542950147 steps/s
Curr learning rate 0.0009393939393939395 	 Curr reward per step 0.8281333333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 164.04it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 169.57it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 169.20it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.89it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.83it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.69it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 176.36it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 171.19it/s]
------------------------------------
| approxkl           | 0.003251909 |
| clipfrac           | 0.28495833  |
| eplenmean          | 400         |
| eprewmean          | 335         |
| explained_variance | 0.249       |
| fps                | 2988        |
| nupdates           | 7           |
| policy_entropy     | 0.79935735  |
| policy_loss        | 0.005366399 |
| serial_timesteps   | 2800        |
| time_elapsed       | 32.4        |
| time_remaining     | 4.55        |
| total_timesteps    | 84000       |
| true_eprew         | 183         |
| value_loss         | 102.13668   |
------------------------------------
Current reward shaping 0.916
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6128220558166504 seconds
Total simulation time for 400 steps: 3.570150852203369 	 Other agent action time: 0 	 112.04008361527198 steps/s
Curr learning rate 0.0009292929292929292 	 Curr reward per step 0.824466

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 158.59it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 167.65it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 167.69it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 169.56it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 168.93it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 167.25it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.25it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 169.09it/s]
-------------------------------------
| approxkl           | 0.0028180112 |
| clipfrac           | 0.2862709    |
| eplenmean          | 400          |
| eprewmean          | 332          |
| explained_variance | 0.27         |
| fps                | 2943         |
| nupdates           | 8            |
| policy_entropy     | 0.7779306    |
| policy_loss        | 0.0051030186 |
| serial_timesteps   | 3200         |
| time_elapsed       | 36.5         |
| time_remaining     | 4.41         |
| total_timesteps    | 96000        |
| true_eprew         | 182          |
| value_loss         | 102.8102     |
-------------------------------------
Current reward shaping 0.904
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6128199100494385 seconds
Total simulation time for 400 steps: 3.5532877445220947 	 Other agent action time: 0 	 112.57180075457093 steps/s
Curr learning rate 0.0009191919191919192 	 Curr reward per step 0.8289073333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 181.11it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 182.48it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.66it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.89it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 182.92it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 188.24it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.60it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.41it/s]
-------------------------------------
| approxkl           | 0.0023507182 |
| clipfrac           | 0.24470837   |
| eplenmean          | 400          |
| eprewmean          | 331          |
| explained_variance | 0.239        |
| fps                | 2988         |
| nupdates           | 9            |
| policy_entropy     | 0.76867807   |
| policy_loss        | 0.0033033155 |
| serial_timesteps   | 3600         |
| time_elapsed       | 40.5         |
| time_remaining     | 4.27         |
| total_timesteps    | 108000       |
| true_eprew         | 182          |
| value_loss         | 101.53423    |
-------------------------------------
Current reward shaping 0.892
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6047735214233398 seconds
Total simulation time for 400 steps: 3.519137382507324 	 Other agent action time: 0 	 113.66421839291962 steps/s
Curr learning rate 0.0009090909090909091 	 Curr reward per step 0.8158766666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.23it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 174.98it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.43it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 171.66it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 170.97it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 175.16it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 174.75it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.09it/s]
------------------------------------
| approxkl           | 0.004426578 |
| clipfrac           | 0.3170625   |
| eplenmean          | 400         |
| eprewmean          | 330         |
| explained_variance | 0.248       |
| fps                | 2992        |
| nupdates           | 10          |
| policy_entropy     | 0.7923037   |
| policy_loss        | 0.007861646 |
| serial_timesteps   | 4000        |
| time_elapsed       | 44.5        |
| time_remaining     | 4.15        |
| total_timesteps    | 120000      |
| true_eprew         | 183         |
| value_loss         | 106.370804  |
------------------------------------
Current reward shaping 0.88
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5998618602752686 seconds
Total simulation time for 400 steps: 3.5313265323638916 	 Other agent action time: 0 	 113.27188135508884 steps/s
Curr learning rate 0.000898989898989899 	 Curr reward per step 0.8216800000000002

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.16it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 178.23it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 181.38it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 179.35it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.65it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.93it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.19it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.01it/s]
-------------------------------------
| approxkl           | 0.0024622139 |
| clipfrac           | 0.28559372   |
| eplenmean          | 400          |
| eprewmean          | 330          |
| explained_variance | 0.218        |
| fps                | 2998         |
| nupdates           | 11           |
| policy_entropy     | 0.79334617   |
| policy_loss        | 0.005194783  |
| serial_timesteps   | 4400         |
| time_elapsed       | 48.5         |
| time_remaining     | 4.04         |
| total_timesteps    | 132000       |
| true_eprew         | 184          |
| value_loss         | 105.3558     |
-------------------------------------
Current reward shaping 0.868
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6048715114593506 seconds
Total simulation time for 400 steps: 3.552940845489502 	 Other agent action time: 0 	 112.58279194482071 steps/s
Curr learning rate 0.0008888888888888889 	 Curr reward per step 0.8361573333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.80it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 179.49it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.89it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.97it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.00it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.10it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.43it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 187.10it/s]
------------------------------------
| approxkl           | 0.002746224 |
| clipfrac           | 0.2893021   |
| eplenmean          | 400         |
| eprewmean          | 330         |
| explained_variance | 0.257       |
| fps                | 2985        |
| nupdates           | 12          |
| policy_entropy     | 0.79866874  |
| policy_loss        | 0.005821281 |
| serial_timesteps   | 4800        |
| time_elapsed       | 52.5        |
| time_remaining     | 3.94        |
| total_timesteps    | 144000      |
| true_eprew         | 185         |
| value_loss         | 100.78536   |
------------------------------------
Current reward shaping 0.856
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5991277694702148 seconds
Total simulation time for 400 steps: 3.501481056213379 	 Other agent action time: 0 	 114.23737372224245 steps/s
Curr learning rate 0.0008787878787878789 	 Curr reward per step 0.8092480000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.00it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 183.70it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 188.13it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.17it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.66it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.07it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 179.62it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.76it/s]
-------------------------------------
| approxkl           | 0.0022492022 |
| clipfrac           | 0.26058334   |
| eplenmean          | 400          |
| eprewmean          | 330          |
| explained_variance | 0.229        |
| fps                | 3025         |
| nupdates           | 13           |
| policy_entropy     | 0.8073298    |
| policy_loss        | 0.004356435  |
| serial_timesteps   | 5200         |
| time_elapsed       | 56.5         |
| time_remaining     | 3.84         |
| total_timesteps    | 156000       |
| true_eprew         | 186          |
| value_loss         | 97.49039     |
-------------------------------------
Current reward shaping 0.844
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6061639785766602 seconds
Total simulation time for 400 steps: 3.523815870285034 	 Other agent action time: 0 	 113.51330907299786 steps/s
Curr learning rate 0.0008686868686868688 	 Curr reward per step 0.8262169999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.29it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.83it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 164.92it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.71it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 166.75it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.05it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 179.26it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.89it/s]
-------------------------------------
| approxkl           | 0.0020644725 |
| clipfrac           | 0.24573961   |
| eplenmean          | 400          |
| eprewmean          | 329          |
| explained_variance | 0.273        |
| fps                | 2980         |
| nupdates           | 14           |
| policy_entropy     | 0.80354595   |
| policy_loss        | 0.0037864465 |
| serial_timesteps   | 5600         |
| time_elapsed       | 60.5         |
| time_remaining     | 3.75         |
| total_timesteps    | 168000       |
| true_eprew         | 187          |
| value_loss         | 97.60844     |
-------------------------------------
Current reward shaping 0.832
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5980987548828125 seconds
Total simulation time for 400 steps: 3.522108554840088 	 Other agent action time: 0 	 113.56833378980306 steps/s
Curr learning rate 0.0008585858585858587 	 Curr reward per step 0.8207573333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.15it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 183.28it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 179.50it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 184.56it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 182.24it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.97it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 181.50it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 176.57it/s]
-------------------------------------
| approxkl           | 0.0032792757 |
| clipfrac           | 0.29031247   |
| eplenmean          | 400          |
| eprewmean          | 326          |
| explained_variance | 0.231        |
| fps                | 3004         |
| nupdates           | 15           |
| policy_entropy     | 0.7807182    |
| policy_loss        | 0.0067065903 |
| serial_timesteps   | 6000         |
| time_elapsed       | 64.5         |
| time_remaining     | 3.66         |
| total_timesteps    | 180000       |
| true_eprew         | 186          |
| value_loss         | 101.79291    |
-------------------------------------
Current reward shaping 0.8200000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6063644886016846 seconds
Total simulation time for 400 steps: 3.5780704021453857 	 Other agent action time: 0 	 111.79209882515526 steps/s
Curr learning rate 0.0008484848484848486 	 Curr reward per step 0.7794199999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.92it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 165.91it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 165.13it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.40it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.86it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 166.65it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 168.37it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.14it/s]
-------------------------------------
| approxkl           | 0.0020708407 |
| clipfrac           | 0.24430215   |
| eplenmean          | 400          |
| eprewmean          | 322          |
| explained_variance | 0.25         |
| fps                | 2935         |
| nupdates           | 16           |
| policy_entropy     | 0.8289239    |
| policy_loss        | 0.0038684402 |
| serial_timesteps   | 6400         |
| time_elapsed       | 68.6         |
| time_remaining     | 3.57         |
| total_timesteps    | 192000       |
| true_eprew         | 185          |
| value_loss         | 89.49165     |
-------------------------------------
Current reward shaping 0.808
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6043872833251953 seconds
Total simulation time for 400 steps: 3.512338399887085 	 Other agent action time: 0 	 113.88424304812408 steps/s
Curr learning rate 0.0008383838383838385 	 Curr reward per step 0.7858426666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.53it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 185.17it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.31it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 184.89it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 185.07it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.45it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 189.32it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.03it/s]
-------------------------------------
| approxkl           | 0.0049336944 |
| clipfrac           | 0.33983332   |
| eplenmean          | 400          |
| eprewmean          | 319          |
| explained_variance | 0.235        |
| fps                | 3019         |
| nupdates           | 17           |
| policy_entropy     | 0.80036724   |
| policy_loss        | 0.007973412  |
| serial_timesteps   | 6800         |
| time_elapsed       | 72.6         |
| time_remaining     | 3.49         |
| total_timesteps    | 204000       |
| true_eprew         | 184          |
| value_loss         | 89.907555    |
-------------------------------------
Current reward shaping 0.796
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.602548360824585 seconds
Total simulation time for 400 steps: 3.522623300552368 	 Other agent action time: 0 	 113.55173854021736 steps/s
Curr learning rate 0.0008282828282828282 	 Curr reward per step 0.7761279999999998

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.12it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 175.37it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 175.39it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 172.71it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.53it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 178.12it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 173.68it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 171.99it/s]
-------------------------------------
| approxkl           | 0.0101727145 |
| clipfrac           | 0.3513333    |
| eplenmean          | 400          |
| eprewmean          | 312          |
| explained_variance | 0.226        |
| fps                | 2992         |
| nupdates           | 18           |
| policy_entropy     | 0.7966598    |
| policy_loss        | 0.012676818  |
| serial_timesteps   | 7200         |
| time_elapsed       | 76.6         |
| time_remaining     | 3.4          |
| total_timesteps    | 216000       |
| true_eprew         | 182          |
| value_loss         | 96.313896    |
-------------------------------------
Current reward shaping 0.784
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.613732099533081 seconds
Total simulation time for 400 steps: 3.5348219871520996 	 Other agent action time: 0 	 113.15987097903847 steps/s
Curr learning rate 0.0008181818181818183 	 Curr reward per step 0.733964

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 176.87it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 183.93it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.54it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.85it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 187.46it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.24it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.08it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.40it/s]
-------------------------------------
| approxkl           | 0.0032019175 |
| clipfrac           | 0.29319784   |
| eplenmean          | 400          |
| eprewmean          | 306          |
| explained_variance | 0.248        |
| fps                | 3003         |
| nupdates           | 19           |
| policy_entropy     | 0.7601436    |
| policy_loss        | 0.0060966355 |
| serial_timesteps   | 7600         |
| time_elapsed       | 80.6         |
| time_remaining     | 3.32         |
| total_timesteps    | 228000       |
| true_eprew         | 179          |
| value_loss         | 91.09767     |
-------------------------------------
Current reward shaping 0.772
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6021347045898438 seconds
Total simulation time for 400 steps: 3.5008816719055176 	 Other agent action time: 0 	 114.2569322493786 steps/s
Curr learning rate 0.0008080808080808081 	 Curr reward per step 0.7519926666666669

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.58it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 174.93it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 177.25it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 175.36it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.84it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 177.86it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 176.59it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 176.15it/s]
-------------------------------------
| approxkl           | 0.0024041974 |
| clipfrac           | 0.2713125    |
| eplenmean          | 400          |
| eprewmean          | 304          |
| explained_variance | 0.269        |
| fps                | 3012         |
| nupdates           | 20           |
| policy_entropy     | 0.761638     |
| policy_loss        | 0.0048980615 |
| serial_timesteps   | 8000         |
| time_elapsed       | 84.6         |
| time_remaining     | 3.24         |
| total_timesteps    | 240000       |
| true_eprew         | 180          |
| value_loss         | 88.20809     |
-------------------------------------
Current reward shaping 0.76
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.607856035232544 seconds
Total simulation time for 400 steps: 3.5257039070129395 	 Other agent action time: 0 	 113.45252197848048 steps/s
Curr learning rate 0.000797979797979798 	 Curr reward per step 0.7315666666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 163.91it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 167.37it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.05it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.56it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 172.01it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 170.86it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 174.03it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 173.84it/s]
-------------------------------------
| approxkl           | 0.0033866283 |
| clipfrac           | 0.29951042   |
| eplenmean          | 400          |
| eprewmean          | 296          |
| explained_variance | 0.26         |
| fps                | 2980         |
| nupdates           | 21           |
| policy_entropy     | 0.74532276   |
| policy_loss        | 0.00609041   |
| serial_timesteps   | 8400         |
| time_elapsed       | 88.6         |
| time_remaining     | 3.16         |
| total_timesteps    | 252000       |
| true_eprew         | 176          |
| value_loss         | 89.144966    |
-------------------------------------
Current reward shaping 0.748
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6102485656738281 seconds
Total simulation time for 400 steps: 3.5091187953948975 	 Other agent action time: 0 	 113.98873145159115 steps/s
Curr learning rate 0.0007878787878787879 	 Curr reward per step 0.7459586666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.26it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 175.55it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 177.55it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 178.41it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.05it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 178.50it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.07it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.58it/s]
-------------------------------------
| approxkl           | 0.0019587406 |
| clipfrac           | 0.2500729    |
| eplenmean          | 400          |
| eprewmean          | 297          |
| explained_variance | 0.272        |
| fps                | 3009         |
| nupdates           | 22           |
| policy_entropy     | 0.7802912    |
| policy_loss        | 0.0033098604 |
| serial_timesteps   | 8800         |
| time_elapsed       | 92.6         |
| time_remaining     | 3.09         |
| total_timesteps    | 264000       |
| true_eprew         | 178          |
| value_loss         | 85.90919     |
-------------------------------------
Current reward shaping 0.736
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6026721000671387 seconds
Total simulation time for 400 steps: 3.480114459991455 	 Other agent action time: 0 	 114.9387483080031 steps/s
Curr learning rate 0.0007777777777777778 	 Curr reward per step 0.7516799999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 176.09it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.09it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 181.61it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 179.67it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.29it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.65it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.68it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 188.41it/s]
-------------------------------------
| approxkl           | 0.0024497171 |
| clipfrac           | 0.27116665   |
| eplenmean          | 400          |
| eprewmean          | 297          |
| explained_variance | 0.263        |
| fps                | 3040         |
| nupdates           | 23           |
| policy_entropy     | 0.7666301    |
| policy_loss        | 0.004825904  |
| serial_timesteps   | 9200         |
| time_elapsed       | 96.5         |
| time_remaining     | 3.01         |
| total_timesteps    | 276000       |
| true_eprew         | 179          |
| value_loss         | 87.23489     |
-------------------------------------
Current reward shaping 0.724
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.602344274520874 seconds
Total simulation time for 400 steps: 3.5149006843566895 	 Other agent action time: 0 	 113.80122396636351 steps/s
Curr learning rate 0.0007676767676767678 	 Curr reward per step 0.7128576666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 182.86it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.49it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.32it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.48it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.32it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.59it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.43it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.22it/s]
------------------------------------
| approxkl           | 0.006022041 |
| clipfrac           | 0.37561455  |
| eplenmean          | 400         |
| eprewmean          | 293         |
| explained_variance | 0.26        |
| fps                | 3018        |
| nupdates           | 24          |
| policy_entropy     | 0.8097113   |
| policy_loss        | 0.010654964 |
| serial_timesteps   | 9600        |
| time_elapsed       | 101         |
| time_remaining     | 2.93        |
| total_timesteps    | 288000      |
| true_eprew         | 178         |
| value_loss         | 86.1595     |
------------------------------------
Current reward shaping 0.712
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6060612201690674 seconds
Total simulation time for 400 steps: 3.5130085945129395 	 Other agent action time: 0 	 113.86251676832516 steps/s
Curr learning rate 0.0007575757575757577 	 Curr reward per step 0.7143466666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 161.31it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 169.48it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 165.68it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.23it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 168.58it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 166.62it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.84it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 166.64it/s]
-------------------------------------
| approxkl           | 0.0021978659 |
| clipfrac           | 0.25814587   |
| eplenmean          | 400          |
| eprewmean          | 293          |
| explained_variance | 0.305        |
| fps                | 2984         |
| nupdates           | 25           |
| policy_entropy     | 0.78306115   |
| policy_loss        | 0.0041431794 |
| serial_timesteps   | 10000        |
| time_elapsed       | 105          |
| time_remaining     | 2.86         |
| total_timesteps    | 300000       |
| true_eprew         | 178          |
| value_loss         | 91.878586    |
-------------------------------------
Current reward shaping 0.7
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6015841960906982 seconds
Total simulation time for 400 steps: 3.5422091484069824 	 Other agent action time: 0 	 112.9238797714386 steps/s
Curr learning rate 0.0007474747474747475 	 Curr reward per step 0.7334499999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 176.55it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 186.39it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 184.53it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.79it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 186.05it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 187.50it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.68it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 189.83it/s]
-------------------------------------
| approxkl           | 0.0028928835 |
| clipfrac           | 0.30363548   |
| eplenmean          | 400          |
| eprewmean          | 289          |
| explained_variance | 0.246        |
| fps                | 2999         |
| nupdates           | 26           |
| policy_entropy     | 0.7793639    |
| policy_loss        | 0.005292463  |
| serial_timesteps   | 10400        |
| time_elapsed       | 109          |
| time_remaining     | 2.78         |
| total_timesteps    | 312000       |
| true_eprew         | 177          |
| value_loss         | 80.70747     |
-------------------------------------
Current reward shaping 0.688
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6098322868347168 seconds
Total simulation time for 400 steps: 3.499558687210083 	 Other agent action time: 0 	 114.30012631646645 steps/s
Curr learning rate 0.0007373737373737374 	 Curr reward per step 0.6839133333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 163.45it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 169.48it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.89it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.69it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 168.25it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 167.84it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.92it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 167.52it/s]
-------------------------------------
| approxkl           | 0.002765316  |
| clipfrac           | 0.28831252   |
| eplenmean          | 400          |
| eprewmean          | 284          |
| explained_variance | 0.282        |
| fps                | 2997         |
| nupdates           | 27           |
| policy_entropy     | 0.7849628    |
| policy_loss        | 0.0054707397 |
| serial_timesteps   | 10800        |
| time_elapsed       | 113          |
| time_remaining     | 2.71         |
| total_timesteps    | 324000       |
| true_eprew         | 175          |
| value_loss         | 94.60805     |
-------------------------------------
Current reward shaping 0.6759999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6097533702850342 seconds
Total simulation time for 400 steps: 3.515982151031494 	 Other agent action time: 0 	 113.76622030992131 steps/s
Curr learning rate 0.0007272727272727272 	 Curr reward per step 0.71479

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.00it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 182.33it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 184.25it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 181.05it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.46it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.55it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.92it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 187.63it/s]
------------------------------------
| approxkl           | 0.00437487  |
| clipfrac           | 0.32686454  |
| eplenmean          | 400         |
| eprewmean          | 284         |
| explained_variance | 0.228       |
| fps                | 3014        |
| nupdates           | 28          |
| policy_entropy     | 0.78459185  |
| policy_loss        | 0.007110647 |
| serial_timesteps   | 11200       |
| time_elapsed       | 117         |
| time_remaining     | 2.64        |
| total_timesteps    | 336000      |
| true_eprew         | 176         |
| value_loss         | 85.67381    |
------------------------------------
Current reward shaping 0.6639999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6059684753417969 seconds
Total simulation time for 400 steps: 3.5162084102630615 	 Other agent action time: 0 	 113.75889973770764 steps/s
Curr learning rate 0.0007171717171717171 	 Curr reward per step 0.6623466666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.96it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 179.29it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 181.37it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 184.35it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 182.19it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 180.45it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.69it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 178.66it/s]
------------------------------------
| approxkl           | 0.008879786 |
| clipfrac           | 0.41244787  |
| eplenmean          | 400         |
| eprewmean          | 276         |
| explained_variance | 0.285       |
| fps                | 3009        |
| nupdates           | 29          |
| policy_entropy     | 0.79516464  |
| policy_loss        | 0.015676536 |
| serial_timesteps   | 11600       |
| time_elapsed       | 121         |
| time_remaining     | 2.56        |
| total_timesteps    | 348000      |
| true_eprew         | 173         |
| value_loss         | 92.034096   |
------------------------------------
Current reward shaping 0.652
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5959358215332031 seconds
Total simulation time for 400 steps: 3.4863431453704834 	 Other agent action time: 0 	 114.73339924417944 steps/s
Curr learning rate 0.0007070707070707071 	 Curr reward per step 0.7009466666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 165.71it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.42it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 175.68it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 175.83it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.93it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 177.57it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.12it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.70it/s]
-------------------------------------
| approxkl           | 0.0029257322 |
| clipfrac           | 0.28694788   |
| eplenmean          | 400          |
| eprewmean          | 278          |
| explained_variance | 0.224        |
| fps                | 3022         |
| nupdates           | 30           |
| policy_entropy     | 0.776649     |
| policy_loss        | 0.0048414906 |
| serial_timesteps   | 12000        |
| time_elapsed       | 124          |
| time_remaining     | 2.49         |
| total_timesteps    | 360000       |
| true_eprew         | 175          |
| value_loss         | 90.49208     |
-------------------------------------
Current reward shaping 0.64
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.602583646774292 seconds
Total simulation time for 400 steps: 3.5167603492736816 	 Other agent action time: 0 	 113.74104581297733 steps/s
Curr learning rate 0.000696969696969697 	 Curr reward per step 0.7240799999999998

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.53it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.56it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 174.38it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 175.91it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 172.53it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.05it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 175.50it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 173.51it/s]
-------------------------------------
| approxkl           | 0.0032921336 |
| clipfrac           | 0.29582292   |
| eplenmean          | 400          |
| eprewmean          | 280          |
| explained_variance | 0.243        |
| fps                | 2995         |
| nupdates           | 31           |
| policy_entropy     | 0.7626127    |
| policy_loss        | 0.005953288  |
| serial_timesteps   | 12400        |
| time_elapsed       | 128          |
| time_remaining     | 2.42         |
| total_timesteps    | 372000       |
| true_eprew         | 177          |
| value_loss         | 80.25902     |
-------------------------------------
Current reward shaping 0.628
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6050019264221191 seconds
Total simulation time for 400 steps: 3.526587724685669 	 Other agent action time: 0 	 113.42408901387891 steps/s
Curr learning rate 0.0006868686868686869 	 Curr reward per step 0.7061113333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 171.28it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.71it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 179.99it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 182.22it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.57it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.40it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 179.26it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 180.48it/s]
-------------------------------------
| approxkl           | 0.0018655151 |
| clipfrac           | 0.24440627   |
| eplenmean          | 400          |
| eprewmean          | 281          |
| explained_variance | 0.226        |
| fps                | 3001         |
| nupdates           | 32           |
| policy_entropy     | 0.77180314   |
| policy_loss        | 0.0032683674 |
| serial_timesteps   | 12800        |
| time_elapsed       | 132          |
| time_remaining     | 2.35         |
| total_timesteps    | 384000       |
| true_eprew         | 179          |
| value_loss         | 86.86226     |
-------------------------------------
Current reward shaping 0.616
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6201677322387695 seconds
Total simulation time for 400 steps: 3.5724282264709473 	 Other agent action time: 0 	 111.96865959015874 steps/s
Curr learning rate 0.0006767676767676768 	 Curr reward per step 0.7456979999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 176.17it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 186.14it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 185.51it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.29it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.09it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.86it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 184.19it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.42it/s]
-------------------------------------
| approxkl           | 0.0017432204 |
| clipfrac           | 0.22767702   |
| eplenmean          | 400          |
| eprewmean          | 290          |
| explained_variance | 0.216        |
| fps                | 2971         |
| nupdates           | 33           |
| policy_entropy     | 0.7555419    |
| policy_loss        | 0.0028043932 |
| serial_timesteps   | 13200        |
| time_elapsed       | 137          |
| time_remaining     | 2.28         |
| total_timesteps    | 396000       |
| true_eprew         | 186          |
| value_loss         | 83.31761     |
-------------------------------------
Current reward shaping 0.604
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6109633445739746 seconds
Total simulation time for 400 steps: 3.5509681701660156 	 Other agent action time: 0 	 112.64533525269508 steps/s
Curr learning rate 0.0006666666666666668 	 Curr reward per step 0.7333723333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.61it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 174.45it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 175.70it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.57it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 172.70it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.26it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 175.08it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.93it/s]
-------------------------------------
| approxkl           | 0.0024730705 |
| clipfrac           | 0.26325002   |
| eplenmean          | 400          |
| eprewmean          | 290          |
| explained_variance | 0.271        |
| fps                | 2971         |
| nupdates           | 34           |
| policy_entropy     | 0.75356114   |
| policy_loss        | 0.004219647  |
| serial_timesteps   | 13600        |
| time_elapsed       | 141          |
| time_remaining     | 2.2          |
| total_timesteps    | 408000       |
| true_eprew         | 188          |
| value_loss         | 81.34156     |
-------------------------------------
Current reward shaping 0.5920000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6058022975921631 seconds
Total simulation time for 400 steps: 3.530082941055298 	 Other agent action time: 0 	 113.31178521273563 steps/s
Curr learning rate 0.0006565656565656567 	 Curr reward per step 0.6799426666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 165.50it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.88it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 174.41it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 173.89it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 171.99it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 176.96it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 177.90it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 179.05it/s]
-------------------------------------
| approxkl           | 0.0024547565 |
| clipfrac           | 0.25457293   |
| eplenmean          | 400          |
| eprewmean          | 287          |
| explained_variance | 0.326        |
| fps                | 2986         |
| nupdates           | 35           |
| policy_entropy     | 0.76304495   |
| policy_loss        | 0.004086473  |
| serial_timesteps   | 14000        |
| time_elapsed       | 145          |
| time_remaining     | 2.13         |
| total_timesteps    | 420000       |
| true_eprew         | 188          |
| value_loss         | 76.23833     |
-------------------------------------
Current reward shaping 0.5800000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6101467609405518 seconds
Total simulation time for 400 steps: 3.560049057006836 	 Other agent action time: 0 	 112.35800226199858 steps/s
Curr learning rate 0.0006464646464646465 	 Curr reward per step 0.6761283333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.16it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 171.79it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 174.43it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.70it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.70it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 175.04it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.89it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 179.77it/s]
-------------------------------------
| approxkl           | 0.0014317303 |
| clipfrac           | 0.2107292    |
| eplenmean          | 400          |
| eprewmean          | 281          |
| explained_variance | 0.308        |
| fps                | 2966         |
| nupdates           | 36           |
| policy_entropy     | 0.7532122    |
| policy_loss        | 0.0018050259 |
| serial_timesteps   | 14400        |
| time_elapsed       | 149          |
| time_remaining     | 2.06         |
| total_timesteps    | 432000       |
| true_eprew         | 185          |
| value_loss         | 81.50569     |
-------------------------------------
Current reward shaping 0.5680000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6068685054779053 seconds
Total simulation time for 400 steps: 3.524571418762207 	 Other agent action time: 0 	 113.48897567253039 steps/s
Curr learning rate 0.0006363636363636364 	 Curr reward per step 0.706856

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.94it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.42it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.17it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.00it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 173.04it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.97it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 175.92it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.38it/s]
------------------------------------
| approxkl           | 0.003998274 |
| clipfrac           | 0.27939588  |
| eplenmean          | 400         |
| eprewmean          | 278         |
| explained_variance | 0.245       |
| fps                | 2990        |
| nupdates           | 37          |
| policy_entropy     | 0.74648273  |
| policy_loss        | 0.006163235 |
| serial_timesteps   | 14800       |
| time_elapsed       | 153         |
| time_remaining     | 1.99        |
| total_timesteps    | 444000      |
| true_eprew         | 184         |
| value_loss         | 84.979164   |
------------------------------------
Current reward shaping 0.556
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6034500598907471 seconds
Total simulation time for 400 steps: 3.5403404235839844 	 Other agent action time: 0 	 112.98348524209685 steps/s
Curr learning rate 0.0006262626262626263 	 Curr reward per step 0.6861503333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.04it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.71it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 184.20it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 191.52it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 186.91it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.89it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 183.42it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.24it/s]
-------------------------------------
| approxkl           | 0.0016666113 |
| clipfrac           | 0.21896878   |
| eplenmean          | 400          |
| eprewmean          | 276          |
| explained_variance | 0.231        |
| fps                | 2999         |
| nupdates           | 38           |
| policy_entropy     | 0.761836     |
| policy_loss        | 0.0027143692 |
| serial_timesteps   | 15200        |
| time_elapsed       | 157          |
| time_remaining     | 1.92         |
| total_timesteps    | 456000       |
| true_eprew         | 184          |
| value_loss         | 82.61363     |
-------------------------------------
Current reward shaping 0.544
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6032085418701172 seconds
Total simulation time for 400 steps: 3.533891439437866 	 Other agent action time: 0 	 113.18966834579042 steps/s
Curr learning rate 0.0006161616161616161 	 Curr reward per step 0.6931066666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.43it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.96it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 181.58it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 182.41it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 187.75it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.43it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.14it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 187.61it/s]
-------------------------------------
| approxkl           | 0.0012136543 |
| clipfrac           | 0.18886462   |
| eplenmean          | 400          |
| eprewmean          | 278          |
| explained_variance | 0.284        |
| fps                | 3002         |
| nupdates           | 39           |
| policy_entropy     | 0.7529612    |
| policy_loss        | 0.0019904335 |
| serial_timesteps   | 15600        |
| time_elapsed       | 161          |
| time_remaining     | 1.85         |
| total_timesteps    | 468000       |
| true_eprew         | 186          |
| value_loss         | 83.59396     |
-------------------------------------
Current reward shaping 0.532
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6077361106872559 seconds
Total simulation time for 400 steps: 3.4843976497650146 	 Other agent action time: 0 	 114.7974600507998 steps/s
Curr learning rate 0.0006060606060606061 	 Curr reward per step 0.6896273333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.15it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 182.50it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.51it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 178.38it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.38it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.23it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.40it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.22it/s]
------------------------------------
| approxkl           | 0.005562229 |
| clipfrac           | 0.33197913  |
| eplenmean          | 400         |
| eprewmean          | 276         |
| explained_variance | 0.289       |
| fps                | 3036        |
| nupdates           | 40          |
| policy_entropy     | 0.73951656  |
| policy_loss        | 0.009068845 |
| serial_timesteps   | 16000       |
| time_elapsed       | 165         |
| time_remaining     | 1.78        |
| total_timesteps    | 480000      |
| true_eprew         | 186         |
| value_loss         | 80.55194    |
------------------------------------
Current reward shaping 0.52
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6036717891693115 seconds
Total simulation time for 400 steps: 3.5386343002319336 	 Other agent action time: 0 	 113.03795929796495 steps/s
Curr learning rate 0.000595959595959596 	 Curr reward per step 0.7089

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 176.62it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 188.99it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 190.12it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 189.00it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 189.44it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 185.37it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.24it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 173.89it/s]
-------------------------------------
| approxkl           | 0.0017356698 |
| clipfrac           | 0.2168229    |
| eplenmean          | 400          |
| eprewmean          | 279          |
| explained_variance | 0.229        |
| fps                | 3000         |
| nupdates           | 41           |
| policy_entropy     | 0.733801     |
| policy_loss        | 0.0024199898 |
| serial_timesteps   | 16400        |
| time_elapsed       | 169          |
| time_remaining     | 1.71         |
| total_timesteps    | 492000       |
| true_eprew         | 190          |
| value_loss         | 80.65504     |
-------------------------------------
Current reward shaping 0.508
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6030547618865967 seconds
Total simulation time for 400 steps: 3.533768892288208 	 Other agent action time: 0 	 113.19359363678973 steps/s
Curr learning rate 0.0005858585858585859 	 Curr reward per step 0.6923863333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 171.85it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.66it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 175.75it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 183.48it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 185.00it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 178.91it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 183.35it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.03it/s]
-------------------------------------
| approxkl           | 0.002048721  |
| clipfrac           | 0.24939585   |
| eplenmean          | 400          |
| eprewmean          | 278          |
| explained_variance | 0.32         |
| fps                | 2994         |
| nupdates           | 42           |
| policy_entropy     | 0.7296154    |
| policy_loss        | 0.0035201397 |
| serial_timesteps   | 16800        |
| time_elapsed       | 173          |
| time_remaining     | 1.64         |
| total_timesteps    | 504000       |
| true_eprew         | 191          |
| value_loss         | 79.32854     |
-------------------------------------
Current reward shaping 0.496
Current self-play randomization 0.9984
SP envs: 30/30
Other agent actions took 0.5990190505981445 seconds
Total simulation time for 400 steps: 3.539862871170044 	 Other agent action time: 0 	 112.99872750940392 steps/s
Curr learning rate 0.0005757575757575758 	 Curr reward per step 0.6887746666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 171.65it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 178.00it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 174.45it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.59it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 179.84it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 178.46it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 181.33it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.16it/s]
-------------------------------------
| approxkl           | 0.004695654  |
| clipfrac           | 0.277875     |
| eplenmean          | 400          |
| eprewmean          | 280          |
| explained_variance | 0.218        |
| fps                | 2985         |
| nupdates           | 43           |
| policy_entropy     | 0.73125005   |
| policy_loss        | 0.0065907673 |
| serial_timesteps   | 17200        |
| time_elapsed       | 177          |
| time_remaining     | 1.57         |
| total_timesteps    | 516000       |
| true_eprew         | 193          |
| value_loss         | 77.87783     |
-------------------------------------
Current reward shaping 0.484
Current self-play randomization 0.9936
SP envs: 30/30
Other agent actions took 0.6143581867218018 seconds
Total simulation time for 400 steps: 3.5710203647613525 	 Other agent action time: 0 	 112.01280282441951 steps/s
Curr learning rate 0.0005656565656565657 	 Curr reward per step 0.6784946666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 176.11it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.13it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.77it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 183.84it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.18it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.02it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.58it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 186.26it/s]
-------------------------------------
| approxkl           | 0.0016863967 |
| clipfrac           | 0.22203124   |
| eplenmean          | 400          |
| eprewmean          | 277          |
| explained_variance | 0.279        |
| fps                | 2972         |
| nupdates           | 44           |
| policy_entropy     | 0.73379266   |
| policy_loss        | 0.002202749  |
| serial_timesteps   | 17600        |
| time_elapsed       | 181          |
| time_remaining     | 1.51         |
| total_timesteps    | 528000       |
| true_eprew         | 193          |
| value_loss         | 82.19945     |
-------------------------------------
Current reward shaping 0.472
Current self-play randomization 0.9888
SP envs: 27/30
Other agent actions took 5.163039684295654 seconds
Total simulation time for 400 steps: 7.950775146484375 	 Other agent action time: 0 	 50.30956009073776 steps/s
Curr learning rate 0.0005555555555555557 	 Curr reward per step 0.590574

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.96it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 183.91it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 188.98it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 185.71it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 183.25it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.54it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 186.65it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.50it/s]
-------------------------------------
| approxkl           | 0.0019297425 |
| clipfrac           | 0.23534378   |
| eplenmean          | 400          |
| eprewmean          | 261          |
| explained_variance | 0.213        |
| fps                | 1426         |
| nupdates           | 45           |
| policy_entropy     | 0.7590944    |
| policy_loss        | 0.0036989595 |
| serial_timesteps   | 18000        |
| time_elapsed       | 189          |
| time_remaining     | 1.47         |
| total_timesteps    | 540000       |
| true_eprew         | 183          |
| value_loss         | 104.455246   |
-------------------------------------
Current reward shaping 0.45999999999999996
Current self-play randomization 0.984
SP envs: 30/30
Other agent actions took 0.6079552173614502 seconds
Total simulation time for 400 steps: 3.5466244220733643 	 Other agent action time: 0 	 112.78329825692656 steps/s
Curr learning rate 0.0005454545454545455 	 Curr reward per step 0.666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.48it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.34it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.18it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 178.94it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.34it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.73it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 179.96it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 176.26it/s]
-------------------------------------
| approxkl           | 0.0025702834 |
| clipfrac           | 0.22760418   |
| eplenmean          | 400          |
| eprewmean          | 261          |
| explained_variance | 0.271        |
| fps                | 2984         |
| nupdates           | 46           |
| policy_entropy     | 0.72974247   |
| policy_loss        | 0.0042643705 |
| serial_timesteps   | 18400        |
| time_elapsed       | 193          |
| time_remaining     | 1.4          |
| total_timesteps    | 552000       |
| true_eprew         | 185          |
| value_loss         | 72.43973     |
-------------------------------------
Current reward shaping 0.44799999999999995
Current self-play randomization 0.9792
SP envs: 30/30
Other agent actions took 0.6092720031738281 seconds
Total simulation time for 400 steps: 3.5359933376312256 	 Other agent action time: 0 	 113.12238508570307 steps/s
Curr learning rate 0.0005353535353535353 	 Curr reward per step 0.66388

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.94it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.57it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 175.58it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 178.15it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 185.65it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 181.64it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.56it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 173.28it/s]
-------------------------------------
| approxkl           | 0.0015168746 |
| clipfrac           | 0.21573956   |
| eplenmean          | 400          |
| eprewmean          | 257          |
| explained_variance | 0.206        |
| fps                | 2985         |
| nupdates           | 47           |
| policy_entropy     | 0.74386203   |
| policy_loss        | 0.0026264421 |
| serial_timesteps   | 18800        |
| time_elapsed       | 197          |
| time_remaining     | 1.33         |
| total_timesteps    | 564000       |
| true_eprew         | 184          |
| value_loss         | 83.6557      |
-------------------------------------
Current reward shaping 0.43600000000000005
Current self-play randomization 0.9744
SP envs: 29/30
Other agent actions took 5.167956829071045 seconds
Total simulation time for 400 steps: 7.938626527786255 	 Other agent action time: 0 	 50.386549688405985 steps/s
Curr learning rate 0.0005252525252525252 	 Curr reward per step 0.6737376666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 178.44it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.33it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 165.99it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.02it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.83it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.84it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 164.96it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 165.71it/s]
-------------------------------------
| approxkl           | 0.0018647795 |
| clipfrac           | 0.22720835   |
| eplenmean          | 400          |
| eprewmean          | 268          |
| explained_variance | 0.229        |
| fps                | 1421         |
| nupdates           | 48           |
| policy_entropy     | 0.7398883    |
| policy_loss        | 0.0028121579 |
| serial_timesteps   | 19200        |
| time_elapsed       | 206          |
| time_remaining     | 1.28         |
| total_timesteps    | 576000       |
| true_eprew         | 193          |
| value_loss         | 74.54152     |
-------------------------------------
Current reward shaping 0.42400000000000004
Current self-play randomization 0.9696
SP envs: 28/30
Other agent actions took 5.175235271453857 seconds
Total simulation time for 400 steps: 7.961055755615234 	 Other agent action time: 0 	 50.24459220975369 steps/s
Curr learning rate 0.0005151515151515151 	 Curr reward per step 0.6260253333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.66it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 171.86it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.54it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 170.66it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 169.99it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 170.00it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 168.70it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 167.48it/s]
-------------------------------------
| approxkl           | 0.0016086723 |
| clipfrac           | 0.19896878   |
| eplenmean          | 400          |
| eprewmean          | 263          |
| explained_variance | 0.264        |
| fps                | 1417         |
| nupdates           | 49           |
| policy_entropy     | 0.73445517   |
| policy_loss        | 0.002504431  |
| serial_timesteps   | 19600        |
| time_elapsed       | 214          |
| time_remaining     | 1.24         |
| total_timesteps    | 588000       |
| true_eprew         | 191          |
| value_loss         | 85.827095    |
-------------------------------------
Current reward shaping 0.41200000000000003
Current self-play randomization 0.9648
SP envs: 29/30
Other agent actions took 5.167886018753052 seconds
Total simulation time for 400 steps: 7.990362644195557 	 Other agent action time: 0 	 50.060306122723006 steps/s
Curr learning rate 0.000505050505050505 	 Curr reward per step 0.6612859999999998

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.87it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.19it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.86it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.31it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.55it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.53it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.38it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.36it/s]
-------------------------------------
| approxkl           | 0.0013865888 |
| clipfrac           | 0.1940833    |
| eplenmean          | 400          |
| eprewmean          | 262          |
| explained_variance | 0.235        |
| fps                | 1415         |
| nupdates           | 50           |
| policy_entropy     | 0.710478     |
| policy_loss        | 0.0016808545 |
| serial_timesteps   | 20000        |
| time_elapsed       | 222          |
| time_remaining     | 1.19         |
| total_timesteps    | 600000       |
| true_eprew         | 191          |
| value_loss         | 79.26631     |
-------------------------------------
Current reward shaping 0.4
Current self-play randomization 0.96
../../thesis_data/dr_ppo/ppo_bc_train_simple/
PPO agent on index 0:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 2
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ←0  ↑1O 
X       X 
X D X S X 


Timestep: 3
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ←o  ↑1O 
X       X 
X D X S X 


Timestep: 4
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O   →o↑1O 
X       X 
X D X S X 


Timestep: 5
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o←1O 
X       X 
X D X S X 


Timestep: 6
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑0←1O 
X       X 
X D X S X 


Timestep: 7
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←0  ←1O 
X       X 
X D X S X 


Timestep: 8
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o  ←1O 
X       X 
X D X S X 


Timestep: 9
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o←1  O 
X       X 
X D X S X 


Timestep: 10
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O →o←1  O 
X       X 
X D X S X 


Timestep: 11
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o↑1  O 
X       X 
X D X S X 


Timestep: 12
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O →o↑1  O 
X       X 
X D X S X 


Timestep: 13
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →o→1O 
X       X 
X D X S X 


Timestep: 14
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o→1O 
X       X 
X D X S X 


Timestep: 15
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑0→1O 
X       X 
X D X S X 


Timestep: 16
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   →0→1O 
X       X 
X D X S X 


Timestep: 17
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←0←1  O 
X       X 
X D X S X 


Timestep: 18
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O →0←1  O 
X       X 
X D X S X 


Timestep: 19
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O →0←1  O 
X       X 
X D X S X 


Timestep: 20
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O →0←1  O 
X       X 
X D X S X 


Timestep: 21
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O →0←1  O 
X       X 
X D X S X 


Timestep: 22
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   →0→1O 
X       X 
X D X S X 


Timestep: 23
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←0  →1O 
X       X 
X D X S X 


Timestep: 24
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0  →oO 
X       X 
X D X S X 


Timestep: 25
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0  →oO 
X       X 
X D X S X 


Timestep: 26
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ↓0    X 
X D X S X 


Timestep: 27
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0←o  O 
X       X 
X D X S X 


Timestep: 28
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ↓0    X 
X D X S X 


Timestep: 29
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0←o  O 
X       X 
X D X S X 


Timestep: 30
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0↑o  O 
X       X 
X D X S X 


Timestep: 31
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0↑o  O 
X       X 
X D X S X 


Timestep: 32
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0↑o  O 
X       X 
X D X S X 


Timestep: 33
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o  O 
X ↓0    X 
X D X S X 


Timestep: 34
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 35
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø1X X 
O   ↑1  O 
X ↓d    X 
X D X S X 


Timestep: 36
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø2X X 
O     →1O 
X   →d  X 
X D X S X 


Timestep: 37
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø3X X 
O     →1O 
X   →d  X 
X D X S X 


Timestep: 38
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø4X X 
O     →1O 
X   →d  X 
X D X S X 


Timestep: 39
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø5X X 
O     →1O 
X   →d  X 
X D X S X 


Timestep: 40
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø6X X 
O   ↑d→1O 
X       X 
X D X S X 


Timestep: 41
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø7X X 
O   ↑d→oO 
X       X 
X D X S X 


Timestep: 42
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø8X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 43
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø9X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 44
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø10X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 45
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø11X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 46
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø12X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 47
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø13X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 48
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø14X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 49
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø15X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 50
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø16X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 51
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø17X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 52
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø18X X 
O   ↑d→oO 
X       X 
X D X S X 


Timestep: 53
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø19X X 
O   ↑d→oO 
X       X 
X D X S X 


Timestep: 54
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d→oO 
X       X 
X D X S X 


Timestep: 55
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 5 
X X P X X 
O   ↑s→oO 
X       X 
X D X S X 


Timestep: 56
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   ↓s  X 
X D X S X 


Timestep: 57
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   ↑s  X 
X D X S X 


Timestep: 58
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   ↑s  X 
X D X S X 


Timestep: 59
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   ↑s  X 
X D X S X 


Timestep: 60
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑s  X 
X D X S X 


Timestep: 61
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑s  X 
X D X S X 


Timestep: 62
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑s  X 
X D X S X 


Timestep: 63
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑s  X 
X D X S X 


Timestep: 64
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑s  X 
X D X S X 


Timestep: 65
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s→oO 
X       X 
X D X S X 


Timestep: 66
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s→oO 
X       X 
X D X S X 


Timestep: 67
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s→oO 
X       X 
X D X S X 


Timestep: 68
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s→oO 
X       X 
X D X S X 


Timestep: 69
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s←oO 
X       X 
X D X S X 


Timestep: 70
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ←s  ←oO 
X       X 
X D X S X 


Timestep: 71
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s←oO 
X       X 
X D X S X 


Timestep: 72
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s←oO 
X       X 
X D X S X 


Timestep: 73
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   ↓s  X 
X D X S X 


Timestep: 74
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   ↑s  X 
X D X S X 


Timestep: 75
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   ↑s  X 
X D X S X 


Timestep: 76
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   ↑s  X 
X D X S X 


Timestep: 77
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑s  X 
X D X S X 


Timestep: 78
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑s  X 
X D X S X 


Timestep: 79
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↓s  X 
X D X S X 


Timestep: 80
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑s  X 
X D X S X 


Timestep: 81
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑1  O 
X   ↑s  X 
X D X S X 


Timestep: 82
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X   ↑s  X 
X D X S X 


Timestep: 83
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑s→1O 
X       X 
X D X S X 


Timestep: 84
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑s→1O 
X       X 
X D X S X 


Timestep: 85
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s↑1O 
X       X 
X D X S X 


Timestep: 86
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s↑1O 
X       X 
X D X S X 


Timestep: 87
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s↑1O 
X       X 
X D X S X 


Timestep: 88
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s→1O 
X       X 
X D X S X 


Timestep: 89
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑s→1O 
X       X 
X D X S X 


Timestep: 90
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s→1O 
X       X 
X D X S X 


Timestep: 91
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s→1O 
X       X 
X D X S X 


Timestep: 92
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s←1O 
X       X 
X D X S X 


Timestep: 93
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s←1O 
X       X 
X D X S X 


Timestep: 94
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s←1O 
X       X 
X D X S X 


Timestep: 95
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →s←1O 
X       X 
X D X S X 


Timestep: 96
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑s→1O 
X       X 
X D X S X 


Timestep: 97
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X   ↓s  X 
X D X S X 


Timestep: 98
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑s→1O 
X       X 
X D X S X 


Timestep: 99
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑s→1O 
X       X 
X D X S X 


tot rew 40 tot rew shaped 40
PPO agent on index 1:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X ↑0    X 
X D X S X 


Timestep: 2
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X ↑0    X 
X D X S X 


Timestep: 3
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑oO 
X ↑0    X 
X D X S X 


Timestep: 4
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X ↑0    X 
X D X S X 


Timestep: 5
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   →0  X 
X D X S X 


Timestep: 6
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X     →0X 
X D X S X 


Timestep: 7
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑1  O 
X     →0X 
X D X S X 


Timestep: 8
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X     →0X 
X D X S X 


Timestep: 9
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X     →0X 
X D X S X 


Timestep: 10
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X   ←0  X 
X D X S X 


Timestep: 11
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ←0    X 
X D X S X 


Timestep: 12
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑1  O 
X ←0    X 
X D X S X 


Timestep: 13
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X ←0    X 
X D X S X 


Timestep: 14
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
X ←0    X 
X D X S X 


Timestep: 15
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ←0    X 
X D X S X 


Timestep: 16
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0↑o  O 
X       X 
X D X S X 


Timestep: 17
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø1X X 
O ↑0↑1  O 
X       X 
X D X S X 


Timestep: 18
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø2X X 
O ↑0  →1O 
X       X 
X D X S X 


Timestep: 19
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø3X X 
O ↑0  →oO 
X       X 
X D X S X 


Timestep: 20
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X ø4X X 
O       O 
X ↓0  ↓oX 
X D X S X 


Timestep: 21
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 3 
X X ø5X X 
O       O 
X ↓d  →oX 
X D X S X 


Timestep: 22
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø6X X 
O ↑d    O 
X     →oX 
X D X S X 


Timestep: 23
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø7X X 
O ↑d    O 
X     →oX 
X D X S X 


Timestep: 24
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø8X X 
O   →d  O 
X     →oX 
X D X S X 


Timestep: 25
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø9X X 
O   ↑d  O 
X     →oX 
X D X S X 


Timestep: 26
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø10X X 
O   ↑d  O 
X   ←o  X 
X D X S X 


Timestep: 27
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø11X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 28
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø12X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 29
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø13X X 
O     →dO 
X   ↑o  X 
X D X S X 


Timestep: 30
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø14X X 
O   ↑o↑dO 
X       X 
X D X S X 


Timestep: 31
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø15X X 
O ←o  ↑dO 
X       X 
X D X S X 


Timestep: 32
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø16X X 
O ←o  ↑dO 
X       X 
X D X S X 


Timestep: 33
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø17XdX 
O   →o↑0O 
X       X 
X D X S X 


Timestep: 34
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X ø18XdX 
O       O 
X   ↓o↓0X 
X D X S X 


Timestep: 35
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø19XdX 
O       O 
X   ↓1↓0X 
X D XoS X 


Timestep: 36
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O       O 
X   ↓o↓0X 
X D X S X 


Timestep: 37
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O       O 
X   ↓1↓0X 
X D XoS X 


Timestep: 38
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O       O 
X   ↓1←0X 
X D XoS X 


Timestep: 39
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O       O 
X ←1  ←0X 
X D XoS X 


Timestep: 40
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O       O 
X ←1←0  X 
X D XoS X 


Timestep: 41
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O       O 
X ←1←0  X 
X D XoS X 


Timestep: 42
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O       O 
X ←1←0  X 
X D XoS X 


Timestep: 43
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O       O 
X ←1←0  X 
X D XoS X 


Timestep: 44
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O       O 
X ←1←0  X 
X D XoS X 


Timestep: 45
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O       O 
X ←1←0  X 
X D XoS X 


Timestep: 46
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O       O 
X ←1  →0X 
X D XoS X 


Timestep: 47
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O       O 
X ←1  →0X 
X D XoS X 


Timestep: 48
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O       O 
X ←1  →0X 
X D XoS X 


Timestep: 49
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O       O 
X ←1  →0X 
X D XoS X 


Timestep: 50
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑0O 
X ←1    X 
X D XoS X 


Timestep: 51
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑0O 
X ←1    X 
X D XoS X 


Timestep: 52
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑0O 
X ↓1    X 
X D XoS X 


Timestep: 53
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑0O 
X ↓d    X 
X D XoS X 


Timestep: 54
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O       O 
X   →d↓0X 
X D XoS X 


Timestep: 55
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑d  O 
X     ↓0X 
X D XoS X 


Timestep: 56
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 5 
X X P XdX 
O   ↑s  O 
X     ↓0X 
X D XoS X 


Timestep: 57
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O     →sO 
X     ↓0X 
X D XoS X 


Timestep: 58
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ←s  O 
X   ←0  X 
X D XoS X 


Timestep: 59
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O     →sO 
X   ←0  X 
X D XoS X 


Timestep: 60
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O     ↑sO 
X   ←0  X 
X D XoS X 


Timestep: 61
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X P XdX 
O       O 
X   ←0↓sX 
X D XoS X 


Timestep: 62
Joint action taken: ('↑', 'interact') 	 Reward: 20 + shape * 0 
X X P XdX 
O   ↑0  O 
X     ↓1X 
X D XoS X 


Timestep: 63
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑0  O 
X     ↓1X 
X D XoS X 


Timestep: 64
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑0  O 
X     ↓1X 
X D XoS X 


Timestep: 65
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑0  O 
X     ↓1X 
X D XoS X 


Timestep: 66
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P XdX 
O     →0O 
X     ↓1X 
X D XoS X 


Timestep: 67
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P XdX 
O     →0O 
X     ↓1X 
X D XoS X 


Timestep: 68
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X P XdX 
O     →oO 
X     ↓1X 
X D XoS X 


Timestep: 69
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ←o  O 
X   ←1  X 
X D XoS X 


Timestep: 70
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ←o  O 
X   ↑1  X 
X D XoS X 


Timestep: 71
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ←o  O 
X ←1    X 
X D XoS X 


Timestep: 72
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ←o  O 
X   →1  X 
X D XoS X 


Timestep: 73
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑o  O 
X   →1  X 
X D XoS X 


Timestep: 74
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑o  O 
X   →1  X 
X D XoS X 


Timestep: 75
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑o  O 
X   →1  X 
X D XoS X 


Timestep: 76
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑o  O 
X   →1  X 
X D XoS X 


Timestep: 77
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑o  O 
X ←1    X 
X D XoS X 


Timestep: 78
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 3 
X X ø-XdX 
O   ↑0  O 
X   →1  X 
X D XoS X 


Timestep: 79
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O     →0O 
X   →1  X 
X D XoS X 


Timestep: 80
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O     →0O 
X   →1  X 
X D XoS X 


Timestep: 81
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O     →0O 
X   →1  X 
X D XoS X 


Timestep: 82
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O     →0O 
X   →1  X 
X D XoS X 


Timestep: 83
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   ←0  O 
X   →1  X 
X D XoS X 


Timestep: 84
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   ←0  O 
X   ↓1  X 
X D XoS X 


Timestep: 85
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   ←0  O 
X     →1X 
X D XoS X 


Timestep: 86
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   ←0  O 
X   ←1  X 
X D XoS X 


Timestep: 87
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O ←0    O 
X   ←1  X 
X D XoS X 


Timestep: 88
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O ←0    O 
X   ←1  X 
X D XoS X 


Timestep: 89
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O ←o    O 
X   ←1  X 
X D XoS X 


Timestep: 90
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O ←o    O 
X   ←1  X 
X D XoS X 


Timestep: 91
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O →o    O 
X   ↑1  X 
X D XoS X 


Timestep: 92
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O →o↑1  O 
X       X 
X D XoS X 


Timestep: 93
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   →o→1O 
X       X 
X D XoS X 


Timestep: 94
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   →o→oO 
X       X 
X D XoS X 


Timestep: 95
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   →o←oO 
X       X 
X D XoS X 


Timestep: 96
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   →o←oO 
X       X 
X D XoS X 


Timestep: 97
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   →o←oO 
X       X 
X D XoS X 


Timestep: 98
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   →o←oO 
X       X 
X D XoS X 


Timestep: 99
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   ↑o←oO 
X       X 
X D XoS X 


tot rew 40 tot rew shaped 34
../../thesis_data/dr_ppo/ppo_bc_train_simple/
SP envs: 29/30
Other agent actions took 5.144634246826172 seconds
Total simulation time for 400 steps: 7.885116338729858 	 Other agent action time: 0 	 50.728484250168 steps/s
Curr learning rate 0.000494949494949495 	 Curr reward per step 0.6285333333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.21it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 184.12it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 185.29it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 187.32it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 185.85it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.22it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.90it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.38it/s]
-------------------------------------
| approxkl           | 0.0015663274 |
| clipfrac           | 0.20602083   |
| eplenmean          | 400          |
| eprewmean          | 258          |
| explained_variance | 0.253        |
| fps                | 1437         |
| nupdates           | 51           |
| policy_entropy     | 0.7412284    |
| policy_loss        | 0.0021814303 |
| serial_timesteps   | 20400        |
| time_elapsed       | 233          |
| time_remaining     | 1.14         |
| total_timesteps    | 612000       |
| true_eprew         | 190          |
| value_loss         | 80.85314     |
-------------------------------------
Current reward shaping 0.388
Current self-play randomization 0.9552
SP envs: 30/30
Other agent actions took 0.6047821044921875 seconds
Total simulation time for 400 steps: 3.539924383163452 	 Other agent action time: 0 	 112.99676397113888 steps/s
Curr learning rate 0.0004848484848484849 	 Curr reward per step 0.6541210000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.86it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 171.36it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.03it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 173.75it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.28it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 184.14it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 186.96it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.49it/s]
-------------------------------------
| approxkl           | 0.0016596531 |
| clipfrac           | 0.2250208    |
| eplenmean          | 400          |
| eprewmean          | 260          |
| explained_variance | 0.241        |
| fps                | 2985         |
| nupdates           | 52           |
| policy_entropy     | 0.73134047   |
| policy_loss        | 0.0026495857 |
| serial_timesteps   | 20800        |
| time_elapsed       | 237          |
| time_remaining     | 1.06         |
| total_timesteps    | 624000       |
| true_eprew         | 192          |
| value_loss         | 79.458786    |
-------------------------------------
Current reward shaping 0.376
Current self-play randomization 0.9504
SP envs: 28/30
Other agent actions took 5.139450788497925 seconds
Total simulation time for 400 steps: 7.916076421737671 	 Other agent action time: 0 	 50.530083173729054 steps/s
Curr learning rate 0.0004747474747474748 	 Curr reward per step 0.626428

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 165.64it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.60it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.09it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 171.71it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.34it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.34it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 175.20it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 177.01it/s]
-------------------------------------
| approxkl           | 0.0024891414 |
| clipfrac           | 0.23843753   |
| eplenmean          | 400          |
| eprewmean          | 255          |
| explained_variance | 0.241        |
| fps                | 1426         |
| nupdates           | 53           |
| policy_entropy     | 0.7314435    |
| policy_loss        | 0.0041801576 |
| serial_timesteps   | 21200        |
| time_elapsed       | 245          |
| time_remaining     | 1            |
| total_timesteps    | 636000       |
| true_eprew         | 190          |
| value_loss         | 83.12112     |
-------------------------------------
Current reward shaping 0.364
Current self-play randomization 0.9456
SP envs: 29/30
Other agent actions took 5.16186785697937 seconds
Total simulation time for 400 steps: 7.949872255325317 	 Other agent action time: 0 	 50.31527390041459 steps/s
Curr learning rate 0.0004646464646464647 	 Curr reward per step 0.6333316666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.99it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 182.27it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.04it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 181.33it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.94it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 182.05it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 182.13it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 181.44it/s]
------------------------------------
| approxkl           | 0.001581925 |
| clipfrac           | 0.20167708  |
| eplenmean          | 400         |
| eprewmean          | 256         |
| explained_variance | 0.248       |
| fps                | 1425        |
| nupdates           | 54          |
| policy_entropy     | 0.7223247   |
| policy_loss        | 0.002560702 |
| serial_timesteps   | 21600       |
| time_elapsed       | 254         |
| time_remaining     | 0.94        |
| total_timesteps    | 648000      |
| true_eprew         | 192         |
| value_loss         | 74.255554   |
------------------------------------
Current reward shaping 0.352
Current self-play randomization 0.9408
SP envs: 28/30
Other agent actions took 5.131928443908691 seconds
Total simulation time for 400 steps: 7.922846078872681 	 Other agent action time: 0 	 50.48690786340694 steps/s
Curr learning rate 0.00045454545454545455 	 Curr reward per step 0.5886586666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 180.78it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 174.28it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.06it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 179.60it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.09it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 187.81it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 190.44it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 189.31it/s]
--------------------------------------
| approxkl           | 0.0007171914  |
| clipfrac           | 0.11887499    |
| eplenmean          | 400           |
| eprewmean          | 247           |
| explained_variance | 0.284         |
| fps                | 1430          |
| nupdates           | 55            |
| policy_entropy     | 0.70378053    |
| policy_loss        | 0.00020172125 |
| serial_timesteps   | 22000         |
| time_elapsed       | 262           |
| time_remaining     | 0.874         |
| total_timesteps    | 660000        |
| true_eprew         | 187           |
| value_loss         | 86.45769      |
--------------------------------------
Current reward shaping 0.33999999999999997
Current self-play randomization 0.9359999999999999
SP envs: 29/30
Other agent actions took 5.1458752155303955 seconds
Total simulation time for 400 steps: 7.956148147583008 	 Other agent action time: 0 	 50.275584690000485 steps/s
Curr learning rate 0.0004444444444444444 	 Curr reward per step 0.6160899999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.81it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.52it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.52it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 177.16it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.25it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 181.70it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 179.73it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 184.28it/s]
--------------------------------------
| approxkl           | 0.00055516895 |
| clipfrac           | 0.106854156   |
| eplenmean          | 400           |
| eprewmean          | 244           |
| explained_variance | 0.305         |
| fps                | 1423          |
| nupdates           | 56            |
| policy_entropy     | 0.72328436    |
| policy_loss        | 6.757948e-05  |
| serial_timesteps   | 22400         |
| time_elapsed       | 271           |
| time_remaining     | 0.806         |
| total_timesteps    | 672000        |
| true_eprew         | 187           |
| value_loss         | 76.59312      |
--------------------------------------
Current reward shaping 0.32799999999999996
Current self-play randomization 0.9312
SP envs: 30/30
Other agent actions took 0.6097469329833984 seconds
Total simulation time for 400 steps: 3.5438287258148193 	 Other agent action time: 0 	 112.8722720390584 steps/s
Curr learning rate 0.00043434343434343433 	 Curr reward per step 0.618634

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.37it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 174.48it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.30it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 175.24it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.01it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 176.24it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 175.27it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.87it/s]
---------------------------------------
| approxkl           | 0.0005948912   |
| clipfrac           | 0.108354166    |
| eplenmean          | 400            |
| eprewmean          | 244            |
| explained_variance | 0.266          |
| fps                | 2978           |
| nupdates           | 57             |
| policy_entropy     | 0.7173248      |
| policy_loss        | -0.00011220549 |
| serial_timesteps   | 22800          |
| time_elapsed       | 275            |
| time_remaining     | 0.723          |
| total_timesteps    | 684000         |
| true_eprew         | 189            |
| value_loss         | 78.71929       |
---------------------------------------
Current reward shaping 0.31599999999999995
Current self-play randomization 0.9264
SP envs: 30/30
Other agent actions took 0.6031901836395264 seconds
Total simulation time for 400 steps: 3.515517473220825 	 Other agent action time: 0 	 113.78125782248793 steps/s
Curr learning rate 0.00042424242424242425 	 Curr reward per step 0.634345

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.85it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 181.56it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 180.30it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 177.14it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 175.27it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.05it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.38it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 173.14it/s]
---------------------------------------
| approxkl           | 0.00045309364  |
| clipfrac           | 0.08604168     |
| eplenmean          | 400            |
| eprewmean          | 249            |
| explained_variance | 0.243          |
| fps                | 3001           |
| nupdates           | 58             |
| policy_entropy     | 0.7116953      |
| policy_loss        | -0.00015224195 |
| serial_timesteps   | 23200          |
| time_elapsed       | 279            |
| time_remaining     | 0.641          |
| total_timesteps    | 696000         |
| true_eprew         | 194            |
| value_loss         | 77.04218       |
---------------------------------------
Current reward shaping 0.30400000000000005
Current self-play randomization 0.9216
SP envs: 27/30
Other agent actions took 5.207336902618408 seconds
Total simulation time for 400 steps: 8.005983829498291 	 Other agent action time: 0 	 49.96262901833349 steps/s
Curr learning rate 0.0004141414141414141 	 Curr reward per step 0.5659053333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.47it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 178.38it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 177.67it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 180.36it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.30it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 177.13it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 181.05it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.28it/s]
--------------------------------------
| approxkl           | 0.0008731306  |
| clipfrac           | 0.14443749    |
| eplenmean          | 400           |
| eprewmean          | 244           |
| explained_variance | 0.319         |
| fps                | 1414          |
| nupdates           | 59            |
| policy_entropy     | 0.7318423     |
| policy_loss        | 0.00033449222 |
| serial_timesteps   | 23600         |
| time_elapsed       | 287           |
| time_remaining     | 0.568         |
| total_timesteps    | 708000        |
| true_eprew         | 192           |
| value_loss         | 88.94831      |
--------------------------------------
Current reward shaping 0.29200000000000004
Current self-play randomization 0.9168000000000001
SP envs: 29/30
Other agent actions took 5.157573223114014 seconds
Total simulation time for 400 steps: 7.93091344833374 	 Other agent action time: 0 	 50.43555229871267 steps/s
Curr learning rate 0.00040404040404040404 	 Curr reward per step 0.602421

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.43it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 186.77it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.84it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 178.55it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.15it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.66it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.23it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.69it/s]
-------------------------------------
| approxkl           | 0.0022103381 |
| clipfrac           | 0.23707287   |
| eplenmean          | 400          |
| eprewmean          | 240          |
| explained_variance | 0.273        |
| fps                | 1428         |
| nupdates           | 60           |
| policy_entropy     | 0.72483814   |
| policy_loss        | 0.0037149652 |
| serial_timesteps   | 24000        |
| time_elapsed       | 296          |
| time_remaining     | 0.493        |
| total_timesteps    | 720000       |
| true_eprew         | 190          |
| value_loss         | 81.74115     |
-------------------------------------
Current reward shaping 0.28
Current self-play randomization 0.912
SP envs: 26/30
Other agent actions took 5.164479732513428 seconds
Total simulation time for 400 steps: 7.942548036575317 	 Other agent action time: 0 	 50.36167211806663 steps/s
Curr learning rate 0.00039393939393939396 	 Curr reward per step 0.56707

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 174.30it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 186.46it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 189.66it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 189.18it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.26it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.83it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.57it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 190.28it/s]
-------------------------------------
| approxkl           | 0.0016634235 |
| clipfrac           | 0.21640626   |
| eplenmean          | 400          |
| eprewmean          | 233          |
| explained_variance | 0.238        |
| fps                | 1428         |
| nupdates           | 61           |
| policy_entropy     | 0.72631663   |
| policy_loss        | 0.0027944935 |
| serial_timesteps   | 24400        |
| time_elapsed       | 304          |
| time_remaining     | 0.415        |
| total_timesteps    | 732000       |
| true_eprew         | 185          |
| value_loss         | 90.59708     |
-------------------------------------
Current reward shaping 0.268
Current self-play randomization 0.9072
SP envs: 25/30
Other agent actions took 5.222759008407593 seconds
Total simulation time for 400 steps: 8.029728174209595 	 Other agent action time: 0 	 49.814886795887574 steps/s
Curr learning rate 0.0003838383838383839 	 Curr reward per step 0.5263179999999998

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.78it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 176.93it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.85it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 181.16it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 180.93it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.49it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 181.76it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 178.09it/s]
-------------------------------------
| approxkl           | 0.001400131  |
| clipfrac           | 0.18000002   |
| eplenmean          | 400          |
| eprewmean          | 225          |
| explained_variance | 0.308        |
| fps                | 1410         |
| nupdates           | 62           |
| policy_entropy     | 0.7323656    |
| policy_loss        | 0.0024927417 |
| serial_timesteps   | 24800        |
| time_elapsed       | 313          |
| time_remaining     | 0.336        |
| total_timesteps    | 744000       |
| true_eprew         | 180          |
| value_loss         | 98.289856    |
-------------------------------------
Current reward shaping 0.256
Current self-play randomization 0.9024
SP envs: 27/30
Other agent actions took 5.2341132164001465 seconds
Total simulation time for 400 steps: 8.292604923248291 	 Other agent action time: 0 	 48.23574783824577 steps/s
Curr learning rate 0.0003737373737373737 	 Curr reward per step 0.5460986666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 171.99it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 176.14it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 174.43it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 175.43it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 175.86it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 173.81it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 174.61it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.27it/s]
-------------------------------------
| approxkl           | 0.0012202945 |
| clipfrac           | 0.18846872   |
| eplenmean          | 400          |
| eprewmean          | 221          |
| explained_variance | 0.252        |
| fps                | 1366         |
| nupdates           | 63           |
| policy_entropy     | 0.7051263    |
| policy_loss        | 0.0020100882 |
| serial_timesteps   | 25200        |
| time_elapsed       | 321          |
| time_remaining     | 0.255        |
| total_timesteps    | 756000       |
| true_eprew         | 179          |
| value_loss         | 88.84902     |
-------------------------------------
Current reward shaping 0.244
Current self-play randomization 0.8976
SP envs: 27/30
Other agent actions took 5.260703086853027 seconds
Total simulation time for 400 steps: 8.086819648742676 	 Other agent action time: 0 	 49.463202763794946 steps/s
Curr learning rate 0.0003636363636363636 	 Curr reward per step 0.5596943333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.40it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.71it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 175.80it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 179.99it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 179.75it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 189.63it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.79it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 182.87it/s]
-------------------------------------
| approxkl           | 0.0015281963 |
| clipfrac           | 0.18459377   |
| eplenmean          | 400          |
| eprewmean          | 220          |
| explained_variance | 0.252        |
| fps                | 1401         |
| nupdates           | 64           |
| policy_entropy     | 0.71258295   |
| policy_loss        | 0.0023475937 |
| serial_timesteps   | 25600        |
| time_elapsed       | 330          |
| time_remaining     | 0.172        |
| total_timesteps    | 768000       |
| true_eprew         | 179          |
| value_loss         | 87.42879     |
-------------------------------------
Current reward shaping 0.23199999999999998
Current self-play randomization 0.8928
SP envs: 22/30
Other agent actions took 5.252551794052124 seconds
Total simulation time for 400 steps: 8.03913402557373 	 Other agent action time: 0 	 49.756602978323045 steps/s
Curr learning rate 0.00035353535353535354 	 Curr reward per step 0.44117733333333325

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 163.90it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.21it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 171.51it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.57it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 170.62it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 169.24it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.27it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 170.95it/s]
-------------------------------------
| approxkl           | 0.0033212162 |
| clipfrac           | 0.26751047   |
| eplenmean          | 400          |
| eprewmean          | 210          |
| explained_variance | 0.349        |
| fps                | 1405         |
| nupdates           | 65           |
| policy_entropy     | 0.7677805    |
| policy_loss        | 0.0059106555 |
| serial_timesteps   | 26000        |
| time_elapsed       | 338          |
| time_remaining     | 0.0868       |
| total_timesteps    | 780000       |
| true_eprew         | 173          |
| value_loss         | 112.9573     |
-------------------------------------
Current reward shaping 0.21999999999999997
Current self-play randomization 0.888
SP envs: 26/30
Other agent actions took 11.183940649032593 seconds
Total simulation time for 400 steps: 18.64083504676819 	 Other agent action time: 0 	 21.458266166533623 steps/s
Curr learning rate 0.0003434343434343434 	 Curr reward per step 0.5270816666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.05it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.44it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 163.10it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 159.60it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.85it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 165.12it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 163.50it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.54it/s]
-------------------------------------
| approxkl           | 0.0016911824 |
| clipfrac           | 0.2371771    |
| eplenmean          | 400          |
| eprewmean          | 203          |
| explained_variance | 0.306        |
| fps                | 626          |
| nupdates           | 66           |
| policy_entropy     | 0.72941977   |
| policy_loss        | 0.0034992925 |
| serial_timesteps   | 26400        |
| time_elapsed       | 358          |
| time_remaining     | 0            |
| total_timesteps    | 792000       |
| true_eprew         | 168          |
| value_loss         | 89.28354     |
-------------------------------------
Current reward shaping 0.20799999999999996
Current self-play randomization 0.8832
LOADING BC MODEL FROM: seed3/worker4
Loading a model without an environment, this model cannot be trained until it has a valid environment.
Loaded MediumLevelPlanner from /home/ubuntu/human_aware_rl/overcooked_ai/overcooked_ai_py/data/planners/simple_am.pkl
TOT NUM UPDATES 66
SP envs: 27/30
Other agent actions took 5.172546863555908 seconds
Total simulation time for 400 steps: 7.923832654953003 	 Other agent action time: 0 	 50.480621868000874 steps/s
Curr learning rate 0.001 	 Curr reward per step 0.5307186666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 175.80it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 180.93it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 182.62it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 181.79it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 182.50it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.49it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 181.92it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.11it/s]
------------------------------------
| approxkl           | 0.007734973 |
| clipfrac           | 0.34430212  |
| eplenmean          | 400         |
| eprewmean          | 212         |
| explained_variance | 0.374       |
| fps                | 1428        |
| nupdates           | 1           |
| policy_entropy     | 0.6907142   |
| policy_loss        | 0.010257212 |
| serial_timesteps   | 400         |
| time_elapsed       | 8.4         |
| time_remaining     | 9.1         |
| total_timesteps    | 12000       |
| true_eprew         | 179         |
| value_loss         | 92.01445    |
------------------------------------
Current reward shaping 0.988
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6075534820556641 seconds
Total simulation time for 400 steps: 3.525175094604492 	 Other agent action time: 0 	 113.46954102002644 steps/s
Curr learning rate 0.00098989898989899 	 Curr reward per step 0.9285249999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 177.57it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 186.07it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 186.12it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 187.54it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.09it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.45it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 187.56it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 187.31it/s]
------------------------------------
| approxkl           | 0.013083838 |
| clipfrac           | 0.37173966  |
| eplenmean          | 400         |
| eprewmean          | 292         |
| explained_variance | 0.211       |
| fps                | 3007        |
| nupdates           | 2           |
| policy_entropy     | 0.69988316  |
| policy_loss        | 0.015876064 |
| serial_timesteps   | 800         |
| time_elapsed       | 12.4        |
| time_remaining     | 6.61        |
| total_timesteps    | 24000       |
| true_eprew         | 189         |
| value_loss         | 128.02452   |
------------------------------------
Current reward shaping 0.976
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6036167144775391 seconds
Total simulation time for 400 steps: 3.5235342979431152 	 Other agent action time: 0 	 113.52238013789236 steps/s
Curr learning rate 0.0009797979797979799 	 Curr reward per step 0.8392186666666664

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 181.74it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 190.64it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 189.57it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 190.01it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 186.91it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 187.88it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.83it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 188.04it/s]
-------------------------------------
| approxkl           | 0.0061181094 |
| clipfrac           | 0.388125     |
| eplenmean          | 400          |
| eprewmean          | 306          |
| explained_variance | 0.228        |
| fps                | 3018         |
| nupdates           | 3            |
| policy_entropy     | 0.7869917    |
| policy_loss        | 0.011527384  |
| serial_timesteps   | 1200         |
| time_elapsed       | 16.4         |
| time_remaining     | 5.73         |
| total_timesteps    | 36000        |
| true_eprew         | 187          |
| value_loss         | 110.801575   |
-------------------------------------
Current reward shaping 0.964
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6100761890411377 seconds
Total simulation time for 400 steps: 3.516309976577759 	 Other agent action time: 0 	 113.7556138862647 steps/s
Curr learning rate 0.0009696969696969698 	 Curr reward per step 0.8493553333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.33it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.63it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.66it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 179.48it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.99it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.36it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 178.87it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 180.52it/s]
-------------------------------------
| approxkl           | 0.002594761  |
| clipfrac           | 0.2842083    |
| eplenmean          | 400          |
| eprewmean          | 337          |
| explained_variance | 0.24         |
| fps                | 3002         |
| nupdates           | 4            |
| policy_entropy     | 0.7927307    |
| policy_loss        | 0.0043906895 |
| serial_timesteps   | 1600         |
| time_elapsed       | 20.4         |
| time_remaining     | 5.26         |
| total_timesteps    | 48000        |
| true_eprew         | 189          |
| value_loss         | 107.6532     |
-------------------------------------
Current reward shaping 0.952
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.605266809463501 seconds
Total simulation time for 400 steps: 3.5045433044433594 	 Other agent action time: 0 	 114.13755381274525 steps/s
Curr learning rate 0.0009595959595959597 	 Curr reward per step 0.8278680000000002

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 181.38it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 188.95it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 190.18it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 187.65it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 188.04it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.76it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 188.71it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 189.09it/s]
-------------------------------------
| approxkl           | 0.0029864507 |
| clipfrac           | 0.28175002   |
| eplenmean          | 400          |
| eprewmean          | 339          |
| explained_variance | 0.203        |
| fps                | 3032         |
| nupdates           | 5            |
| policy_entropy     | 0.7581075    |
| policy_loss        | 0.0049427813 |
| serial_timesteps   | 2000         |
| time_elapsed       | 24.3         |
| time_remaining     | 4.95         |
| total_timesteps    | 60000        |
| true_eprew         | 185          |
| value_loss         | 106.932396   |
-------------------------------------
Current reward shaping 0.94
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6097598075866699 seconds
Total simulation time for 400 steps: 3.489739179611206 	 Other agent action time: 0 	 114.62174661562078 steps/s
Curr learning rate 0.0009494949494949496 	 Curr reward per step 0.8253599999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 179.74it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 185.01it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 186.39it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 186.59it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 187.67it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 186.21it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 185.56it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 178.60it/s]
-------------------------------------
| approxkl           | 0.0033662305 |
| clipfrac           | 0.3078021    |
| eplenmean          | 400          |
| eprewmean          | 335          |
| explained_variance | 0.259        |
| fps                | 3037         |
| nupdates           | 6            |
| policy_entropy     | 0.7692424    |
| policy_loss        | 0.005010541  |
| serial_timesteps   | 2400         |
| time_elapsed       | 28.3         |
| time_remaining     | 4.71         |
| total_timesteps    | 72000        |
| true_eprew         | 184          |
| value_loss         | 101.63706    |
-------------------------------------
Current reward shaping 0.928
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.5992083549499512 seconds
Total simulation time for 400 steps: 3.499640464782715 	 Other agent action time: 0 	 114.29745541727674 steps/s
Curr learning rate 0.0009393939393939395 	 Curr reward per step 0.8255626666666669

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 181.37it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 186.91it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 189.31it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 189.84it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 189.15it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 190.44it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 189.90it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 188.62it/s]
-------------------------------------
| approxkl           | 0.003820983  |
| clipfrac           | 0.31634384   |
| eplenmean          | 400          |
| eprewmean          | 332          |
| explained_variance | 0.327        |
| fps                | 3037         |
| nupdates           | 7            |
| policy_entropy     | 0.75095195   |
| policy_loss        | 0.0070001394 |
| serial_timesteps   | 2800         |
| time_elapsed       | 32.2         |
| time_remaining     | 4.53         |
| total_timesteps    | 84000        |
| true_eprew         | 184          |
| value_loss         | 94.612335    |
-------------------------------------
Current reward shaping 0.916
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6094341278076172 seconds
Total simulation time for 400 steps: 3.5239996910095215 	 Other agent action time: 0 	 113.50738793209482 steps/s
Curr learning rate 0.0009292929292929292 	 Curr reward per step 0.8473413333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 176.79it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 185.08it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 183.84it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 184.31it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 181.97it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 181.84it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 181.60it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 183.21it/s]
------------------------------------
| approxkl           | 0.011709877 |
| clipfrac           | 0.39101043  |
| eplenmean          | 400         |
| eprewmean          | 334         |
| explained_variance | 0.201       |
| fps                | 3007        |
| nupdates           | 8           |
| policy_entropy     | 0.73898935  |
| policy_loss        | 0.014794285 |
| serial_timesteps   | 3200        |
| time_elapsed       | 36.2        |
| time_remaining     | 4.38        |
| total_timesteps    | 96000       |
| true_eprew         | 186         |
| value_loss         | 101.282486  |
------------------------------------
Current reward shaping 0.904
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6354169845581055 seconds
Total simulation time for 400 steps: 3.790540933609009 	 Other agent action time: 0 	 105.52583575958282 steps/s
Curr learning rate 0.0009191919191919192 	 Curr reward per step 0.8151873333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 148.63it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 152.77it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 149.68it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 172.55it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.16it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 177.06it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 174.37it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 173.19it/s]
-------------------------------------
| approxkl           | 0.0028332402 |
| clipfrac           | 0.26645836   |
| eplenmean          | 400          |
| eprewmean          | 331          |
| explained_variance | 0.222        |
| fps                | 2780         |
| nupdates           | 9            |
| policy_entropy     | 0.7306318    |
| policy_loss        | 0.004461354  |
| serial_timesteps   | 3600         |
| time_elapsed       | 40.5         |
| time_remaining     | 4.28         |
| total_timesteps    | 108000       |
| true_eprew         | 186          |
| value_loss         | 101.26838    |
-------------------------------------
Current reward shaping 0.892
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 1.9205207824707031 seconds
Total simulation time for 400 steps: 20.162497997283936 	 Other agent action time: 0 	 19.83881164198422 steps/s
Curr learning rate 0.0009090909090909091 	 Curr reward per step 0.7823246666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8:  80%|████████  | 8/10 [00:00<00:00, 73.71it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 77.85it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 94.35it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 93.91it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 132.90it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 136.78it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 125.52it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 141.86it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 173.06it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 155.39it/s]
-------------------------------------
| approxkl           | 0.0024943906 |
| clipfrac           | 0.2658854    |
| eplenmean          | 400          |
| eprewmean          | 327          |
| explained_variance | 0.341        |
| fps                | 574          |
| nupdates           | 10           |
| policy_entropy     | 0.7546805    |
| policy_loss        | 0.0044131265 |
| serial_timesteps   | 4000         |
| time_elapsed       | 61.4         |
| time_remaining     | 5.73         |
| total_timesteps    | 120000       |
| true_eprew         | 184          |
| value_loss         | 96.32073     |
-------------------------------------
Current reward shaping 0.88
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.6602725982666016 seconds
Total simulation time for 400 steps: 4.186242580413818 	 Other agent action time: 0 	 95.55108007153737 steps/s
Curr learning rate 0.000898989898989899 	 Curr reward per step 0.8048666666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.66it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 170.36it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.35it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.49it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 170.65it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 168.06it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 168.39it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 170.40it/s]
------------------------------------
| approxkl           | 0.005945193 |
| clipfrac           | 0.34856242  |
| eplenmean          | 400         |
| eprewmean          | 322         |
| explained_variance | 0.247       |
| fps                | 2558        |
| nupdates           | 11          |
| policy_entropy     | 0.7538539   |
| policy_loss        | 0.009845564 |
| serial_timesteps   | 4400        |
| time_elapsed       | 66.1        |
| time_remaining     | 5.51        |
| total_timesteps    | 132000      |
| true_eprew         | 183         |
| value_loss         | 93.15494    |
------------------------------------
Current reward shaping 0.868
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8193714618682861 seconds
Total simulation time for 400 steps: 5.086386203765869 	 Other agent action time: 0 	 78.64129540612689 steps/s
Curr learning rate 0.0008888888888888889 	 Curr reward per step 0.8003283333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 128.74it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 146.59it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 147.19it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 140.11it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 156.50it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 151.87it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 145.88it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 147.47it/s]
-------------------------------------
| approxkl           | 0.0077236993 |
| clipfrac           | 0.39718756   |
| eplenmean          | 400          |
| eprewmean          | 318          |
| explained_variance | 0.283        |
| fps                | 2117         |
| nupdates           | 12           |
| policy_entropy     | 0.72550136   |
| policy_loss        | 0.012908081  |
| serial_timesteps   | 4800         |
| time_elapsed       | 71.8         |
| time_remaining     | 5.38         |
| total_timesteps    | 144000       |
| true_eprew         | 182          |
| value_loss         | 101.36849    |
-------------------------------------
Current reward shaping 0.856
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.873765230178833 seconds
Total simulation time for 400 steps: 5.028025388717651 	 Other agent action time: 0 	 79.55409312322826 steps/s
Curr learning rate 0.0008787878787878789 	 Curr reward per step 0.7561686666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 145.63it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 154.49it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 144.97it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 144.24it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 142.20it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 147.58it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 142.77it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 147.11it/s]
------------------------------------
| approxkl           | 0.008704419 |
| clipfrac           | 0.37762508  |
| eplenmean          | 400         |
| eprewmean          | 317         |
| explained_variance | 0.349       |
| fps                | 2140        |
| nupdates           | 13          |
| policy_entropy     | 0.7443024   |
| policy_loss        | 0.013462922 |
| serial_timesteps   | 5200        |
| time_elapsed       | 77.4        |
| time_remaining     | 5.26        |
| total_timesteps    | 156000      |
| true_eprew         | 182         |
| value_loss         | 99.79551    |
------------------------------------
Current reward shaping 0.844
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8682675361633301 seconds
Total simulation time for 400 steps: 5.028814315795898 	 Other agent action time: 0 	 79.54161257129117 steps/s
Curr learning rate 0.0008686868686868688 	 Curr reward per step 0.721097

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 136.48it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 150.83it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 143.47it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 140.10it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 148.08it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 160.49it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 151.02it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 148.10it/s]
-------------------------------------
| approxkl           | 0.0060755825 |
| clipfrac           | 0.33939582   |
| eplenmean          | 400          |
| eprewmean          | 306          |
| explained_variance | 0.319        |
| fps                | 2140         |
| nupdates           | 14           |
| policy_entropy     | 0.7453038    |
| policy_loss        | 0.008983821  |
| serial_timesteps   | 5600         |
| time_elapsed       | 83           |
| time_remaining     | 5.14         |
| total_timesteps    | 168000       |
| true_eprew         | 177          |
| value_loss         | 93.85771     |
-------------------------------------
Current reward shaping 0.832
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8760559558868408 seconds
Total simulation time for 400 steps: 5.081555366516113 	 Other agent action time: 0 	 78.71605663016476 steps/s
Curr learning rate 0.0008585858585858587 	 Curr reward per step 0.7495013333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 134.31it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 143.95it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 147.12it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 141.77it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 141.97it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 133.85it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 151.10it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 139.69it/s]
------------------------------------
| approxkl           | 0.005343239 |
| clipfrac           | 0.34201044  |
| eplenmean          | 400         |
| eprewmean          | 299         |
| explained_variance | 0.306       |
| fps                | 2113        |
| nupdates           | 15          |
| policy_entropy     | 0.7408797   |
| policy_loss        | 0.008410981 |
| serial_timesteps   | 6000        |
| time_elapsed       | 88.7        |
| time_remaining     | 5.03        |
| total_timesteps    | 180000      |
| true_eprew         | 174         |
| value_loss         | 91.92188    |
------------------------------------
Current reward shaping 0.8200000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8827011585235596 seconds
Total simulation time for 400 steps: 5.1267619132995605 	 Other agent action time: 0 	 78.02195747813883 steps/s
Curr learning rate 0.0008484848484848486 	 Curr reward per step 0.7054966666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 141.79it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 147.04it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 135.04it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 152.06it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 142.94it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 138.92it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 141.52it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 148.20it/s]
------------------------------------
| approxkl           | 0.005210331 |
| clipfrac           | 0.31516662  |
| eplenmean          | 400         |
| eprewmean          | 291         |
| explained_variance | 0.213       |
| fps                | 2099        |
| nupdates           | 16          |
| policy_entropy     | 0.72848105  |
| policy_loss        | 0.008181134 |
| serial_timesteps   | 6400        |
| time_elapsed       | 94.4        |
| time_remaining     | 4.92        |
| total_timesteps    | 192000      |
| true_eprew         | 170         |
| value_loss         | 85.27514    |
------------------------------------
Current reward shaping 0.808
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8865666389465332 seconds
Total simulation time for 400 steps: 5.130305051803589 	 Other agent action time: 0 	 77.96807323560178 steps/s
Curr learning rate 0.0008383838383838385 	 Curr reward per step 0.6947699999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 143.00it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 134.10it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 139.14it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 140.83it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 142.78it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 145.16it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 140.39it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 134.48it/s]
------------------------------------
| approxkl           | 0.006228703 |
| clipfrac           | 0.34128124  |
| eplenmean          | 400         |
| eprewmean          | 288         |
| explained_variance | 0.362       |
| fps                | 2093        |
| nupdates           | 17          |
| policy_entropy     | 0.76195776  |
| policy_loss        | 0.008758336 |
| serial_timesteps   | 6800        |
| time_elapsed       | 100         |
| time_remaining     | 4.81        |
| total_timesteps    | 204000      |
| true_eprew         | 171         |
| value_loss         | 81.20486    |
------------------------------------
Current reward shaping 0.796
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.883474588394165 seconds
Total simulation time for 400 steps: 5.060265064239502 	 Other agent action time: 0 	 79.04724256971612 steps/s
Curr learning rate 0.0008282828282828282 	 Curr reward per step 0.6806399999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 153.67it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 154.94it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 143.48it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 146.10it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 148.34it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 149.74it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 149.70it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 162.77it/s]
------------------------------------
| approxkl           | 0.001829068 |
| clipfrac           | 0.24168754  |
| eplenmean          | 400         |
| eprewmean          | 279         |
| explained_variance | 0.314       |
| fps                | 2134        |
| nupdates           | 18          |
| policy_entropy     | 0.7648696   |
| policy_loss        | 0.002989242 |
| serial_timesteps   | 7200        |
| time_elapsed       | 106         |
| time_remaining     | 4.7         |
| total_timesteps    | 216000      |
| true_eprew         | 166         |
| value_loss         | 89.179375   |
------------------------------------
Current reward shaping 0.784
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8697466850280762 seconds
Total simulation time for 400 steps: 5.126450061798096 	 Other agent action time: 0 	 78.02670369907017 steps/s
Curr learning rate 0.0008181818181818183 	 Curr reward per step 0.7330199999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.04it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 165.33it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 153.25it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 145.10it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 149.17it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 144.67it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 137.68it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 135.97it/s]
------------------------------------
| approxkl           | 0.004159515 |
| clipfrac           | 0.31838542  |
| eplenmean          | 400         |
| eprewmean          | 282         |
| explained_variance | 0.263       |
| fps                | 2104        |
| nupdates           | 19          |
| policy_entropy     | 0.7512658   |
| policy_loss        | 0.007119181 |
| serial_timesteps   | 7600        |
| time_elapsed       | 111         |
| time_remaining     | 4.6         |
| total_timesteps    | 228000      |
| true_eprew         | 169         |
| value_loss         | 91.28108    |
------------------------------------
Current reward shaping 0.772
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8758931159973145 seconds
Total simulation time for 400 steps: 5.029018402099609 	 Other agent action time: 0 	 79.53838463446475 steps/s
Curr learning rate 0.0008080808080808081 	 Curr reward per step 0.639356

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 148.09it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 143.99it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 140.64it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 143.27it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 139.11it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 158.82it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 152.17it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 144.45it/s]
-------------------------------------
| approxkl           | 0.0040604533 |
| clipfrac           | 0.33073956   |
| eplenmean          | 400          |
| eprewmean          | 274          |
| explained_variance | 0.555        |
| fps                | 2140         |
| nupdates           | 20           |
| policy_entropy     | 0.78418785   |
| policy_loss        | 0.0075259237 |
| serial_timesteps   | 8000         |
| time_elapsed       | 117          |
| time_remaining     | 4.49         |
| total_timesteps    | 240000       |
| true_eprew         | 165          |
| value_loss         | 78.46681     |
-------------------------------------
Current reward shaping 0.76
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8572952747344971 seconds
Total simulation time for 400 steps: 5.074660301208496 	 Other agent action time: 0 	 78.82301006527328 steps/s
Curr learning rate 0.000797979797979798 	 Curr reward per step 0.7492266666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 144.66it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 140.72it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 157.61it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 138.67it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 149.53it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 141.54it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 148.09it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 145.77it/s]
-------------------------------------
| approxkl           | 0.0041587953 |
| clipfrac           | 0.3069688    |
| eplenmean          | 400          |
| eprewmean          | 279          |
| explained_variance | 0.21         |
| fps                | 2121         |
| nupdates           | 21           |
| policy_entropy     | 0.7221287    |
| policy_loss        | 0.0066659846 |
| serial_timesteps   | 8400         |
| time_elapsed       | 123          |
| time_remaining     | 4.38         |
| total_timesteps    | 252000       |
| true_eprew         | 169          |
| value_loss         | 90.9969      |
-------------------------------------
Current reward shaping 0.748
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8751320838928223 seconds
Total simulation time for 400 steps: 5.068072557449341 	 Other agent action time: 0 	 78.92546830491945 steps/s
Curr learning rate 0.0007878787878787879 	 Curr reward per step 0.7600399999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 134.74it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 152.34it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 152.46it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 142.49it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 139.59it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 154.57it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 153.77it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 154.26it/s]
-------------------------------------
| approxkl           | 0.00307388   |
| clipfrac           | 0.28270832   |
| eplenmean          | 400          |
| eprewmean          | 286          |
| explained_variance | 0.235        |
| fps                | 2126         |
| nupdates           | 22           |
| policy_entropy     | 0.73486394   |
| policy_loss        | 0.0054713963 |
| serial_timesteps   | 8800         |
| time_elapsed       | 128          |
| time_remaining     | 4.28         |
| total_timesteps    | 264000       |
| true_eprew         | 174          |
| value_loss         | 84.1427      |
-------------------------------------
Current reward shaping 0.736
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8735635280609131 seconds
Total simulation time for 400 steps: 5.0151426792144775 	 Other agent action time: 0 	 79.75844867940069 steps/s
Curr learning rate 0.0007777777777777778 	 Curr reward per step 0.7366106666666664

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 144.35it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 144.66it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 152.03it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 150.88it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 165.02it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 151.84it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 157.52it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 154.52it/s]
------------------------------------
| approxkl           | 0.003410954 |
| clipfrac           | 0.2828125   |
| eplenmean          | 400         |
| eprewmean          | 297         |
| explained_variance | 0.428       |
| fps                | 2154        |
| nupdates           | 23          |
| policy_entropy     | 0.7406397   |
| policy_loss        | 0.006261213 |
| serial_timesteps   | 9200        |
| time_elapsed       | 134         |
| time_remaining     | 4.17        |
| total_timesteps    | 276000      |
| true_eprew         | 181         |
| value_loss         | 83.02823    |
------------------------------------
Current reward shaping 0.724
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8827991485595703 seconds
Total simulation time for 400 steps: 5.132079124450684 	 Other agent action time: 0 	 77.94112099602796 steps/s
Curr learning rate 0.0007676767676767678 	 Curr reward per step 0.7262513333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 138.25it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 148.53it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 149.07it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 137.09it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 152.16it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 148.58it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 153.20it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 156.47it/s]
------------------------------------
| approxkl           | 0.009334061 |
| clipfrac           | 0.40163547  |
| eplenmean          | 400         |
| eprewmean          | 297         |
| explained_variance | 0.382       |
| fps                | 2104        |
| nupdates           | 24          |
| policy_entropy     | 0.7528483   |
| policy_loss        | 0.01245953  |
| serial_timesteps   | 9600        |
| time_elapsed       | 140         |
| time_remaining     | 4.07        |
| total_timesteps    | 288000      |
| true_eprew         | 182         |
| value_loss         | 91.10805    |
------------------------------------
Current reward shaping 0.712
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8708291053771973 seconds
Total simulation time for 400 steps: 5.014967918395996 	 Other agent action time: 0 	 79.76122808935881 steps/s
Curr learning rate 0.0007575757575757577 	 Curr reward per step 0.7250260000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 135.38it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 143.66it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 149.76it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 144.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 139.66it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 129.97it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 138.47it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 141.03it/s]
-------------------------------------
| approxkl           | 0.0018918773 |
| clipfrac           | 0.237323     |
| eplenmean          | 400          |
| eprewmean          | 293          |
| explained_variance | 0.349        |
| fps                | 2135         |
| nupdates           | 25           |
| policy_entropy     | 0.768802     |
| policy_loss        | 0.0029616484 |
| serial_timesteps   | 10000        |
| time_elapsed       | 145          |
| time_remaining     | 3.97         |
| total_timesteps    | 300000       |
| true_eprew         | 181          |
| value_loss         | 84.84739     |
-------------------------------------
Current reward shaping 0.7
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8900797367095947 seconds
Total simulation time for 400 steps: 5.132691860198975 	 Other agent action time: 0 	 77.93181646102043 steps/s
Curr learning rate 0.0007474747474747475 	 Curr reward per step 0.7022333333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 156.13it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 154.84it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.69it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 152.72it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 157.71it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 150.98it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 152.02it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 144.92it/s]
-------------------------------------
| approxkl           | 0.0040975385 |
| clipfrac           | 0.3032396    |
| eplenmean          | 400          |
| eprewmean          | 289          |
| explained_variance | 0.447        |
| fps                | 2109         |
| nupdates           | 26           |
| policy_entropy     | 0.7696985    |
| policy_loss        | 0.0062638395 |
| serial_timesteps   | 10400        |
| time_elapsed       | 151          |
| time_remaining     | 3.87         |
| total_timesteps    | 312000       |
| true_eprew         | 180          |
| value_loss         | 83.97478     |
-------------------------------------
Current reward shaping 0.688
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8677628040313721 seconds
Total simulation time for 400 steps: 5.036531925201416 	 Other agent action time: 0 	 79.41972888100051 steps/s
Curr learning rate 0.0007373737373737374 	 Curr reward per step 0.7413506666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 141.81it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 137.12it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 144.00it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 146.18it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 139.85it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 148.55it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 146.05it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 143.60it/s]
-------------------------------------
| approxkl           | 0.003959802  |
| clipfrac           | 0.28897914   |
| eplenmean          | 400          |
| eprewmean          | 290          |
| explained_variance | 0.334        |
| fps                | 2133         |
| nupdates           | 27           |
| policy_entropy     | 0.7369256    |
| policy_loss        | 0.0063865767 |
| serial_timesteps   | 10800        |
| time_elapsed       | 157          |
| time_remaining     | 3.77         |
| total_timesteps    | 324000       |
| true_eprew         | 182          |
| value_loss         | 85.487236    |
-------------------------------------
Current reward shaping 0.6759999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8693134784698486 seconds
Total simulation time for 400 steps: 5.070391654968262 	 Other agent action time: 0 	 78.88936934646004 steps/s
Curr learning rate 0.0007272727272727272 	 Curr reward per step 0.7545026666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 123.69it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 108.37it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 125.77it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 154.06it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 144.58it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 146.63it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 153.13it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 147.21it/s]
-------------------------------------
| approxkl           | 0.0014122074 |
| clipfrac           | 0.20236453   |
| eplenmean          | 400          |
| eprewmean          | 290          |
| explained_variance | 0.247        |
| fps                | 2109         |
| nupdates           | 28           |
| policy_entropy     | 0.7604385    |
| policy_loss        | 0.0016562222 |
| serial_timesteps   | 11200        |
| time_elapsed       | 162          |
| time_remaining     | 3.67         |
| total_timesteps    | 336000       |
| true_eprew         | 182          |
| value_loss         | 87.33434     |
-------------------------------------
Current reward shaping 0.6639999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8573036193847656 seconds
Total simulation time for 400 steps: 4.998478651046753 	 Other agent action time: 0 	 80.02434899191462 steps/s
Curr learning rate 0.0007171717171717171 	 Curr reward per step 0.7251639999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 109.85it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 121.08it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 112.84it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 109.31it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 111.25it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 111.60it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 123.22it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 143.67it/s]
-------------------------------------
| approxkl           | 0.0024265274 |
| clipfrac           | 0.24780206   |
| eplenmean          | 400          |
| eprewmean          | 296          |
| explained_variance | 0.377        |
| fps                | 2100         |
| nupdates           | 29           |
| policy_entropy     | 0.75635946   |
| policy_loss        | 0.0040051895 |
| serial_timesteps   | 11600        |
| time_elapsed       | 168          |
| time_remaining     | 3.57         |
| total_timesteps    | 348000       |
| true_eprew         | 187          |
| value_loss         | 81.39385     |
-------------------------------------
Current reward shaping 0.652
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8367819786071777 seconds
Total simulation time for 400 steps: 5.073987007141113 	 Other agent action time: 0 	 78.83346950574396 steps/s
Curr learning rate 0.0007070707070707071 	 Curr reward per step 0.730872

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 141.23it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 101.79it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 103.74it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 105.27it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 105.24it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 106.15it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 104.73it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 102.86it/s]
-------------------------------------
| approxkl           | 0.0060387817 |
| clipfrac           | 0.37375      |
| eplenmean          | 400          |
| eprewmean          | 295          |
| explained_variance | 0.263        |
| fps                | 2051         |
| nupdates           | 30           |
| policy_entropy     | 0.755451     |
| policy_loss        | 0.009852468  |
| serial_timesteps   | 12000        |
| time_elapsed       | 174          |
| time_remaining     | 3.48         |
| total_timesteps    | 360000       |
| true_eprew         | 187          |
| value_loss         | 82.2621      |
-------------------------------------
Current reward shaping 0.64
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8540036678314209 seconds
Total simulation time for 400 steps: 5.120754957199097 	 Other agent action time: 0 	 78.11348196571161 steps/s
Curr learning rate 0.000696969696969697 	 Curr reward per step 0.7035733333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 141.01it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 144.84it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 141.46it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 142.46it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 131.45it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 116.40it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 117.87it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 118.19it/s]
------------------------------------
| approxkl           | 0.004314753 |
| clipfrac           | 0.32412496  |
| eplenmean          | 400         |
| eprewmean          | 289         |
| explained_variance | 0.313       |
| fps                | 2082        |
| nupdates           | 31          |
| policy_entropy     | 0.7561082   |
| policy_loss        | 0.006959279 |
| serial_timesteps   | 12400       |
| time_elapsed       | 180         |
| time_remaining     | 3.38        |
| total_timesteps    | 372000      |
| true_eprew         | 185         |
| value_loss         | 83.57042    |
------------------------------------
Current reward shaping 0.628
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8619177341461182 seconds
Total simulation time for 400 steps: 5.027268409729004 	 Other agent action time: 0 	 79.5660719499085 steps/s
Curr learning rate 0.0006868686868686869 	 Curr reward per step 0.713853

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 125.35it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 141.66it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.49it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 150.50it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 137.77it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 139.33it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 146.87it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 141.19it/s]
-------------------------------------
| approxkl           | 0.0014052783 |
| clipfrac           | 0.20545836   |
| eplenmean          | 400          |
| eprewmean          | 287          |
| explained_variance | 0.28         |
| fps                | 2133         |
| nupdates           | 32           |
| policy_entropy     | 0.7716481    |
| policy_loss        | 0.0014087486 |
| serial_timesteps   | 12800        |
| time_elapsed       | 185          |
| time_remaining     | 3.28         |
| total_timesteps    | 384000       |
| true_eprew         | 185          |
| value_loss         | 80.056725    |
-------------------------------------
Current reward shaping 0.616
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8711190223693848 seconds
Total simulation time for 400 steps: 5.053790092468262 	 Other agent action time: 0 	 79.14851877131302 steps/s
Curr learning rate 0.0006767676767676768 	 Curr reward per step 0.7130426666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 136.91it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 140.21it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 142.29it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 146.07it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 149.55it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 139.31it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 135.02it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 152.63it/s]
-------------------------------------
| approxkl           | 0.0057756943 |
| clipfrac           | 0.3199688    |
| eplenmean          | 400          |
| eprewmean          | 283          |
| explained_variance | 0.274        |
| fps                | 2122         |
| nupdates           | 33           |
| policy_entropy     | 0.7725693    |
| policy_loss        | 0.008204736  |
| serial_timesteps   | 13200        |
| time_elapsed       | 191          |
| time_remaining     | 3.18         |
| total_timesteps    | 396000       |
| true_eprew         | 184          |
| value_loss         | 82.52263     |
-------------------------------------
Current reward shaping 0.604
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.878502607345581 seconds
Total simulation time for 400 steps: 5.135074138641357 	 Other agent action time: 0 	 77.89566210738923 steps/s
Curr learning rate 0.0006666666666666668 	 Curr reward per step 0.6963596666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 142.16it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 130.00it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 150.73it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 133.17it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 139.08it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 141.08it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 141.16it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 139.12it/s]
-------------------------------------
| approxkl           | 0.0038506326 |
| clipfrac           | 0.3106146    |
| eplenmean          | 400          |
| eprewmean          | 282          |
| explained_variance | 0.244        |
| fps                | 2088         |
| nupdates           | 34           |
| policy_entropy     | 0.8139308    |
| policy_loss        | 0.0059495433 |
| serial_timesteps   | 13600        |
| time_elapsed       | 197          |
| time_remaining     | 3.08         |
| total_timesteps    | 408000       |
| true_eprew         | 184          |
| value_loss         | 82.25158     |
-------------------------------------
Current reward shaping 0.5920000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8786158561706543 seconds
Total simulation time for 400 steps: 5.061662912368774 	 Other agent action time: 0 	 79.02541258181229 steps/s
Curr learning rate 0.0006565656565656567 	 Curr reward per step 0.6882480000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 142.78it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 137.26it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 149.21it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 145.19it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 140.98it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 150.86it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 135.90it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 142.71it/s]
------------------------------------
| approxkl           | 0.005123809 |
| clipfrac           | 0.3463333   |
| eplenmean          | 400         |
| eprewmean          | 281         |
| explained_variance | 0.321       |
| fps                | 2123        |
| nupdates           | 35          |
| policy_entropy     | 0.7979592   |
| policy_loss        | 0.008381191 |
| serial_timesteps   | 14000       |
| time_elapsed       | 202         |
| time_remaining     | 2.99        |
| total_timesteps    | 420000      |
| true_eprew         | 184         |
| value_loss         | 84.16847    |
------------------------------------
Current reward shaping 0.5800000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.883267879486084 seconds
Total simulation time for 400 steps: 5.0951361656188965 	 Other agent action time: 0 	 78.50624340505976 steps/s
Curr learning rate 0.0006464646464646465 	 Curr reward per step 0.7063266666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 135.54it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 143.56it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 146.89it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 142.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 153.60it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 133.99it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 147.01it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 151.15it/s]
-------------------------------------
| approxkl           | 0.0031408444 |
| clipfrac           | 0.30118746   |
| eplenmean          | 400          |
| eprewmean          | 280          |
| explained_variance | 0.284        |
| fps                | 2111         |
| nupdates           | 36           |
| policy_entropy     | 0.7926695    |
| policy_loss        | 0.004736158  |
| serial_timesteps   | 14400        |
| time_elapsed       | 208          |
| time_remaining     | 2.89         |
| total_timesteps    | 432000       |
| true_eprew         | 184          |
| value_loss         | 80.33319     |
-------------------------------------
Current reward shaping 0.5680000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8818666934967041 seconds
Total simulation time for 400 steps: 5.183855772018433 	 Other agent action time: 0 	 77.16264062729746 steps/s
Curr learning rate 0.0006363636363636364 	 Curr reward per step 0.6699680000000002

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 131.87it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 142.34it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 146.56it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 142.52it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 148.44it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 146.59it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 146.77it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 141.52it/s]
--------------------------------------
| approxkl           | 0.0011002715  |
| clipfrac           | 0.17194793    |
| eplenmean          | 400           |
| eprewmean          | 275           |
| explained_variance | 0.274         |
| fps                | 2077          |
| nupdates           | 37            |
| policy_entropy     | 0.7819952     |
| policy_loss        | 0.00081036444 |
| serial_timesteps   | 14800         |
| time_elapsed       | 214           |
| time_remaining     | 2.79          |
| total_timesteps    | 444000        |
| true_eprew         | 183           |
| value_loss         | 81.43765      |
--------------------------------------
Current reward shaping 0.556
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.888176679611206 seconds
Total simulation time for 400 steps: 5.057803630828857 	 Other agent action time: 0 	 79.08571174291502 steps/s
Curr learning rate 0.0006262626262626263 	 Curr reward per step 0.6886046666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 135.44it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 150.30it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 152.41it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 145.84it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 138.78it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 146.43it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 138.18it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 134.96it/s]
-------------------------------------
| approxkl           | 0.0026764078 |
| clipfrac           | 0.24489585   |
| eplenmean          | 400          |
| eprewmean          | 274          |
| explained_variance | 0.298        |
| fps                | 2123         |
| nupdates           | 38           |
| policy_entropy     | 0.79121506   |
| policy_loss        | 0.0041872384 |
| serial_timesteps   | 15200        |
| time_elapsed       | 219          |
| time_remaining     | 2.69         |
| total_timesteps    | 456000       |
| true_eprew         | 184          |
| value_loss         | 76.61151     |
-------------------------------------
Current reward shaping 0.544
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8661937713623047 seconds
Total simulation time for 400 steps: 5.149629354476929 	 Other agent action time: 0 	 77.67549321821625 steps/s
Curr learning rate 0.0006161616161616161 	 Curr reward per step 0.662296

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 135.60it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 139.81it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.67it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 154.14it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 141.80it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 137.79it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 145.53it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 143.14it/s]
------------------------------------
| approxkl           | 0.006490547 |
| clipfrac           | 0.36251044  |
| eplenmean          | 400         |
| eprewmean          | 272         |
| explained_variance | 0.299       |
| fps                | 2090        |
| nupdates           | 39          |
| policy_entropy     | 0.78874135  |
| policy_loss        | 0.009846933 |
| serial_timesteps   | 15600       |
| time_elapsed       | 225         |
| time_remaining     | 2.6         |
| total_timesteps    | 468000      |
| true_eprew         | 184         |
| value_loss         | 82.52079    |
------------------------------------
Current reward shaping 0.532
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8554916381835938 seconds
Total simulation time for 400 steps: 5.064330101013184 	 Other agent action time: 0 	 78.98379292455184 steps/s
Curr learning rate 0.0006060606060606061 	 Curr reward per step 0.6790939999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 147.85it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 153.07it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 154.15it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 152.68it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 149.38it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 151.47it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 144.29it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 153.43it/s]
-------------------------------------
| approxkl           | 0.0050862664 |
| clipfrac           | 0.33083335   |
| eplenmean          | 400          |
| eprewmean          | 271          |
| explained_variance | 0.3          |
| fps                | 2132         |
| nupdates           | 40           |
| policy_entropy     | 0.7673031    |
| policy_loss        | 0.0074575422 |
| serial_timesteps   | 16000        |
| time_elapsed       | 231          |
| time_remaining     | 2.5          |
| total_timesteps    | 480000       |
| true_eprew         | 184          |
| value_loss         | 71.83516     |
-------------------------------------
Current reward shaping 0.52
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.879108190536499 seconds
Total simulation time for 400 steps: 5.137528419494629 	 Other agent action time: 0 	 77.85845008315252 steps/s
Curr learning rate 0.000595959595959596 	 Curr reward per step 0.65806

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 142.85it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 143.79it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 144.06it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 141.50it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.86it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 149.99it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 149.37it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 128.81it/s]
--------------------------------------
| approxkl           | 0.00092827703 |
| clipfrac           | 0.15812501    |
| eplenmean          | 400           |
| eprewmean          | 268           |
| explained_variance | 0.315         |
| fps                | 2093          |
| nupdates           | 41            |
| policy_entropy     | 0.7937929     |
| policy_loss        | 0.0001747691  |
| serial_timesteps   | 16400         |
| time_elapsed       | 237           |
| time_remaining     | 2.4           |
| total_timesteps    | 492000        |
| true_eprew         | 183           |
| value_loss         | 77.64058      |
--------------------------------------
Current reward shaping 0.508
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8826334476470947 seconds
Total simulation time for 400 steps: 5.090155601501465 	 Other agent action time: 0 	 78.58305940235114 steps/s
Curr learning rate 0.0005858585858585859 	 Curr reward per step 0.637358

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 139.08it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 135.22it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 145.02it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 136.18it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 141.32it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 146.22it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 148.71it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 145.46it/s]
-------------------------------------
| approxkl           | 0.0012482943 |
| clipfrac           | 0.18832293   |
| eplenmean          | 400          |
| eprewmean          | 263          |
| explained_variance | 0.221        |
| fps                | 2110         |
| nupdates           | 42           |
| policy_entropy     | 0.7951993    |
| policy_loss        | 0.0015660409 |
| serial_timesteps   | 16800        |
| time_elapsed       | 242          |
| time_remaining     | 2.31         |
| total_timesteps    | 504000       |
| true_eprew         | 181          |
| value_loss         | 87.94771     |
-------------------------------------
Current reward shaping 0.496
Current self-play randomization 0.9984
SP envs: 30/30
Other agent actions took 0.8551721572875977 seconds
Total simulation time for 400 steps: 5.13489031791687 	 Other agent action time: 0 	 77.89845064544097 steps/s
Curr learning rate 0.0005757575757575758 	 Curr reward per step 0.6538266666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 142.19it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 148.04it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 143.86it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 143.11it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 136.27it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 134.59it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 155.15it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 156.89it/s]
-------------------------------------
| approxkl           | 0.0035025063 |
| clipfrac           | 0.2757708    |
| eplenmean          | 400          |
| eprewmean          | 261          |
| explained_variance | 0.349        |
| fps                | 2098         |
| nupdates           | 43           |
| policy_entropy     | 0.7953614    |
| policy_loss        | 0.005700899  |
| serial_timesteps   | 17200        |
| time_elapsed       | 248          |
| time_remaining     | 2.21         |
| total_timesteps    | 516000       |
| true_eprew         | 181          |
| value_loss         | 76.995895    |
-------------------------------------
Current reward shaping 0.484
Current self-play randomization 0.9936
SP envs: 30/30
Other agent actions took 0.8696742057800293 seconds
Total simulation time for 400 steps: 5.071995973587036 	 Other agent action time: 0 	 78.86441591890903 steps/s
Curr learning rate 0.0005656565656565657 	 Curr reward per step 0.6924616666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 151.15it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 141.29it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.40it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 141.98it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.66it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 152.36it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 143.73it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 146.27it/s]
-------------------------------------
| approxkl           | 0.0015150558 |
| clipfrac           | 0.20536458   |
| eplenmean          | 400          |
| eprewmean          | 265          |
| explained_variance | 0.201        |
| fps                | 2124         |
| nupdates           | 44           |
| policy_entropy     | 0.77871245   |
| policy_loss        | 0.0016627522 |
| serial_timesteps   | 17600        |
| time_elapsed       | 254          |
| time_remaining     | 2.11         |
| total_timesteps    | 528000       |
| true_eprew         | 184          |
| value_loss         | 83.03522     |
-------------------------------------
Current reward shaping 0.472
Current self-play randomization 0.9888
SP envs: 30/30
Other agent actions took 0.8823397159576416 seconds
Total simulation time for 400 steps: 5.053853750228882 	 Other agent action time: 0 	 79.14752182567305 steps/s
Curr learning rate 0.0005555555555555557 	 Curr reward per step 0.6656439999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 143.60it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 159.10it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 158.43it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 140.04it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 157.20it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.52it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 153.37it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 149.28it/s]
-------------------------------------
| approxkl           | 0.001896246  |
| clipfrac           | 0.23140626   |
| eplenmean          | 400          |
| eprewmean          | 267          |
| explained_variance | 0.275        |
| fps                | 2139         |
| nupdates           | 45           |
| policy_entropy     | 0.7862848    |
| policy_loss        | 0.0028515789 |
| serial_timesteps   | 18000        |
| time_elapsed       | 259          |
| time_remaining     | 2.02         |
| total_timesteps    | 540000       |
| true_eprew         | 187          |
| value_loss         | 74.96873     |
-------------------------------------
Current reward shaping 0.45999999999999996
Current self-play randomization 0.984
SP envs: 30/30
Other agent actions took 0.8753976821899414 seconds
Total simulation time for 400 steps: 5.131054401397705 	 Other agent action time: 0 	 77.95668662001314 steps/s
Curr learning rate 0.0005454545454545455 	 Curr reward per step 0.6747716666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 142.01it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 156.77it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 157.05it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 151.56it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 154.60it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 152.56it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 141.57it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 150.41it/s]
-------------------------------------
| approxkl           | 0.0017579974 |
| clipfrac           | 0.21742705   |
| eplenmean          | 400          |
| eprewmean          | 269          |
| explained_variance | 0.237        |
| fps                | 2107         |
| nupdates           | 46           |
| policy_entropy     | 0.7766973    |
| policy_loss        | 0.002700218  |
| serial_timesteps   | 18400        |
| time_elapsed       | 265          |
| time_remaining     | 1.92         |
| total_timesteps    | 552000       |
| true_eprew         | 190          |
| value_loss         | 78.794014    |
-------------------------------------
Current reward shaping 0.44799999999999995
Current self-play randomization 0.9792
SP envs: 30/30
Other agent actions took 0.8794541358947754 seconds
Total simulation time for 400 steps: 5.072288513183594 	 Other agent action time: 0 	 78.85986748591756 steps/s
Curr learning rate 0.0005353535353535353 	 Curr reward per step 0.6555706666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 113.94it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 141.30it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 146.10it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 146.60it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 136.39it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 141.57it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 151.47it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 139.85it/s]
-------------------------------------
| approxkl           | 0.002477036  |
| clipfrac           | 0.26641667   |
| eplenmean          | 400          |
| eprewmean          | 267          |
| explained_variance | 0.22         |
| fps                | 2111         |
| nupdates           | 47           |
| policy_entropy     | 0.795468     |
| policy_loss        | 0.0035436712 |
| serial_timesteps   | 18800        |
| time_elapsed       | 271          |
| time_remaining     | 1.82         |
| total_timesteps    | 564000       |
| true_eprew         | 190          |
| value_loss         | 80.727486    |
-------------------------------------
Current reward shaping 0.43600000000000005
Current self-play randomization 0.9744
SP envs: 29/30
Other agent actions took 6.451698541641235 seconds
Total simulation time for 400 steps: 10.895607709884644 	 Other agent action time: 0 	 36.71204127853415 steps/s
Curr learning rate 0.0005252525252525252 	 Curr reward per step 0.6036909999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.66it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 153.28it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.40it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 156.91it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 147.13it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 135.10it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 152.79it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 150.51it/s]
-------------------------------------
| approxkl           | 0.0011360717 |
| clipfrac           | 0.17138538   |
| eplenmean          | 400          |
| eprewmean          | 259          |
| explained_variance | 0.32         |
| fps                | 1047         |
| nupdates           | 48           |
| policy_entropy     | 0.8171012    |
| policy_loss        | 0.000529909  |
| serial_timesteps   | 19200        |
| time_elapsed       | 282          |
| time_remaining     | 1.76         |
| total_timesteps    | 576000       |
| true_eprew         | 186          |
| value_loss         | 88.517365    |
-------------------------------------
Current reward shaping 0.42400000000000004
Current self-play randomization 0.9696
SP envs: 29/30
Other agent actions took 6.547195911407471 seconds
Total simulation time for 400 steps: 11.21875548362732 	 Other agent action time: 0 	 35.65457867263094 steps/s
Curr learning rate 0.0005151515151515151 	 Curr reward per step 0.6335926666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 140.98it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 149.57it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 143.17it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 136.87it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 143.69it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 139.48it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 143.22it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 143.13it/s]
-------------------------------------
| approxkl           | 0.0014815128 |
| clipfrac           | 0.1975833    |
| eplenmean          | 400          |
| eprewmean          | 254          |
| explained_variance | 0.163        |
| fps                | 1015         |
| nupdates           | 49           |
| policy_entropy     | 0.78941834   |
| policy_loss        | 0.0016287636 |
| serial_timesteps   | 19600        |
| time_elapsed       | 294          |
| time_remaining     | 1.7          |
| total_timesteps    | 588000       |
| true_eprew         | 183          |
| value_loss         | 90.298676    |
-------------------------------------
Current reward shaping 0.41200000000000003
Current self-play randomization 0.9648
SP envs: 30/30
Other agent actions took 0.9085097312927246 seconds
Total simulation time for 400 steps: 5.123828887939453 	 Other agent action time: 0 	 78.06661946528428 steps/s
Curr learning rate 0.000505050505050505 	 Curr reward per step 0.6507896666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 132.91it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 150.98it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 144.18it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 155.54it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.40it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 152.48it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 156.04it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 148.39it/s]
--------------------------------------
| approxkl           | 0.00060934474 |
| clipfrac           | 0.11295833    |
| eplenmean          | 400           |
| eprewmean          | 253           |
| explained_variance | 0.222         |
| fps                | 2109          |
| nupdates           | 50            |
| policy_entropy     | 0.7849929     |
| policy_loss        | -0.000674748  |
| serial_timesteps   | 20000         |
| time_elapsed       | 300           |
| time_remaining     | 1.6           |
| total_timesteps    | 600000        |
| true_eprew         | 184           |
| value_loss         | 77.02846      |
--------------------------------------
Current reward shaping 0.4
Current self-play randomization 0.96
../../thesis_data/dr_ppo/ppo_bc_train_simple/
PPO agent on index 0:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 2
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 3
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 4
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 5
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 6
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0←1  O 
X       X 
X D X S X 


Timestep: 7
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O →0←1  O 
X       X 
X D X S X 


Timestep: 8
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O ←0←1  O 
X       X 
X D X S X 


Timestep: 9
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O →0←1  O 
X       X 
X D X S X 


Timestep: 10
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  →1O 
X       X 
X D X S X 


Timestep: 11
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  →1O 
X       X 
X D X S X 


Timestep: 12
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  →1O 
X       X 
X D X S X 


Timestep: 13
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ←0  →1O 
X       X 
X D X S X 


Timestep: 14
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O ←o  →oO 
X       X 
X D X S X 


Timestep: 15
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O →o  ←oO 
X       X 
X D X S X 


Timestep: 16
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →o←oO 
X       X 
X D X S X 


Timestep: 17
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o←oO 
X       X 
X D X S X 


Timestep: 18
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑0←oO 
X       X 
X D X S X 


Timestep: 19
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑0←oO 
X       X 
X D X S X 


Timestep: 20
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←0←o  O 
X       X 
X D X S X 


Timestep: 21
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X ↓0    X 
X D X S X 


Timestep: 22
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 23
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 24
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 25
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑1  O 
X ↓d    X 
X D X S X 


Timestep: 26
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑1  O 
X ↓d    X 
X D X S X 


Timestep: 27
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑1  O 
X ↓d    X 
X D X S X 


Timestep: 28
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 29
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 30
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 31
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 32
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     ↑1O 
X ↓d    X 
X D X S X 


Timestep: 33
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     ↑1O 
X ↓d    X 
X D X S X 


Timestep: 34
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 35
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 36
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 37
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 38
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←1  O 
X ↓d    X 
X D X S X 


Timestep: 39
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←1  O 
X ↓d    X 
X D X S X 


Timestep: 40
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←1  O 
X   →d  X 
X D X S X 


Timestep: 41
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X   →d  X 
X D X S X 


Timestep: 42
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X   →d  X 
X D X S X 


Timestep: 43
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X   →d  X 
X D X S X 


Timestep: 44
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
X   →d  X 
X D X S X 


Timestep: 45
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     ↑oO 
X ←d    X 
X D X S X 


Timestep: 46
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑d←o  O 
X       X 
X D X S X 


Timestep: 47
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑d←o  O 
X       X 
X D X S X 


Timestep: 48
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 49
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 50
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø1X X 
O   ↑1  O 
X ↓d    X 
X D X S X 


Timestep: 51
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø2X X 
O   ↑1  O 
X   →d  X 
X D X S X 


Timestep: 52
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X ø3X X 
O     →1O 
X   ↓d  X 
X D X S X 


Timestep: 53
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø4X X 
O   ↑d→1O 
X       X 
X D X S X 


Timestep: 54
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø5X X 
O   ↑d→1O 
X       X 
X D X S X 


Timestep: 55
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø6X X 
O   ↑d→1O 
X       X 
X D X S X 


Timestep: 56
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø7X X 
O   ↑d→oO 
X       X 
X D X S X 


Timestep: 57
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø8X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 58
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø9X X 
O     ←oO 
X   ↓d  X 
X D X S X 


Timestep: 59
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø10X X 
O     ←oO 
X ←d    X 
X D X S X 


Timestep: 60
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø11X X 
O     ←oO 
X   →d  X 
X D X S X 


Timestep: 61
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø12X X 
O     ←oO 
X   ↑d  X 
X D X S X 


Timestep: 62
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø13X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 63
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø14X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 64
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø15X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 65
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø16X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 66
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø17XoX 
O   ↑d↑1O 
X       X 
X D X S X 


Timestep: 67
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø18XoX 
O   ↑d←1O 
X       X 
X D X S X 


Timestep: 68
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø19XoX 
O   ↑d←1O 
X       X 
X D X S X 


Timestep: 69
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XoX 
O   ↑d←1O 
X       X 
X D X S X 


Timestep: 70
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 5 
X X P XoX 
O   ↑s←1O 
X       X 
X D X S X 


Timestep: 71
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s←1O 
X       X 
X D X S X 


Timestep: 72
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s↑1O 
X       X 
X D X S X 


Timestep: 73
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑1O 
X       X 
X D X S X 


Timestep: 74
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑1O 
X       X 
X D X S X 


Timestep: 75
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑1O 
X       X 
X D X S X 


Timestep: 76
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 77
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s←oO 
X       X 
X D X S X 


Timestep: 78
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s←oO 
X       X 
X D X S X 


Timestep: 79
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s←oO 
X       X 
X D X S X 


Timestep: 80
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ←s  ←oO 
X       X 
X D X S X 


Timestep: 81
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O →s  ←oO 
X       X 
X D X S X 


Timestep: 82
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 83
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 84
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 85
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 86
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑1O 
X       X 
X D X S X 


Timestep: 87
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←1O 
X       X 
X D X S X 


Timestep: 88
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s←1O 
X       X 
X D X S X 


Timestep: 89
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O ←s  ←1O 
X       X 
X D X S X 


Timestep: 90
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←1O 
X       X 
X D X S X 


Timestep: 91
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s↑1O 
X       X 
X D X S X 


Timestep: 92
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑1O 
X       X 
X D X S X 


Timestep: 93
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑1O 
X       X 
X D X S X 


Timestep: 94
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑1O 
X       X 
X D X S X 


Timestep: 95
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 96
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s←oO 
X       X 
X D X S X 


Timestep: 97
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     ←oO 
X   ↓s  X 
X D X S X 


Timestep: 98
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s←oO 
X       X 
X D X S X 


Timestep: 99
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s←oO 
X       X 
X D X S X 


tot rew 60 tot rew shaped 51
PPO agent on index 1:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←1  O 
X ↑0    X 
X D X S X 


Timestep: 2
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X ←0    X 
X D X S X 


Timestep: 3
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X ←0    X 
X D X S X 


Timestep: 4
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X ←0    X 
X D X S X 


Timestep: 5
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X ←0    X 
X D X S X 


Timestep: 6
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑1  O 
X ↓0    X 
X D X S X 


Timestep: 7
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X ↓0    X 
X D X S X 


Timestep: 8
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ↑0  →oO 
X       X 
X D X S X 


Timestep: 9
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ↑0←o  O 
X       X 
X D X S X 


Timestep: 10
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ↑0↑o  O 
X       X 
X D X S X 


Timestep: 11
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O ↑0↑1  O 
X       X 
X D X S X 


Timestep: 12
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0  →1O 
X       X 
X D X S X 


Timestep: 13
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   →0→oO 
X       X 
X D X S X 


Timestep: 14
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←0←o  O 
X       X 
X D X S X 


Timestep: 15
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o  O 
X ↓0    X 
X D X S X 


Timestep: 16
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø1X X 
O   ↑1  O 
X ↓0    X 
X D X S X 


Timestep: 17
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 3 
X X ø2X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 18
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø3X X 
O     →oO 
X ↓d    X 
X D X S X 


Timestep: 19
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X ø4X X 
O       O 
X   →d↓oX 
X D X S X 


Timestep: 20
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø5X X 
O       O 
X   →d→oX 
X D X S X 


Timestep: 21
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø6X X 
O       O 
X   →d↓oX 
X D X S X 


Timestep: 22
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø7X X 
O       O 
X   →d↓oX 
X D X S X 


Timestep: 23
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø8X X 
O       O 
X   →d→oX 
X D X S X 


Timestep: 24
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø9X X 
O   ↑d  O 
X     →oX 
X D X S X 


Timestep: 25
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø10X X 
O   ↑d  O 
X   ←o  X 
X D X S X 


Timestep: 26
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø11X X 
O   ↑d  O 
X   ←o  X 
X D X S X 


Timestep: 27
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø12X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 28
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø13X X 
O     →dO 
X   ↑o  X 
X D X S X 


Timestep: 29
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø14X X 
O     →dO 
X   ↓o  X 
X D X S X 


Timestep: 30
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø15X X 
O     →dO 
X   ↓o  X 
X D X S X 


Timestep: 31
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø16X X 
O     →dO 
X     →oX 
X D X S X 


Timestep: 32
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø17X X 
O     ↑dO 
X   ←o  X 
X D X S X 


Timestep: 33
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø18X X 
O     ↑dO 
X     →oX 
X D X S X 


Timestep: 34
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø19X X 
O     ↑dO 
X     →oX 
X D X S X 


Timestep: 35
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
X     →oX 
X D X S X 


Timestep: 36
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑0O 
X   ←o  X 
X D X S X 


Timestep: 37
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →0O 
X   ↓o  X 
X D X S X 


Timestep: 38
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →0O 
X   ↓1  X 
X D XoS X 


Timestep: 39
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →0O 
X   ↓1  X 
X D XoS X 


Timestep: 40
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →0O 
X   ↓o  X 
X D X S X 


Timestep: 41
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑0O 
X   ↓1  X 
X D XoS X 


Timestep: 42
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑0O 
X   ↓o  X 
X D X S X 


Timestep: 43
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑0O 
X ←o    X 
X D X S X 


Timestep: 44
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑0O 
Xo←1    X 
X D X S X 


Timestep: 45
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo  →1  X 
X D X S X 


Timestep: 46
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo←1    X 
X D X S X 


Timestep: 47
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo  →1  X 
X D X S X 


Timestep: 48
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo←1    X 
X D X S X 


Timestep: 49
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo  →1  X 
X D X S X 


Timestep: 50
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo←1    X 
X D X S X 


Timestep: 51
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo  →1  X 
X D X S X 


Timestep: 52
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo←1    X 
X D X S X 


Timestep: 53
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo↓1    X 
X D X S X 


Timestep: 54
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo↓1    X 
X D X S X 


Timestep: 55
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo↓1    X 
X D X S X 


Timestep: 56
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo  →1  X 
X D X S X 


Timestep: 57
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo←1    X 
X D X S X 


Timestep: 58
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo  →1  X 
X D X S X 


Timestep: 59
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑1↑dO 
Xo      X 
X D X S X 


Timestep: 60
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo  ↓1  X 
X D X S X 


Timestep: 61
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo←1    X 
X D X S X 


Timestep: 62
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo  →1  X 
X D X S X 


Timestep: 63
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo←1    X 
X D X S X 


Timestep: 64
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo  →1  X 
X D X S X 


Timestep: 65
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo←1    X 
X D X S X 


Timestep: 66
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo  →1  X 
X D X S X 


Timestep: 67
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo←1    X 
X D X S X 


Timestep: 68
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo↓1    X 
X D X S X 


Timestep: 69
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo←1    X 
X D X S X 


Timestep: 70
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo↓1    X 
X D X S X 


Timestep: 71
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ←d  O 
Xo←1    X 
X D X S X 


Timestep: 72
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ←d  O 
Xo←1    X 
X D X S X 


Timestep: 73
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ←d  O 
Xo←1    X 
X D X S X 


Timestep: 74
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ←d  O 
Xo←1    X 
X D X S X 


Timestep: 75
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d  O 
Xo←1    X 
X D X S X 


Timestep: 76
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d  O 
Xo←1    X 
X D X S X 


Timestep: 77
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d  O 
Xo←1    X 
X D X S X 


Timestep: 78
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑1↑d  O 
Xo      X 
X D X S X 


Timestep: 79
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑1  →dO 
Xo      X 
X D X S X 


Timestep: 80
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo↓1    X 
X D X S X 


Timestep: 81
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo  →1  X 
X D X S X 


Timestep: 82
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo←1    X 
X D X S X 


Timestep: 83
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo  →1  X 
X D X S X 


Timestep: 84
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo←1    X 
X D X S X 


Timestep: 85
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo  →1  X 
X D X S X 


Timestep: 86
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo←1    X 
X D X S X 


Timestep: 87
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo↓1    X 
X D X S X 


Timestep: 88
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo↓1    X 
X D X S X 


Timestep: 89
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo↓1    X 
X D X S X 


Timestep: 90
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo↓1    X 
X D X S X 


Timestep: 91
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo  →1  X 
X D X S X 


Timestep: 92
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo←1    X 
X D X S X 


Timestep: 93
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo←1    X 
X D X S X 


Timestep: 94
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo  →1  X 
X D X S X 


Timestep: 95
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑dO 
Xo←1    X 
X D X S X 


Timestep: 96
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo↓1    X 
X D X S X 


Timestep: 97
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo  →1  X 
X D X S X 


Timestep: 98
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo←1    X 
X D X S X 


Timestep: 99
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
Xo←1    X 
X D X S X 


tot rew 60 tot rew shaped 54
../../thesis_data/dr_ppo/ppo_bc_train_simple/
SP envs: 29/30
Other agent actions took 6.585472583770752 seconds
Total simulation time for 400 steps: 10.997314691543579 	 Other agent action time: 0 	 36.37251558397081 steps/s
Curr learning rate 0.000494949494949495 	 Curr reward per step 0.6116333333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 113.35it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 106.98it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 102.97it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 106.54it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 105.91it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 111.36it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 134.68it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 139.10it/s]
-------------------------------------
| approxkl           | 0.0010380617 |
| clipfrac           | 0.16128124   |
| eplenmean          | 400          |
| eprewmean          | 252          |
| explained_variance | 0.293        |
| fps                | 1022         |
| nupdates           | 51           |
| policy_entropy     | 0.7851005    |
| policy_loss        | 0.00070562   |
| serial_timesteps   | 20400        |
| time_elapsed       | 314          |
| time_remaining     | 1.54         |
| total_timesteps    | 612000       |
| true_eprew         | 184          |
| value_loss         | 87.465706    |
-------------------------------------
Current reward shaping 0.388
Current self-play randomization 0.9552
SP envs: 29/30
Other agent actions took 6.541017293930054 seconds
Total simulation time for 400 steps: 10.914748191833496 	 Other agent action time: 0 	 36.64766176642382 steps/s
Curr learning rate 0.0004848484848484849 	 Curr reward per step 0.6071930000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 140.16it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 143.41it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 144.28it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 146.46it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 143.98it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 137.79it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 140.43it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 145.99it/s]
-------------------------------------
| approxkl           | 0.0027610383 |
| clipfrac           | 0.25732297   |
| eplenmean          | 400          |
| eprewmean          | 248          |
| explained_variance | 0.281        |
| fps                | 1042         |
| nupdates           | 52           |
| policy_entropy     | 0.77261865   |
| policy_loss        | 0.0042870194 |
| serial_timesteps   | 20800        |
| time_elapsed       | 326          |
| time_remaining     | 1.46         |
| total_timesteps    | 624000       |
| true_eprew         | 183          |
| value_loss         | 85.60366     |
-------------------------------------
Current reward shaping 0.376
Current self-play randomization 0.9504
SP envs: 27/30
Other agent actions took 6.593669891357422 seconds
Total simulation time for 400 steps: 11.09820032119751 	 Other agent action time: 0 	 36.041879622230454 steps/s
Curr learning rate 0.0004747474747474748 	 Curr reward per step 0.582504

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 133.48it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 147.95it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.47it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 135.97it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 148.06it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 142.63it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 145.52it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 150.64it/s]
------------------------------------
| approxkl           | 0.004410538 |
| clipfrac           | 0.32529163  |
| eplenmean          | 400         |
| eprewmean          | 243         |
| explained_variance | 0.248       |
| fps                | 1026        |
| nupdates           | 53          |
| policy_entropy     | 0.77599597  |
| policy_loss        | 0.008598484 |
| serial_timesteps   | 21200       |
| time_elapsed       | 338         |
| time_remaining     | 1.38        |
| total_timesteps    | 636000      |
| true_eprew         | 182         |
| value_loss         | 96.3271     |
------------------------------------
Current reward shaping 0.364
Current self-play randomization 0.9456
SP envs: 29/30
Other agent actions took 6.5331645011901855 seconds
Total simulation time for 400 steps: 10.929579019546509 	 Other agent action time: 0 	 36.59793293818895 steps/s
Curr learning rate 0.0004646464646464647 	 Curr reward per step 0.6025716666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 129.63it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 135.69it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 136.03it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 149.18it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 131.21it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 150.20it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 144.63it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 143.70it/s]
-------------------------------------
| approxkl           | 0.0050663156 |
| clipfrac           | 0.31166664   |
| eplenmean          | 400          |
| eprewmean          | 240          |
| explained_variance | 0.238        |
| fps                | 1040         |
| nupdates           | 54           |
| policy_entropy     | 0.77685064   |
| policy_loss        | 0.008032922  |
| serial_timesteps   | 21600        |
| time_elapsed       | 349          |
| time_remaining     | 1.29         |
| total_timesteps    | 648000       |
| true_eprew         | 181          |
| value_loss         | 86.14262     |
-------------------------------------
Current reward shaping 0.352
Current self-play randomization 0.9408
SP envs: 25/30
Other agent actions took 6.499723434448242 seconds
Total simulation time for 400 steps: 10.740437030792236 	 Other agent action time: 0 	 37.242432393879525 steps/s
Curr learning rate 0.00045454545454545455 	 Curr reward per step 0.53216

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 143.19it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 165.59it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 157.33it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 162.14it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.19it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.89it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 165.87it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.91it/s]
-------------------------------------
| approxkl           | 0.0032020286 |
| clipfrac           | 0.27799997   |
| eplenmean          | 400          |
| eprewmean          | 231          |
| explained_variance | 0.186        |
| fps                | 1065         |
| nupdates           | 55           |
| policy_entropy     | 0.79094297   |
| policy_loss        | 0.00644984   |
| serial_timesteps   | 22000        |
| time_elapsed       | 360          |
| time_remaining     | 1.2          |
| total_timesteps    | 660000       |
| true_eprew         | 174          |
| value_loss         | 110.24333    |
-------------------------------------
Current reward shaping 0.33999999999999997
Current self-play randomization 0.9359999999999999
SP envs: 27/30
Other agent actions took 6.518907308578491 seconds
Total simulation time for 400 steps: 10.855908632278442 	 Other agent action time: 0 	 36.84629389848207 steps/s
Curr learning rate 0.0004444444444444444 	 Curr reward per step 0.55369

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 120.41it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 115.60it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 126.11it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 120.11it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 136.85it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 144.11it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 151.77it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 151.69it/s]
------------------------------------
| approxkl           | 0.0105716   |
| clipfrac           | 0.38659376  |
| eplenmean          | 400         |
| eprewmean          | 225         |
| explained_variance | 0.21        |
| fps                | 1044        |
| nupdates           | 56          |
| policy_entropy     | 0.7635077   |
| policy_loss        | 0.016945882 |
| serial_timesteps   | 22400       |
| time_elapsed       | 372         |
| time_remaining     | 1.11        |
| total_timesteps    | 672000      |
| true_eprew         | 171         |
| value_loss         | 102.82235   |
------------------------------------
Current reward shaping 0.32799999999999996
Current self-play randomization 0.9312
SP envs: 27/30
Other agent actions took 6.412250280380249 seconds
Total simulation time for 400 steps: 10.831072807312012 	 Other agent action time: 0 	 36.93078304579041 steps/s
Curr learning rate 0.00043434343434343433 	 Curr reward per step 0.5239273333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 136.69it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 132.66it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 137.08it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 146.34it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 133.91it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 133.12it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 137.23it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 125.66it/s]
------------------------------------
| approxkl           | 0.005793226 |
| clipfrac           | 0.35758322  |
| eplenmean          | 400         |
| eprewmean          | 215         |
| explained_variance | 0.214       |
| fps                | 1047        |
| nupdates           | 57          |
| policy_entropy     | 0.76060593  |
| policy_loss        | 0.011348894 |
| serial_timesteps   | 22800       |
| time_elapsed       | 383         |
| time_remaining     | 1.01        |
| total_timesteps    | 684000      |
| true_eprew         | 165         |
| value_loss         | 103.61998   |
------------------------------------
Current reward shaping 0.31599999999999995
Current self-play randomization 0.9264
SP envs: 28/30
Other agent actions took 6.4780333042144775 seconds
Total simulation time for 400 steps: 10.954843282699585 	 Other agent action time: 0 	 36.51353010514529 steps/s
Curr learning rate 0.00042424242424242425 	 Curr reward per step 0.5564756666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 143.58it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 140.27it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 140.69it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 141.36it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 141.77it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 147.47it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 152.04it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.89it/s]
-------------------------------------
| approxkl           | 0.0021021354 |
| clipfrac           | 0.21226041   |
| eplenmean          | 400          |
| eprewmean          | 219          |
| explained_variance | 0.275        |
| fps                | 1040         |
| nupdates           | 58           |
| policy_entropy     | 0.7247714    |
| policy_loss        | 0.0029106226 |
| serial_timesteps   | 23200        |
| time_elapsed       | 395          |
| time_remaining     | 0.908        |
| total_timesteps    | 696000       |
| true_eprew         | 169          |
| value_loss         | 92.32327     |
-------------------------------------
Current reward shaping 0.30400000000000005
Current self-play randomization 0.9216
SP envs: 29/30
Other agent actions took 6.414766788482666 seconds
Total simulation time for 400 steps: 10.776694059371948 	 Other agent action time: 0 	 37.11713423395741 steps/s
Curr learning rate 0.0004141414141414141 	 Curr reward per step 0.6010586666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 135.96it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 149.43it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 147.85it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 140.03it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 153.98it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 138.55it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 147.47it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 140.02it/s]
-------------------------------------
| approxkl           | 0.00119222   |
| clipfrac           | 0.16008332   |
| eplenmean          | 400          |
| eprewmean          | 222          |
| explained_variance | 0.229        |
| fps                | 1055         |
| nupdates           | 59           |
| policy_entropy     | 0.7255195    |
| policy_loss        | 0.0012870425 |
| serial_timesteps   | 23600        |
| time_elapsed       | 406          |
| time_remaining     | 0.803        |
| total_timesteps    | 708000       |
| true_eprew         | 173          |
| value_loss         | 81.877       |
-------------------------------------
Current reward shaping 0.29200000000000004
Current self-play randomization 0.9168000000000001
SP envs: 29/30
Other agent actions took 6.474672794342041 seconds
Total simulation time for 400 steps: 11.024625062942505 	 Other agent action time: 0 	 36.28241302686432 steps/s
Curr learning rate 0.00040404040404040404 	 Curr reward per step 0.5956570000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 142.81it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 139.95it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 143.67it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 143.34it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 141.89it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 145.54it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 150.18it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 135.44it/s]
-------------------------------------
| approxkl           | 0.001977233  |
| clipfrac           | 0.22196874   |
| eplenmean          | 400          |
| eprewmean          | 233          |
| explained_variance | 0.288        |
| fps                | 1032         |
| nupdates           | 60           |
| policy_entropy     | 0.71530855   |
| policy_loss        | 0.0029316288 |
| serial_timesteps   | 24000        |
| time_elapsed       | 418          |
| time_remaining     | 0.696        |
| total_timesteps    | 720000       |
| true_eprew         | 184          |
| value_loss         | 84.16722     |
-------------------------------------
Current reward shaping 0.28
Current self-play randomization 0.912
SP envs: 24/30
Other agent actions took 6.5539467334747314 seconds
Total simulation time for 400 steps: 11.19202709197998 	 Other agent action time: 0 	 35.73972763938655 steps/s
Curr learning rate 0.00039393939393939396 	 Curr reward per step 0.50614

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 135.06it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 153.68it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 146.70it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 144.78it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 143.97it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 149.38it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 155.16it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 147.10it/s]
-------------------------------------
| approxkl           | 0.0020927964 |
| clipfrac           | 0.24964583   |
| eplenmean          | 400          |
| eprewmean          | 230          |
| explained_variance | 0.209        |
| fps                | 1019         |
| nupdates           | 61           |
| policy_entropy     | 0.76019734   |
| policy_loss        | 0.0046843537 |
| serial_timesteps   | 24400        |
| time_elapsed       | 430          |
| time_remaining     | 0.587        |
| total_timesteps    | 732000       |
| true_eprew         | 182          |
| value_loss         | 113.26292    |
-------------------------------------
Current reward shaping 0.268
Current self-play randomization 0.9072
SP envs: 28/30
Other agent actions took 6.543733835220337 seconds
Total simulation time for 400 steps: 10.988038063049316 	 Other agent action time: 0 	 36.40322300530829 steps/s
Curr learning rate 0.0003838383838383839 	 Curr reward per step 0.5569543333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 147.02it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 138.54it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 142.47it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 148.66it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 142.36it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 145.30it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 144.42it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 150.37it/s]
-------------------------------------
| approxkl           | 0.0013757495 |
| clipfrac           | 0.18081248   |
| eplenmean          | 400          |
| eprewmean          | 222          |
| explained_variance | 0.286        |
| fps                | 1037         |
| nupdates           | 62           |
| policy_entropy     | 0.7139677    |
| policy_loss        | 0.0020498154 |
| serial_timesteps   | 24800        |
| time_elapsed       | 441          |
| time_remaining     | 0.474        |
| total_timesteps    | 744000       |
| true_eprew         | 177          |
| value_loss         | 92.04259     |
-------------------------------------
Current reward shaping 0.256
Current self-play randomization 0.9024
SP envs: 24/30
Other agent actions took 6.53928542137146 seconds
Total simulation time for 400 steps: 10.961904048919678 	 Other agent action time: 0 	 36.490011061483514 steps/s
Curr learning rate 0.0003737373737373737 	 Curr reward per step 0.49940266666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 141.40it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 140.31it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 142.02it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 120.69it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 117.91it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 113.54it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 105.82it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 111.39it/s]
-------------------------------------
| approxkl           | 0.0021247885 |
| clipfrac           | 0.22322913   |
| eplenmean          | 400          |
| eprewmean          | 213          |
| explained_variance | 0.255        |
| fps                | 1030         |
| nupdates           | 63           |
| policy_entropy     | 0.7192538    |
| policy_loss        | 0.003207225  |
| serial_timesteps   | 25200        |
| time_elapsed       | 453          |
| time_remaining     | 0.359        |
| total_timesteps    | 756000       |
| true_eprew         | 171          |
| value_loss         | 112.83507    |
-------------------------------------
Current reward shaping 0.244
Current self-play randomization 0.8976
SP envs: 28/30
Other agent actions took 6.487286329269409 seconds
Total simulation time for 400 steps: 10.873346090316772 	 Other agent action time: 0 	 36.78720392761331 steps/s
Curr learning rate 0.0003636363636363636 	 Curr reward per step 0.5558316666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 107.88it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 121.10it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 134.36it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 154.36it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 146.68it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 146.99it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 133.38it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 150.67it/s]
-------------------------------------
| approxkl           | 0.0020969328 |
| clipfrac           | 0.20281248   |
| eplenmean          | 400          |
| eprewmean          | 216          |
| explained_variance | 0.383        |
| fps                | 1043         |
| nupdates           | 64           |
| policy_entropy     | 0.700359     |
| policy_loss        | 0.0032378074 |
| serial_timesteps   | 25600        |
| time_elapsed       | 464          |
| time_remaining     | 0.242        |
| total_timesteps    | 768000       |
| true_eprew         | 176          |
| value_loss         | 82.73123     |
-------------------------------------
Current reward shaping 0.23199999999999998
Current self-play randomization 0.8928
SP envs: 23/30
Other agent actions took 6.4918293952941895 seconds
Total simulation time for 400 steps: 10.926830768585205 	 Other agent action time: 0 	 36.60713783085263 steps/s
Curr learning rate 0.00035353535353535354 	 Curr reward per step 0.4674993333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 152.43it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 142.33it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 143.83it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 149.47it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 149.62it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 143.76it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 152.02it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 145.99it/s]
-------------------------------------
| approxkl           | 0.0022206563 |
| clipfrac           | 0.23564582   |
| eplenmean          | 400          |
| eprewmean          | 207          |
| explained_variance | 0.35         |
| fps                | 1042         |
| nupdates           | 65           |
| policy_entropy     | 0.72037405   |
| policy_loss        | 0.0036836858 |
| serial_timesteps   | 26000        |
| time_elapsed       | 476          |
| time_remaining     | 0.122        |
| total_timesteps    | 780000       |
| true_eprew         | 170          |
| value_loss         | 106.850914   |
-------------------------------------
Current reward shaping 0.21999999999999997
Current self-play randomization 0.888
SP envs: 27/30
Other agent actions took 6.5353968143463135 seconds
Total simulation time for 400 steps: 10.964111566543579 	 Other agent action time: 0 	 36.48266415133711 steps/s
Curr learning rate 0.0003434343434343434 	 Curr reward per step 0.5475616666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 139.10it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 154.41it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.89it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 153.77it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 153.22it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 147.02it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 146.21it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 144.53it/s]
------------------------------------
| approxkl           | 0.00197777  |
| clipfrac           | 0.20396876  |
| eplenmean          | 400         |
| eprewmean          | 212         |
| explained_variance | 0.298       |
| fps                | 1040        |
| nupdates           | 66          |
| policy_entropy     | 0.6963808   |
| policy_loss        | 0.003980715 |
| serial_timesteps   | 26400       |
| time_elapsed       | 487         |
| time_remaining     | 0           |
| total_timesteps    | 792000      |
| true_eprew         | 176         |
| value_loss         | 88.88351    |
------------------------------------
Current reward shaping 0.20799999999999996
Current self-play randomization 0.8832
LOADING BC MODEL FROM: seed1/worker4
Loading a model without an environment, this model cannot be trained until it has a valid environment.
Loaded MediumLevelPlanner from /home/ubuntu/human_aware_rl/overcooked_ai/overcooked_ai_py/data/planners/simple_am.pkl
TOT NUM UPDATES 66
SP envs: 27/30
Other agent actions took 6.4951019287109375 seconds
Total simulation time for 400 steps: 10.915663480758667 	 Other agent action time: 0 	 36.64458882459053 steps/s
Curr learning rate 0.001 	 Curr reward per step 0.5443439999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 140.21it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 148.19it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 149.07it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 113.75it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 111.02it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 109.29it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 107.58it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 103.53it/s]
------------------------------------
| approxkl           | 0.005669596 |
| clipfrac           | 0.32361466  |
| eplenmean          | 400         |
| eprewmean          | 218         |
| explained_variance | 0.268       |
| fps                | 1033        |
| nupdates           | 1           |
| policy_entropy     | 0.66562223  |
| policy_loss        | 0.008900853 |
| serial_timesteps   | 400         |
| time_elapsed       | 11.6        |
| time_remaining     | 12.6        |
| total_timesteps    | 12000       |
| true_eprew         | 184         |
| value_loss         | 91.7616     |
------------------------------------
Current reward shaping 0.988
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8573179244995117 seconds
Total simulation time for 400 steps: 5.009903907775879 	 Other agent action time: 0 	 79.84185073473354 steps/s
Curr learning rate 0.00098989898989899 	 Curr reward per step 0.9595800000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 142.96it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 140.66it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 142.26it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 140.72it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.32it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 170.78it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 115.32it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 116.74it/s]
------------------------------------
| approxkl           | 0.007084281 |
| clipfrac           | 0.3202395   |
| eplenmean          | 400         |
| eprewmean          | 301         |
| explained_variance | 0.19        |
| fps                | 2132        |
| nupdates           | 2           |
| policy_entropy     | 0.6646339   |
| policy_loss        | 0.010211242 |
| serial_timesteps   | 800         |
| time_elapsed       | 17.2        |
| time_remaining     | 9.2         |
| total_timesteps    | 24000       |
| true_eprew         | 195         |
| value_loss         | 126.6216    |
------------------------------------
Current reward shaping 0.976
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8675034046173096 seconds
Total simulation time for 400 steps: 5.105329513549805 	 Other agent action time: 0 	 78.34949711637213 steps/s
Curr learning rate 0.0009797979797979799 	 Curr reward per step 0.909192

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 134.40it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 140.83it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 134.48it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 153.88it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.27it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 151.36it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 151.51it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 145.08it/s]
------------------------------------
| approxkl           | 0.006225019 |
| clipfrac           | 0.3146042   |
| eplenmean          | 400         |
| eprewmean          | 322         |
| explained_variance | 0.199       |
| fps                | 2110        |
| nupdates           | 3           |
| policy_entropy     | 0.68889844  |
| policy_loss        | 0.008196685 |
| serial_timesteps   | 1200        |
| time_elapsed       | 22.9        |
| time_remaining     | 8.03        |
| total_timesteps    | 36000       |
| true_eprew         | 195         |
| value_loss         | 114.86188   |
------------------------------------
Current reward shaping 0.964
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8657877445220947 seconds
Total simulation time for 400 steps: 5.003663539886475 	 Other agent action time: 0 	 79.94142627924886 steps/s
Curr learning rate 0.0009696969696969698 	 Curr reward per step 0.8351973333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 131.11it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 142.24it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 153.63it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 138.05it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 138.60it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 143.59it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 145.42it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 145.75it/s]
-------------------------------------
| approxkl           | 0.0059878947 |
| clipfrac           | 0.31976038   |
| eplenmean          | 400          |
| eprewmean          | 347          |
| explained_variance | 0.438        |
| fps                | 2143         |
| nupdates           | 4            |
| policy_entropy     | 0.68188274   |
| policy_loss        | 0.009473498  |
| serial_timesteps   | 1600         |
| time_elapsed       | 28.5         |
| time_remaining     | 7.37         |
| total_timesteps    | 48000        |
| true_eprew         | 194          |
| value_loss         | 107.3187     |
-------------------------------------
Current reward shaping 0.952
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8594682216644287 seconds
Total simulation time for 400 steps: 5.084979772567749 	 Other agent action time: 0 	 78.66304644079499 steps/s
Curr learning rate 0.0009595959595959597 	 Curr reward per step 0.8787266666666664

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 129.01it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 147.21it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 146.00it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 137.30it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 142.43it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 147.32it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 140.84it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 138.33it/s]
-------------------------------------
| approxkl           | 0.0077090114 |
| clipfrac           | 0.3447395    |
| eplenmean          | 400          |
| eprewmean          | 353          |
| explained_variance | 0.189        |
| fps                | 2111         |
| nupdates           | 5            |
| policy_entropy     | 0.6841877    |
| policy_loss        | 0.011616923  |
| serial_timesteps   | 2000         |
| time_elapsed       | 34.2         |
| time_remaining     | 6.96         |
| total_timesteps    | 60000        |
| true_eprew         | 191          |
| value_loss         | 113.33972    |
-------------------------------------
Current reward shaping 0.94
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.9110372066497803 seconds
Total simulation time for 400 steps: 5.025535583496094 	 Other agent action time: 0 	 79.59350667292135 steps/s
Curr learning rate 0.0009494949494949496 	 Curr reward per step 0.8866716666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 138.50it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 148.63it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 149.46it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 155.70it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 149.05it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 147.56it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 151.56it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 145.17it/s]
-------------------------------------
| approxkl           | 0.004818913  |
| clipfrac           | 0.2899375    |
| eplenmean          | 400          |
| eprewmean          | 347          |
| explained_variance | 0.187        |
| fps                | 2143         |
| nupdates           | 6            |
| policy_entropy     | 0.6918097    |
| policy_loss        | 0.0073648067 |
| serial_timesteps   | 2400         |
| time_elapsed       | 39.8         |
| time_remaining     | 6.64         |
| total_timesteps    | 72000        |
| true_eprew         | 189          |
| value_loss         | 108.96708    |
-------------------------------------
Current reward shaping 0.928
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8638715744018555 seconds
Total simulation time for 400 steps: 5.055621385574341 	 Other agent action time: 0 	 79.1198488758189 steps/s
Curr learning rate 0.0009393939393939395 	 Curr reward per step 0.8339413333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 140.81it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 149.03it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 158.98it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 147.51it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 148.41it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 146.58it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 149.19it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 155.26it/s]
------------------------------------
| approxkl           | 0.003807736 |
| clipfrac           | 0.25441667  |
| eplenmean          | 400         |
| eprewmean          | 343         |
| explained_variance | 0.198       |
| fps                | 2134        |
| nupdates           | 7           |
| policy_entropy     | 0.69950116  |
| policy_loss        | 0.005692622 |
| serial_timesteps   | 2800        |
| time_elapsed       | 45.4        |
| time_remaining     | 6.38        |
| total_timesteps    | 84000       |
| true_eprew         | 187         |
| value_loss         | 105.62343   |
------------------------------------
Current reward shaping 0.916
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8724105358123779 seconds
Total simulation time for 400 steps: 5.124622583389282 	 Other agent action time: 0 	 78.05452860012399 steps/s
Curr learning rate 0.0009292929292929292 	 Curr reward per step 0.8735493333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 144.30it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 144.96it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 155.23it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 142.21it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 134.40it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 147.97it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 141.00it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 146.02it/s]
------------------------------------
| approxkl           | 0.00289413  |
| clipfrac           | 0.2623958   |
| eplenmean          | 400         |
| eprewmean          | 347         |
| explained_variance | 0.185       |
| fps                | 2101        |
| nupdates           | 8           |
| policy_entropy     | 0.6978289   |
| policy_loss        | 0.004835695 |
| serial_timesteps   | 3200        |
| time_elapsed       | 51.1        |
| time_remaining     | 6.18        |
| total_timesteps    | 96000       |
| true_eprew         | 190         |
| value_loss         | 106.14242   |
------------------------------------
Current reward shaping 0.904
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8586668968200684 seconds
Total simulation time for 400 steps: 4.964223623275757 	 Other agent action time: 0 	 80.57654738286162 steps/s
Curr learning rate 0.0009191919191919192 	 Curr reward per step 0.8355553333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 144.73it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 154.84it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.18it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 138.47it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 147.23it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 146.29it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 154.04it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 149.56it/s]
------------------------------------
| approxkl           | 0.003821836 |
| clipfrac           | 0.27243748  |
| eplenmean          | 400         |
| eprewmean          | 341         |
| explained_variance | 0.223       |
| fps                | 2167        |
| nupdates           | 9           |
| policy_entropy     | 0.7004801   |
| policy_loss        | 0.006200562 |
| serial_timesteps   | 3600        |
| time_elapsed       | 56.7        |
| time_remaining     | 5.98        |
| total_timesteps    | 108000      |
| true_eprew         | 188         |
| value_loss         | 102.728836  |
------------------------------------
Current reward shaping 0.892
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8717596530914307 seconds
Total simulation time for 400 steps: 5.155061721801758 	 Other agent action time: 0 	 77.59363933671682 steps/s
Curr learning rate 0.0009090909090909091 	 Curr reward per step 0.8525273333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 135.69it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 137.47it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 154.36it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 149.91it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 143.55it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 142.90it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 132.29it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 141.85it/s]
-------------------------------------
| approxkl           | 0.0033288575 |
| clipfrac           | 0.27232295   |
| eplenmean          | 400          |
| eprewmean          | 342          |
| explained_variance | 0.215        |
| fps                | 2087         |
| nupdates           | 10           |
| policy_entropy     | 0.71975213   |
| policy_loss        | 0.005705506  |
| serial_timesteps   | 4000         |
| time_elapsed       | 62.4         |
| time_remaining     | 5.83         |
| total_timesteps    | 120000       |
| true_eprew         | 190          |
| value_loss         | 104.10709    |
-------------------------------------
Current reward shaping 0.88
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8614630699157715 seconds
Total simulation time for 400 steps: 4.964604616165161 	 Other agent action time: 0 	 80.57036379041487 steps/s
Curr learning rate 0.000898989898989899 	 Curr reward per step 0.8502400000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 143.45it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 140.55it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 159.84it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 152.82it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 148.02it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 152.09it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 153.42it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 154.40it/s]
-------------------------------------
| approxkl           | 0.0068010376 |
| clipfrac           | 0.33794788   |
| eplenmean          | 400          |
| eprewmean          | 339          |
| explained_variance | 0.22         |
| fps                | 2170         |
| nupdates           | 11           |
| policy_entropy     | 0.7239901    |
| policy_loss        | 0.010475113  |
| serial_timesteps   | 4400         |
| time_elapsed       | 68           |
| time_remaining     | 5.66         |
| total_timesteps    | 132000       |
| true_eprew         | 190          |
| value_loss         | 102.92285    |
-------------------------------------
Current reward shaping 0.868
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.879035234451294 seconds
Total simulation time for 400 steps: 5.146835088729858 	 Other agent action time: 0 	 77.71766398264617 steps/s
Curr learning rate 0.0008888888888888889 	 Curr reward per step 0.8235593333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 140.45it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 148.18it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 152.97it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 146.41it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 141.26it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 144.55it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 145.00it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 141.42it/s]
-------------------------------------
| approxkl           | 0.0052832467 |
| clipfrac           | 0.3005833    |
| eplenmean          | 400          |
| eprewmean          | 337          |
| explained_variance | 0.23         |
| fps                | 2093         |
| nupdates           | 12           |
| policy_entropy     | 0.69708645   |
| policy_loss        | 0.007875668  |
| serial_timesteps   | 4800         |
| time_elapsed       | 73.7         |
| time_remaining     | 5.53         |
| total_timesteps    | 144000       |
| true_eprew         | 190          |
| value_loss         | 104.00266    |
-------------------------------------
Current reward shaping 0.856
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8591599464416504 seconds
Total simulation time for 400 steps: 5.035536527633667 	 Other agent action time: 0 	 79.43542814254407 steps/s
Curr learning rate 0.0008787878787878789 	 Curr reward per step 0.7751500000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 148.07it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 139.59it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 147.37it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 149.24it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 146.61it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 143.52it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 129.81it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 142.36it/s]
-------------------------------------
| approxkl           | 0.0061226143 |
| clipfrac           | 0.34108335   |
| eplenmean          | 400          |
| eprewmean          | 329          |
| explained_variance | 0.229        |
| fps                | 2134         |
| nupdates           | 13           |
| policy_entropy     | 0.75906926   |
| policy_loss        | 0.009125518  |
| serial_timesteps   | 5200         |
| time_elapsed       | 79.3         |
| time_remaining     | 5.39         |
| total_timesteps    | 156000       |
| true_eprew         | 186          |
| value_loss         | 97.7913      |
-------------------------------------
Current reward shaping 0.844
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8623089790344238 seconds
Total simulation time for 400 steps: 5.010074138641357 	 Other agent action time: 0 	 79.83913789117557 steps/s
Curr learning rate 0.0008686868686868688 	 Curr reward per step 0.8098230000000002

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 139.51it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 144.85it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.59it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 139.48it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 145.73it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 154.10it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 161.53it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 166.96it/s]
-------------------------------------
| approxkl           | 0.004527745  |
| clipfrac           | 0.3263125    |
| eplenmean          | 400          |
| eprewmean          | 322          |
| explained_variance | 0.233        |
| fps                | 2152         |
| nupdates           | 14           |
| policy_entropy     | 0.76072407   |
| policy_loss        | 0.0076671964 |
| serial_timesteps   | 5600         |
| time_elapsed       | 84.9         |
| time_remaining     | 5.26         |
| total_timesteps    | 168000       |
| true_eprew         | 184          |
| value_loss         | 92.279106    |
-------------------------------------
Current reward shaping 0.832
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8788356781005859 seconds
Total simulation time for 400 steps: 4.945655822753906 	 Other agent action time: 0 	 80.87906120755217 steps/s
Curr learning rate 0.0008585858585858587 	 Curr reward per step 0.785024

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 137.33it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 150.43it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 142.68it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 147.09it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 146.82it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 149.42it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 150.26it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 148.23it/s]
-------------------------------------
| approxkl           | 0.0037293904 |
| clipfrac           | 0.30871877   |
| eplenmean          | 400          |
| eprewmean          | 316          |
| explained_variance | 0.346        |
| fps                | 2173         |
| nupdates           | 15           |
| policy_entropy     | 0.77010435   |
| policy_loss        | 0.006063801  |
| serial_timesteps   | 6000         |
| time_elapsed       | 90.4         |
| time_remaining     | 5.12         |
| total_timesteps    | 180000       |
| true_eprew         | 182          |
| value_loss         | 99.69959     |
-------------------------------------
Current reward shaping 0.8200000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8933863639831543 seconds
Total simulation time for 400 steps: 5.092947959899902 	 Other agent action time: 0 	 78.53997393051345 steps/s
Curr learning rate 0.0008484848484848486 	 Curr reward per step 0.8150266666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 144.77it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 157.33it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 155.81it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 150.15it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 158.80it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 147.56it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 153.28it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 155.30it/s]
------------------------------------
| approxkl           | 0.005245127 |
| clipfrac           | 0.31922916  |
| eplenmean          | 400         |
| eprewmean          | 321         |
| explained_variance | 0.189       |
| fps                | 2125        |
| nupdates           | 16          |
| policy_entropy     | 0.76728106  |
| policy_loss        | 0.008232439 |
| serial_timesteps   | 6400        |
| time_elapsed       | 96.1        |
| time_remaining     | 5           |
| total_timesteps    | 192000      |
| true_eprew         | 186         |
| value_loss         | 94.86068    |
------------------------------------
Current reward shaping 0.808
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8739607334136963 seconds
Total simulation time for 400 steps: 5.025160312652588 	 Other agent action time: 0 	 79.59945058725012 steps/s
Curr learning rate 0.0008383838383838385 	 Curr reward per step 0.7736046666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 139.16it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 155.27it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.10it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 135.94it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 146.90it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 141.58it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 156.93it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 149.08it/s]
-------------------------------------
| approxkl           | 0.003451929  |
| clipfrac           | 0.2993229    |
| eplenmean          | 400          |
| eprewmean          | 316          |
| explained_variance | 0.266        |
| fps                | 2139         |
| nupdates           | 17           |
| policy_entropy     | 0.80055285   |
| policy_loss        | 0.0050339224 |
| serial_timesteps   | 6800         |
| time_elapsed       | 102          |
| time_remaining     | 4.88         |
| total_timesteps    | 204000       |
| true_eprew         | 184          |
| value_loss         | 87.94954     |
-------------------------------------
Current reward shaping 0.796
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8784315586090088 seconds
Total simulation time for 400 steps: 5.055846691131592 	 Other agent action time: 0 	 79.11632302886791 steps/s
Curr learning rate 0.0008282828282828282 	 Curr reward per step 0.8045606666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 111.49it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 114.56it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.72it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 157.13it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 162.72it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 160.35it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 158.02it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 155.20it/s]
-------------------------------------
| approxkl           | 0.0025073146 |
| clipfrac           | 0.26314577   |
| eplenmean          | 400          |
| eprewmean          | 318          |
| explained_variance | 0.216        |
| fps                | 2125         |
| nupdates           | 18           |
| policy_entropy     | 0.79356295   |
| policy_loss        | 0.0035312667 |
| serial_timesteps   | 7200         |
| time_elapsed       | 107          |
| time_remaining     | 4.77         |
| total_timesteps    | 216000       |
| true_eprew         | 187          |
| value_loss         | 95.55156     |
-------------------------------------
Current reward shaping 0.784
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7648780345916748 seconds
Total simulation time for 400 steps: 4.549289703369141 	 Other agent action time: 0 	 87.92581393613284 steps/s
Curr learning rate 0.0008181818181818183 	 Curr reward per step 0.7905906666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 164.25it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 157.37it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 160.60it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 147.27it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 149.07it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 161.80it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 157.03it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 167.31it/s]
-------------------------------------
| approxkl           | 0.0035737972 |
| clipfrac           | 0.31298953   |
| eplenmean          | 400          |
| eprewmean          | 318          |
| explained_variance | 0.233        |
| fps                | 2359         |
| nupdates           | 19           |
| policy_entropy     | 0.80142796   |
| policy_loss        | 0.0061999243 |
| serial_timesteps   | 7600         |
| time_elapsed       | 112          |
| time_remaining     | 4.63         |
| total_timesteps    | 228000       |
| true_eprew         | 188          |
| value_loss         | 95.59661     |
-------------------------------------
Current reward shaping 0.772
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7790379524230957 seconds
Total simulation time for 400 steps: 4.587655067443848 	 Other agent action time: 0 	 87.19051326212113 steps/s
Curr learning rate 0.0008080808080808081 	 Curr reward per step 0.7946783333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 146.03it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.09it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 149.89it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 155.00it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 153.18it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 156.12it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 140.13it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 167.27it/s]
-------------------------------------
| approxkl           | 0.0046400726 |
| clipfrac           | 0.33009383   |
| eplenmean          | 400          |
| eprewmean          | 317          |
| explained_variance | 0.179        |
| fps                | 2336         |
| nupdates           | 20           |
| policy_entropy     | 0.75522095   |
| policy_loss        | 0.0069005787 |
| serial_timesteps   | 8000         |
| time_elapsed       | 118          |
| time_remaining     | 4.51         |
| total_timesteps    | 240000       |
| true_eprew         | 189          |
| value_loss         | 103.61648    |
-------------------------------------
Current reward shaping 0.76
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8509562015533447 seconds
Total simulation time for 400 steps: 5.008619070053101 	 Other agent action time: 0 	 79.86233219284517 steps/s
Curr learning rate 0.000797979797979798 	 Curr reward per step 0.7889866666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 138.90it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 147.51it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 141.71it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 142.32it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 126.71it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 130.38it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 140.92it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 148.10it/s]
-------------------------------------
| approxkl           | 0.0060103447 |
| clipfrac           | 0.33726043   |
| eplenmean          | 400          |
| eprewmean          | 318          |
| explained_variance | 0.271        |
| fps                | 2138         |
| nupdates           | 21           |
| policy_entropy     | 0.7746206    |
| policy_loss        | 0.009927534  |
| serial_timesteps   | 8400         |
| time_elapsed       | 123          |
| time_remaining     | 4.4          |
| total_timesteps    | 252000       |
| true_eprew         | 190          |
| value_loss         | 95.34242     |
-------------------------------------
Current reward shaping 0.748
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8184647560119629 seconds
Total simulation time for 400 steps: 4.790920972824097 	 Other agent action time: 0 	 83.4912540342348 steps/s
Curr learning rate 0.0007878787878787879 	 Curr reward per step 0.7909023333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.52it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.47it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 129.35it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 146.67it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 133.45it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 150.01it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 158.63it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 159.46it/s]
------------------------------------
| approxkl           | 0.012806033 |
| clipfrac           | 0.37594795  |
| eplenmean          | 400         |
| eprewmean          | 317         |
| explained_variance | 0.25        |
| fps                | 2240        |
| nupdates           | 22          |
| policy_entropy     | 0.7594219   |
| policy_loss        | 0.014371762 |
| serial_timesteps   | 8800        |
| time_elapsed       | 129         |
| time_remaining     | 4.28        |
| total_timesteps    | 264000      |
| true_eprew         | 191         |
| value_loss         | 100.39707   |
------------------------------------
Current reward shaping 0.736
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7628684043884277 seconds
Total simulation time for 400 steps: 4.4951183795928955 	 Other agent action time: 0 	 88.98542067678012 steps/s
Curr learning rate 0.0007777777777777778 	 Curr reward per step 0.7222960000000004

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.80it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.26it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 163.05it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 138.69it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 156.34it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 146.79it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 141.97it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 140.12it/s]
-------------------------------------
| approxkl           | 0.0031992937 |
| clipfrac           | 0.30814585   |
| eplenmean          | 400          |
| eprewmean          | 309          |
| explained_variance | 0.238        |
| fps                | 2374         |
| nupdates           | 23           |
| policy_entropy     | 0.8283394    |
| policy_loss        | 0.0056439107 |
| serial_timesteps   | 9200         |
| time_elapsed       | 134          |
| time_remaining     | 4.16         |
| total_timesteps    | 276000       |
| true_eprew         | 186          |
| value_loss         | 112.2375     |
-------------------------------------
Current reward shaping 0.724
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8027002811431885 seconds
Total simulation time for 400 steps: 4.562146186828613 	 Other agent action time: 0 	 87.67803214084662 steps/s
Curr learning rate 0.0007676767676767678 	 Curr reward per step 0.7075936666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.29it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 154.37it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 159.79it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 158.77it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 159.68it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 158.45it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 150.55it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.61it/s]
-------------------------------------
| approxkl           | 0.0028738817 |
| clipfrac           | 0.3041458    |
| eplenmean          | 400          |
| eprewmean          | 299          |
| explained_variance | 0.373        |
| fps                | 2351         |
| nupdates           | 24           |
| policy_entropy     | 0.8673027    |
| policy_loss        | 0.0038935593 |
| serial_timesteps   | 9600         |
| time_elapsed       | 139          |
| time_remaining     | 4.04         |
| total_timesteps    | 288000       |
| true_eprew         | 181          |
| value_loss         | 102.48958    |
-------------------------------------
Current reward shaping 0.712
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7630598545074463 seconds
Total simulation time for 400 steps: 4.563300848007202 	 Other agent action time: 0 	 87.65584679227985 steps/s
Curr learning rate 0.0007575757575757577 	 Curr reward per step 0.7733766666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.34it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 153.46it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.24it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 158.18it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 168.39it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.12it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 141.80it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 154.37it/s]
-------------------------------------
| approxkl           | 0.0060674893 |
| clipfrac           | 0.37015623   |
| eplenmean          | 400          |
| eprewmean          | 295          |
| explained_variance | 0.196        |
| fps                | 2352         |
| nupdates           | 25           |
| policy_entropy     | 0.8230653    |
| policy_loss        | 0.0104813855 |
| serial_timesteps   | 10000        |
| time_elapsed       | 144          |
| time_remaining     | 3.93         |
| total_timesteps    | 300000       |
| true_eprew         | 179          |
| value_loss         | 110.52135    |
-------------------------------------
Current reward shaping 0.7
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7557928562164307 seconds
Total simulation time for 400 steps: 4.430241346359253 	 Other agent action time: 0 	 90.2885348060592 steps/s
Curr learning rate 0.0007474747474747475 	 Curr reward per step 0.7445749999999998

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 146.53it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 161.67it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 149.39it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.42it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 159.29it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.91it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 154.37it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 131.89it/s]
------------------------------------
| approxkl           | 0.00724418  |
| clipfrac           | 0.36731255  |
| eplenmean          | 400         |
| eprewmean          | 293         |
| explained_variance | 0.262       |
| fps                | 2408        |
| nupdates           | 26          |
| policy_entropy     | 0.8487298   |
| policy_loss        | 0.010809791 |
| serial_timesteps   | 10400       |
| time_elapsed       | 149         |
| time_remaining     | 3.81        |
| total_timesteps    | 312000      |
| true_eprew         | 179         |
| value_loss         | 101.413895  |
------------------------------------
Current reward shaping 0.688
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.862492561340332 seconds
Total simulation time for 400 steps: 5.061718940734863 	 Other agent action time: 0 	 79.02453784641148 steps/s
Curr learning rate 0.0007373737373737374 	 Curr reward per step 0.7336973333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 137.49it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 143.87it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 142.79it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 144.99it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 145.87it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 144.22it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 149.04it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.47it/s]
------------------------------------
| approxkl           | 0.005722089 |
| clipfrac           | 0.32083336  |
| eplenmean          | 400         |
| eprewmean          | 297         |
| explained_variance | 0.276       |
| fps                | 2125        |
| nupdates           | 27          |
| policy_entropy     | 0.8275708   |
| policy_loss        | 0.008165185 |
| serial_timesteps   | 10800       |
| time_elapsed       | 154         |
| time_remaining     | 3.72        |
| total_timesteps    | 324000      |
| true_eprew         | 182         |
| value_loss         | 98.63594    |
------------------------------------
Current reward shaping 0.6759999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8140749931335449 seconds
Total simulation time for 400 steps: 4.629662036895752 	 Other agent action time: 0 	 86.39939520687025 steps/s
Curr learning rate 0.0007272727272727272 	 Curr reward per step 0.6757629999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.11it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 165.96it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 164.64it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 143.60it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.14it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 151.32it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 149.69it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 166.46it/s]
------------------------------------
| approxkl           | 0.00549299  |
| clipfrac           | 0.3650208   |
| eplenmean          | 400         |
| eprewmean          | 288         |
| explained_variance | 0.28        |
| fps                | 2321        |
| nupdates           | 28          |
| policy_entropy     | 0.8497899   |
| policy_loss        | 0.009687213 |
| serial_timesteps   | 11200       |
| time_elapsed       | 160         |
| time_remaining     | 3.61        |
| total_timesteps    | 336000      |
| true_eprew         | 179         |
| value_loss         | 102.119     |
------------------------------------
Current reward shaping 0.6639999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.760979413986206 seconds
Total simulation time for 400 steps: 4.5312395095825195 	 Other agent action time: 0 	 88.27606643923652 steps/s
Curr learning rate 0.0007171717171717171 	 Curr reward per step 0.7243073333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.52it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.80it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 165.41it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 159.94it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.25it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 154.40it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 133.38it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 160.49it/s]
------------------------------------
| approxkl           | 0.003673707 |
| clipfrac           | 0.3379479   |
| eplenmean          | 400         |
| eprewmean          | 285         |
| explained_variance | 0.245       |
| fps                | 2364        |
| nupdates           | 29          |
| policy_entropy     | 0.82844734  |
| policy_loss        | 0.007243313 |
| serial_timesteps   | 11600       |
| time_elapsed       | 165         |
| time_remaining     | 3.5         |
| total_timesteps    | 348000      |
| true_eprew         | 178         |
| value_loss         | 106.42818   |
------------------------------------
Current reward shaping 0.652
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.812145471572876 seconds
Total simulation time for 400 steps: 4.718456268310547 	 Other agent action time: 0 	 84.77348888161272 steps/s
Curr learning rate 0.0007070707070707071 	 Curr reward per step 0.7378443333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 156.45it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 153.52it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 142.09it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 142.00it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 146.64it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 144.38it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 147.76it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 145.93it/s]
-------------------------------------
| approxkl           | 0.0038100872 |
| clipfrac           | 0.34346884   |
| eplenmean          | 400          |
| eprewmean          | 287          |
| explained_variance | 0.246        |
| fps                | 2267         |
| nupdates           | 30           |
| policy_entropy     | 0.83311594   |
| policy_loss        | 0.0070674284 |
| serial_timesteps   | 12000        |
| time_elapsed       | 170          |
| time_remaining     | 3.4          |
| total_timesteps    | 360000       |
| true_eprew         | 180          |
| value_loss         | 99.71389     |
-------------------------------------
Current reward shaping 0.64
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.880220890045166 seconds
Total simulation time for 400 steps: 5.034351110458374 	 Other agent action time: 0 	 79.45413246387186 steps/s
Curr learning rate 0.000696969696969697 	 Curr reward per step 0.7104533333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 143.07it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 144.83it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 142.29it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 154.22it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 147.73it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 151.62it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 154.47it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 148.95it/s]
------------------------------------
| approxkl           | 0.021827582 |
| clipfrac           | 0.45858335  |
| eplenmean          | 400         |
| eprewmean          | 288         |
| explained_variance | 0.24        |
| fps                | 2140        |
| nupdates           | 31          |
| policy_entropy     | 0.8053039   |
| policy_loss        | 0.020129625 |
| serial_timesteps   | 12400       |
| time_elapsed       | 176         |
| time_remaining     | 3.3         |
| total_timesteps    | 372000      |
| true_eprew         | 182         |
| value_loss         | 106.21735   |
------------------------------------
Current reward shaping 0.628
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8846971988677979 seconds
Total simulation time for 400 steps: 4.925607442855835 	 Other agent action time: 0 	 81.2082579946896 steps/s
Curr learning rate 0.0006868686868686869 	 Curr reward per step 0.670061

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.86it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 157.19it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 147.13it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 138.97it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 153.09it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 162.45it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 144.00it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 146.36it/s]
-------------------------------------
| approxkl           | 0.0064299563 |
| clipfrac           | 0.39657295   |
| eplenmean          | 400          |
| eprewmean          | 283          |
| explained_variance | 0.28         |
| fps                | 2188         |
| nupdates           | 32           |
| policy_entropy     | 0.8073524    |
| policy_loss        | 0.011672002  |
| serial_timesteps   | 12800        |
| time_elapsed       | 181          |
| time_remaining     | 3.21         |
| total_timesteps    | 384000       |
| true_eprew         | 180          |
| value_loss         | 108.916504   |
-------------------------------------
Current reward shaping 0.616
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7918558120727539 seconds
Total simulation time for 400 steps: 4.467278480529785 	 Other agent action time: 0 	 89.53997422443274 steps/s
Curr learning rate 0.0006767676767676768 	 Curr reward per step 0.6724713333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.86it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.84it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.80it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 159.16it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.71it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 167.88it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 142.44it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 163.41it/s]
-------------------------------------
| approxkl           | 0.0053014713 |
| clipfrac           | 0.36541665   |
| eplenmean          | 400          |
| eprewmean          | 277          |
| explained_variance | 0.147        |
| fps                | 2398         |
| nupdates           | 33           |
| policy_entropy     | 0.789659     |
| policy_loss        | 0.009503744  |
| serial_timesteps   | 13200        |
| time_elapsed       | 186          |
| time_remaining     | 3.1          |
| total_timesteps    | 396000       |
| true_eprew         | 177          |
| value_loss         | 114.129166   |
-------------------------------------
Current reward shaping 0.604
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7513656616210938 seconds
Total simulation time for 400 steps: 4.426726579666138 	 Other agent action time: 0 	 90.36022279699233 steps/s
Curr learning rate 0.0006666666666666668 	 Curr reward per step 0.6407953333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 152.02it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 159.69it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 157.08it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 172.08it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 151.48it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 157.87it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 148.30it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 159.22it/s]
-------------------------------------
| approxkl           | 0.0069027403 |
| clipfrac           | 0.36923963   |
| eplenmean          | 400          |
| eprewmean          | 265          |
| explained_variance | 0.187        |
| fps                | 2417         |
| nupdates           | 34           |
| policy_entropy     | 0.7876899    |
| policy_loss        | 0.011955248  |
| serial_timesteps   | 13600        |
| time_elapsed       | 191          |
| time_remaining     | 3            |
| total_timesteps    | 408000       |
| true_eprew         | 170          |
| value_loss         | 121.93302    |
-------------------------------------
Current reward shaping 0.5920000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8151669502258301 seconds
Total simulation time for 400 steps: 4.600747585296631 	 Other agent action time: 0 	 86.94239198828166 steps/s
Curr learning rate 0.0006565656565656567 	 Curr reward per step 0.5749586666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 154.09it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 142.50it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.66it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 169.80it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.57it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 153.83it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.46it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 135.43it/s]
------------------------------------
| approxkl           | 0.007829199 |
| clipfrac           | 0.4051354   |
| eplenmean          | 400         |
| eprewmean          | 253         |
| explained_variance | 0.228       |
| fps                | 2331        |
| nupdates           | 35          |
| policy_entropy     | 0.80995166  |
| policy_loss        | 0.01199185  |
| serial_timesteps   | 14000       |
| time_elapsed       | 196         |
| time_remaining     | 2.9         |
| total_timesteps    | 420000      |
| true_eprew         | 163         |
| value_loss         | 126.105064  |
------------------------------------
Current reward shaping 0.5800000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7697489261627197 seconds
Total simulation time for 400 steps: 4.510838747024536 	 Other agent action time: 0 	 88.67530462352488 steps/s
Curr learning rate 0.0006464646464646465 	 Curr reward per step 0.5744466666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 137.64it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 152.95it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.07it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 153.42it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 151.73it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 144.70it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 163.70it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.38it/s]
------------------------------------
| approxkl           | 0.017225856 |
| clipfrac           | 0.46681252  |
| eplenmean          | 400         |
| eprewmean          | 240         |
| explained_variance | 0.222       |
| fps                | 2370        |
| nupdates           | 36          |
| policy_entropy     | 0.7807445   |
| policy_loss        | 0.021518474 |
| serial_timesteps   | 14400       |
| time_elapsed       | 201         |
| time_remaining     | 2.79        |
| total_timesteps    | 432000      |
| true_eprew         | 156         |
| value_loss         | 120.92644   |
------------------------------------
Current reward shaping 0.5680000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7910544872283936 seconds
Total simulation time for 400 steps: 4.512543439865112 	 Other agent action time: 0 	 88.6418059638572 steps/s
Curr learning rate 0.0006363636363636364 	 Curr reward per step 0.6257753333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 153.63it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.70it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 165.86it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.55it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.85it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 159.46it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 157.48it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 161.38it/s]
-------------------------------------
| approxkl           | 0.0083867945 |
| clipfrac           | 0.41878128   |
| eplenmean          | 400          |
| eprewmean          | 238          |
| explained_variance | 0.273        |
| fps                | 2378         |
| nupdates           | 37           |
| policy_entropy     | 0.7998649    |
| policy_loss        | 0.014282195  |
| serial_timesteps   | 14800        |
| time_elapsed       | 206          |
| time_remaining     | 2.69         |
| total_timesteps    | 444000       |
| true_eprew         | 156          |
| value_loss         | 100.03267    |
-------------------------------------
Current reward shaping 0.556
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8796782493591309 seconds
Total simulation time for 400 steps: 5.016278028488159 	 Other agent action time: 0 	 79.74039671013905 steps/s
Curr learning rate 0.0006262626262626263 	 Curr reward per step 0.6554063333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 139.40it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 154.24it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 143.71it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 152.40it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 146.90it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 128.33it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 136.98it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 147.25it/s]
------------------------------------
| approxkl           | 0.008155319 |
| clipfrac           | 0.40195832  |
| eplenmean          | 400         |
| eprewmean          | 250         |
| explained_variance | 0.22        |
| fps                | 2140        |
| nupdates           | 38          |
| policy_entropy     | 0.7931284   |
| policy_loss        | 0.012335426 |
| serial_timesteps   | 15200       |
| time_elapsed       | 212         |
| time_remaining     | 2.6         |
| total_timesteps    | 456000      |
| true_eprew         | 165         |
| value_loss         | 109.81981   |
------------------------------------
Current reward shaping 0.544
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8054771423339844 seconds
Total simulation time for 400 steps: 4.603062629699707 	 Other agent action time: 0 	 86.89866555782558 steps/s
Curr learning rate 0.0006161616161616161 	 Curr reward per step 0.6911226666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 156.78it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 161.59it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 159.63it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.86it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 158.82it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.11it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 165.06it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 160.94it/s]
------------------------------------
| approxkl           | 0.006229497 |
| clipfrac           | 0.35636455  |
| eplenmean          | 400         |
| eprewmean          | 260         |
| explained_variance | 0.216       |
| fps                | 2340        |
| nupdates           | 39          |
| policy_entropy     | 0.760584    |
| policy_loss        | 0.009267278 |
| serial_timesteps   | 15600       |
| time_elapsed       | 217         |
| time_remaining     | 2.5         |
| total_timesteps    | 468000      |
| true_eprew         | 173         |
| value_loss         | 93.2057     |
------------------------------------
Current reward shaping 0.532
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7654392719268799 seconds
Total simulation time for 400 steps: 4.472468376159668 	 Other agent action time: 0 	 89.43607117094123 steps/s
Curr learning rate 0.0006060606060606061 	 Curr reward per step 0.6460349999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 154.35it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 148.46it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 147.12it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 129.79it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.39it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 140.97it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 150.02it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 149.28it/s]
-------------------------------------
| approxkl           | 0.0041937428 |
| clipfrac           | 0.309375     |
| eplenmean          | 400          |
| eprewmean          | 264          |
| explained_variance | 0.285        |
| fps                | 2373         |
| nupdates           | 40           |
| policy_entropy     | 0.76288897   |
| policy_loss        | 0.007724244  |
| serial_timesteps   | 16000        |
| time_elapsed       | 222          |
| time_remaining     | 2.41         |
| total_timesteps    | 480000       |
| true_eprew         | 178          |
| value_loss         | 98.97116     |
-------------------------------------
Current reward shaping 0.52
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8290750980377197 seconds
Total simulation time for 400 steps: 4.711587429046631 	 Other agent action time: 0 	 84.8970768395437 steps/s
Curr learning rate 0.000595959595959596 	 Curr reward per step 0.6786766666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.16it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 154.51it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 147.58it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 136.80it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 141.48it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 139.58it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 155.28it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 150.89it/s]
------------------------------------
| approxkl           | 0.005331527 |
| clipfrac           | 0.33967716  |
| eplenmean          | 400         |
| eprewmean          | 268         |
| explained_variance | 0.241       |
| fps                | 2269        |
| nupdates           | 41          |
| policy_entropy     | 0.78204435  |
| policy_loss        | 0.008508786 |
| serial_timesteps   | 16400       |
| time_elapsed       | 227         |
| time_remaining     | 2.31        |
| total_timesteps    | 492000      |
| true_eprew         | 181         |
| value_loss         | 92.29445    |
------------------------------------
Current reward shaping 0.508
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8611211776733398 seconds
Total simulation time for 400 steps: 4.972183465957642 	 Other agent action time: 0 	 80.44755442727013 steps/s
Curr learning rate 0.0005858585858585859 	 Curr reward per step 0.6540466666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 148.87it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 165.17it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.05it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.69it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 168.65it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.61it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 151.95it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 170.10it/s]
-------------------------------------
| approxkl           | 0.0026113784 |
| clipfrac           | 0.2741459    |
| eplenmean          | 400          |
| eprewmean          | 264          |
| explained_variance | 0.268        |
| fps                | 2184         |
| nupdates           | 42           |
| policy_entropy     | 0.83174115   |
| policy_loss        | 0.0047944523 |
| serial_timesteps   | 16800        |
| time_elapsed       | 233          |
| time_remaining     | 2.22         |
| total_timesteps    | 504000       |
| true_eprew         | 180          |
| value_loss         | 95.885414    |
-------------------------------------
Current reward shaping 0.496
Current self-play randomization 0.9984
SP envs: 30/30
Other agent actions took 0.7492311000823975 seconds
Total simulation time for 400 steps: 4.4343931674957275 	 Other agent action time: 0 	 90.20399971117027 steps/s
Curr learning rate 0.0005757575757575758 	 Curr reward per step 0.671584

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 161.89it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.49it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 144.65it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 159.05it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 152.86it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 162.07it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.52it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 166.12it/s]
-------------------------------------
| approxkl           | 0.003808194  |
| clipfrac           | 0.3116354    |
| eplenmean          | 400          |
| eprewmean          | 267          |
| explained_variance | 0.2          |
| fps                | 2416         |
| nupdates           | 43           |
| policy_entropy     | 0.77388716   |
| policy_loss        | 0.0067558563 |
| serial_timesteps   | 17200        |
| time_elapsed       | 238          |
| time_remaining     | 2.12         |
| total_timesteps    | 516000       |
| true_eprew         | 184          |
| value_loss         | 95.48619     |
-------------------------------------
Current reward shaping 0.484
Current self-play randomization 0.9936
SP envs: 30/30
Other agent actions took 0.7706363201141357 seconds
Total simulation time for 400 steps: 4.508636236190796 	 Other agent action time: 0 	 88.71862333652079 steps/s
Curr learning rate 0.0005656565656565657 	 Curr reward per step 0.6285856666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.49it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 153.20it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 149.80it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 143.87it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.18it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 113.23it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 117.05it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 115.72it/s]
-------------------------------------
| approxkl           | 0.0032215272 |
| clipfrac           | 0.27437496   |
| eplenmean          | 400          |
| eprewmean          | 260          |
| explained_variance | 0.338        |
| fps                | 2341         |
| nupdates           | 44           |
| policy_entropy     | 0.78549      |
| policy_loss        | 0.006086001  |
| serial_timesteps   | 17600        |
| time_elapsed       | 243          |
| time_remaining     | 2.02         |
| total_timesteps    | 528000       |
| true_eprew         | 180          |
| value_loss         | 98.54501     |
-------------------------------------
Current reward shaping 0.472
Current self-play randomization 0.9888
SP envs: 30/30
Other agent actions took 0.8599467277526855 seconds
Total simulation time for 400 steps: 5.029307842254639 	 Other agent action time: 0 	 79.5338071452552 steps/s
Curr learning rate 0.0005555555555555557 	 Curr reward per step 0.5682533333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 141.44it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 144.41it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 149.34it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 135.28it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 141.53it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 136.17it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 151.37it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 149.26it/s]
------------------------------------
| approxkl           | 0.005574651 |
| clipfrac           | 0.3340208   |
| eplenmean          | 400         |
| eprewmean          | 249         |
| explained_variance | 0.341       |
| fps                | 2136        |
| nupdates           | 45          |
| policy_entropy     | 0.8399192   |
| policy_loss        | 0.009708379 |
| serial_timesteps   | 18000       |
| time_elapsed       | 249         |
| time_remaining     | 1.93        |
| total_timesteps    | 540000      |
| true_eprew         | 173         |
| value_loss         | 111.28457   |
------------------------------------
Current reward shaping 0.45999999999999996
Current self-play randomization 0.984
SP envs: 30/30
Other agent actions took 0.8258447647094727 seconds
Total simulation time for 400 steps: 4.572623014450073 	 Other agent action time: 0 	 87.47714358606622 steps/s
Curr learning rate 0.0005454545454545455 	 Curr reward per step 0.5836299999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 147.23it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 156.79it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 152.73it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 148.72it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.33it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 148.16it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 158.34it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 163.86it/s]
-------------------------------------
| approxkl           | 0.003805134  |
| clipfrac           | 0.29172915   |
| eplenmean          | 400          |
| eprewmean          | 241          |
| explained_variance | 0.319        |
| fps                | 2342         |
| nupdates           | 46           |
| policy_entropy     | 0.7910601    |
| policy_loss        | 0.0072483653 |
| serial_timesteps   | 18400        |
| time_elapsed       | 254          |
| time_remaining     | 1.84         |
| total_timesteps    | 552000       |
| true_eprew         | 169          |
| value_loss         | 104.654526   |
-------------------------------------
Current reward shaping 0.44799999999999995
Current self-play randomization 0.9792
SP envs: 28/30
Other agent actions took 6.107564926147461 seconds
Total simulation time for 400 steps: 9.888309001922607 	 Other agent action time: 0 	 40.45181030671949 steps/s
Curr learning rate 0.0005353535353535353 	 Curr reward per step 0.5642906666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 137.96it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 159.05it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 145.64it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 145.31it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.84it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 154.65it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 154.83it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 166.34it/s]
------------------------------------
| approxkl           | 0.009960919 |
| clipfrac           | 0.4120833   |
| eplenmean          | 400         |
| eprewmean          | 233         |
| explained_variance | 0.286       |
| fps                | 1149        |
| nupdates           | 47          |
| policy_entropy     | 0.78058976  |
| policy_loss        | 0.016827742 |
| serial_timesteps   | 18800       |
| time_elapsed       | 264         |
| time_remaining     | 1.78        |
| total_timesteps    | 564000      |
| true_eprew         | 164         |
| value_loss         | 114.518616  |
------------------------------------
Current reward shaping 0.43600000000000005
Current self-play randomization 0.9744
SP envs: 30/30
Other agent actions took 0.7611067295074463 seconds
Total simulation time for 400 steps: 4.443001985549927 	 Other agent action time: 0 	 90.02921927582496 steps/s
Curr learning rate 0.0005252525252525252 	 Curr reward per step 0.5602253333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 137.33it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 158.46it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 151.77it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.06it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.03it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.28it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 152.88it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 132.10it/s]
------------------------------------
| approxkl           | 0.007278681 |
| clipfrac           | 0.35030213  |
| eplenmean          | 400         |
| eprewmean          | 231         |
| explained_variance | 0.362       |
| fps                | 2401        |
| nupdates           | 48          |
| policy_entropy     | 0.79024476  |
| policy_loss        | 0.011928191 |
| serial_timesteps   | 19200       |
| time_elapsed       | 269         |
| time_remaining     | 1.68        |
| total_timesteps    | 576000      |
| true_eprew         | 164         |
| value_loss         | 111.72819   |
------------------------------------
Current reward shaping 0.42400000000000004
Current self-play randomization 0.9696
SP envs: 28/30
Other agent actions took 6.0289084911346436 seconds
Total simulation time for 400 steps: 9.78330397605896 	 Other agent action time: 0 	 40.88598299499361 steps/s
Curr learning rate 0.0005151515151515151 	 Curr reward per step 0.5019753333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.89it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 160.53it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.41it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 160.61it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.70it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 154.71it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 159.27it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 163.74it/s]
------------------------------------
| approxkl           | 0.003665533 |
| clipfrac           | 0.32898957  |
| eplenmean          | 400         |
| eprewmean          | 218         |
| explained_variance | 0.32        |
| fps                | 1163        |
| nupdates           | 49          |
| policy_entropy     | 0.8210953   |
| policy_loss        | 0.007633362 |
| serial_timesteps   | 19600       |
| time_elapsed       | 279         |
| time_remaining     | 1.62        |
| total_timesteps    | 588000      |
| true_eprew         | 156         |
| value_loss         | 125.58568   |
------------------------------------
Current reward shaping 0.41200000000000003
Current self-play randomization 0.9648
SP envs: 28/30
Other agent actions took 6.080578088760376 seconds
Total simulation time for 400 steps: 9.857919454574585 	 Other agent action time: 0 	 40.57651331431596 steps/s
Curr learning rate 0.000505050505050505 	 Curr reward per step 0.52518

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 153.37it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 149.04it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.49it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.43it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.41it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 154.36it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 163.11it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.90it/s]
-------------------------------------
| approxkl           | 0.0025091944 |
| clipfrac           | 0.2621666    |
| eplenmean          | 400          |
| eprewmean          | 212          |
| explained_variance | 0.317        |
| fps                | 1155         |
| nupdates           | 50           |
| policy_entropy     | 0.81363344   |
| policy_loss        | 0.004097908  |
| serial_timesteps   | 20000        |
| time_elapsed       | 290          |
| time_remaining     | 1.55         |
| total_timesteps    | 600000       |
| true_eprew         | 153          |
| value_loss         | 119.066444   |
-------------------------------------
Current reward shaping 0.4
Current self-play randomization 0.96
../../thesis_data/dr_ppo/ppo_bc_train_simple/
PPO agent on index 0:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 2
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 3
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 4
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0←1  O 
X       X 
X D X S X 


Timestep: 5
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O →0←1  O 
X       X 
X D X S X 


Timestep: 6
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ←0←1  O 
X       X 
X D X S X 


Timestep: 7
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O →0←1  O 
X       X 
X D X S X 


Timestep: 8
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   →0→1O 
X       X 
X D X S X 


Timestep: 9
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ←0  →1O 
X       X 
X D X S X 


Timestep: 10
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ←o  →1O 
X       X 
X D X S X 


Timestep: 11
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →o→1O 
X       X 
X D X S X 


Timestep: 12
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o→oO 
X       X 
X D X S X 


Timestep: 13
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑0↑oO 
X       X 
X D X S X 


Timestep: 14
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø-XoX 
O ←0  ↑1O 
X       X 
X D X S X 


Timestep: 15
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o  ↑oO 
X       X 
X D X S X 


Timestep: 16
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø-XoX 
O   →o↑1O 
X       X 
X D X S X 


Timestep: 17
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø-XoX 
O   ↑o↑1O 
X       X 
X D X S X 


Timestep: 18
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 3 
X X ø=XoX 
O   ↑0←1O 
X       X 
X D X S X 


Timestep: 19
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø=XoX 
O ←0  ←1O 
X       X 
X D X S X 


Timestep: 20
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=XoX 
O ←o  ←1O 
X       X 
X D X S X 


Timestep: 21
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø=XoX 
O   →o←1O 
X       X 
X D X S X 


Timestep: 22
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø=XoX 
O   ↑o←1O 
X       X 
X D X S X 


Timestep: 23
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 3 
X X ø1XoX 
O   ↑0↑1O 
X       X 
X D X S X 


Timestep: 24
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø2XoX 
O ←0  ↑1O 
X       X 
X D X S X 


Timestep: 25
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø3XoX 
O ←0  ↑1O 
X       X 
X D X S X 


Timestep: 26
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø4X X 
O ←0  ↑oO 
X       X 
X D X S X 


Timestep: 27
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø5X X 
O →0  ←oO 
X       X 
X D X S X 


Timestep: 28
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø6X X 
O   →0←oO 
X       X 
X D X S X 


Timestep: 29
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø7X X 
O ←0  ←oO 
X       X 
X D X S X 


Timestep: 30
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø8X X 
O     ←oO 
X ↓0    X 
X D X S X 


Timestep: 31
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø9X X 
O     ↑oO 
X ↓0    X 
X D X S X 


Timestep: 32
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø10X X 
O ↑0  ↑oO 
X       X 
X D X S X 


Timestep: 33
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø11X X 
O ←0  ↑oO 
X       X 
X D X S X 


Timestep: 34
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø12X X 
O     ↑oO 
X ↓0    X 
X D X S X 


Timestep: 35
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø13XoX 
O     ↑1O 
X ←0    X 
X D X S X 


Timestep: 36
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X ø14XoX 
O     →1O 
X ↓0    X 
X D X S X 


Timestep: 37
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø15XoX 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 38
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø16XoX 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 39
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø17XoX 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 40
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø18XoX 
O     →oO 
X   →d  X 
X D X S X 


Timestep: 41
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø19XoX 
O     ←oO 
X   ↑d  X 
X D X S X 


Timestep: 42
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XoX 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 43
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 5 
X X P XoX 
O   ↑s←oO 
X       X 
X D X S X 


Timestep: 44
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 45
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 46
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 47
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 48
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 49
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 50
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 51
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 52
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 53
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 54
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 55
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 56
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 57
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 58
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 59
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 60
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 61
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 62
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 63
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 64
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 65
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 66
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 67
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 68
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 69
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 70
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 71
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 72
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 73
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s←oO 
X       X 
X D X S X 


Timestep: 74
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 75
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 76
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 77
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 78
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 79
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 80
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 81
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 82
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 83
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 84
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 85
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 86
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 87
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 88
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 89
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 90
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 91
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 92
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 93
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 94
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 95
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 96
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 97
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 98
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


Timestep: 99
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←oO 
X       X 
X D X S X 


tot rew 40 tot rew shaped 60
PPO agent on index 1:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X ↑0    X 
X D X S X 


Timestep: 2
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X ↑0    X 
X D X S X 


Timestep: 3
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑oO 
X ↑0    X 
X D X S X 


Timestep: 4
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X ↑0    X 
X D X S X 


Timestep: 5
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X   →0  X 
X D X S X 


Timestep: 6
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   →0  X 
X D X S X 


Timestep: 7
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   →0  X 
X D X S X 


Timestep: 8
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑1  O 
X   →0  X 
X D X S X 


Timestep: 9
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X ←0    X 
X D X S X 


Timestep: 10
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X ←0    X 
X D X S X 


Timestep: 11
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X ←0    X 
X D X S X 


Timestep: 12
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ←0    X 
X D X S X 


Timestep: 13
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑1  O 
X ↓0    X 
X D X S X 


Timestep: 14
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X ↓0    X 
X D X S X 


Timestep: 15
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
X ↓0    X 
X D X S X 


Timestep: 16
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ↓0    X 
X D X S X 


Timestep: 17
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 18
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 3 
X X ø1X X 
O ↑d↑1  O 
X       X 
X D X S X 


Timestep: 19
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø2X X 
O ↑d  →1O 
X       X 
X D X S X 


Timestep: 20
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø3X X 
O   →d→oO 
X       X 
X D X S X 


Timestep: 21
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø4X X 
O   →d→oO 
X       X 
X D X S X 


Timestep: 22
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø5X X 
O   →d→oO 
X       X 
X D X S X 


Timestep: 23
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø6X X 
O   →d→oO 
X       X 
X D X S X 


Timestep: 24
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X ø7X X 
O   ↑d→oO 
X       X 
X D X S X 


Timestep: 25
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø8X X 
O   ↑d→oO 
X       X 
X D X S X 


Timestep: 26
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø9X X 
O   ↑d→oO 
X       X 
X D X S X 


Timestep: 27
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø10X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 28
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø11X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 29
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø12X X 
O   ↑d  O 
X     ↓oX 
X D X S X 


Timestep: 30
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø13X X 
O ←d    O 
X     ↓oX 
X D X S X 


Timestep: 31
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø14X X 
O ←d    O 
X   ←o  X 
X D X S X 


Timestep: 32
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø15X X 
O ←d    O 
X   ←o  X 
X D X S X 


Timestep: 33
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø16X X 
O ←d    O 
X   ↓o  X 
X D X S X 


Timestep: 34
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø17X X 
O   →d  O 
X     →oX 
X D X S X 


Timestep: 35
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø18X X 
O   →d  O 
X     →oX 
X D X S X 


Timestep: 36
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø19X X 
O   ↑d  O 
X   ←o  X 
X D X S X 


Timestep: 37
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d  O 
X   ←o  X 
X D X S X 


Timestep: 38
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d  O 
X   ←o  X 
X D X S X 


Timestep: 39
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d  O 
X   ←o  X 
X D X S X 


Timestep: 40
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 5 
X X P X X 
O   ↑s  O 
X   ←o  X 
X D X S X 


Timestep: 41
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o→sO 
X       X 
X D X S X 


Timestep: 42
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o→sO 
X       X 
X D X S X 


Timestep: 43
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o→sO 
X       X 
X D X S X 


Timestep: 44
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o→sO 
X       X 
X D X S X 


Timestep: 45
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X     ↓sX 
X D X S X 


Timestep: 46
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑1  O 
X     ↓sX 
X D X S X 


Timestep: 47
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑1  O 
X     ↓sX 
X D X S X 


Timestep: 48
Joint action taken: ('interact', 'interact') 	 Reward: 20 + shape * 0 
X X ø-X X 
O   ↑1  O 
X     ↓0X 
X D X S X 


Timestep: 49
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X   ←0  X 
X D X S X 


Timestep: 50
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X ←0    X 
X D X S X 


Timestep: 51
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X ←0    X 
X D X S X 


Timestep: 52
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ←0    X 
X D X S X 


Timestep: 53
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑1  O 
X ↓0    X 
X D X S X 


Timestep: 54
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X   →0  X 
X D X S X 


Timestep: 55
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
X   →0  X 
X D X S X 


Timestep: 56
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X   →0  X 
X D X S X 


Timestep: 57
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o  O 
X   →0  X 
X D X S X 


Timestep: 58
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 3 
X X ø1X X 
O   ↑1  O 
X ←0    X 
X D X S X 


Timestep: 59
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø2X X 
O     →1O 
X ←0    X 
X D X S X 


Timestep: 60
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø3X X 
O     →oO 
X ←0    X 
X D X S X 


Timestep: 61
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø4X X 
O     →oO 
X ←0    X 
X D X S X 


Timestep: 62
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X ø5X X 
O     →oO 
X ↓0    X 
X D X S X 


Timestep: 63
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø6X X 
O     →oO 
X ↓0    X 
X D X S X 


Timestep: 64
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø7X X 
O     →oO 
X ↓d    X 
X D X S X 


Timestep: 65
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø8X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 66
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X ø9X X 
O   ↑o  O 
X   →d  X 
X D X S X 


Timestep: 67
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø10X X 
O   ↑o  O 
X   →d  X 
X D X S X 


Timestep: 68
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø11X X 
O   ↑o  O 
X   →d  X 
X D X S X 


Timestep: 69
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø12X X 
O   ↑o  O 
X   →d  X 
X D X S X 


Timestep: 70
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø13X X 
O   ↑o  O 
X   ↑d  X 
X D X S X 


Timestep: 71
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø14X X 
O   ↑o  O 
X   ↑d  X 
X D X S X 


Timestep: 72
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø15X X 
O   ↑o  O 
X   ↑d  X 
X D X S X 


Timestep: 73
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø16X X 
O   ↑o  O 
X   ↑d  X 
X D X S X 


Timestep: 74
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø17X X 
O   ↑o  O 
X   ↑d  X 
X D X S X 


Timestep: 75
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø18X X 
O   ↑o  O 
X ←d    X 
X D X S X 


Timestep: 76
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø19X X 
O   ↑o  O 
X ←d    X 
X D X S X 


Timestep: 77
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X ←d    X 
X D X S X 


Timestep: 78
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑d    O 
X   ↓o  X 
X D X S X 


Timestep: 79
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑d    O 
X     →oX 
X D X S X 


Timestep: 80
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X Xdø20X X 
O ↑0    O 
X     →oX 
X D X S X 


Timestep: 81
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑d    O 
X     →oX 
X D X S X 


Timestep: 82
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   →d  O 
X     →oX 
X D X S X 


Timestep: 83
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   →d  O 
X     →oX 
X D X S X 


Timestep: 84
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   →d  O 
X     →oX 
X D X S X 


Timestep: 85
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   →d  O 
X     →oX 
X D X S X 


Timestep: 86
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d  O 
X     →oX 
X D X S X 


Timestep: 87
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 5 
X X P X X 
O   ↑s  O 
X     →oX 
X D X S X 


Timestep: 88
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s  O 
X   ←o  X 
X D X S X 


Timestep: 89
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s  O 
X   ↑o  X 
X D X S X 


Timestep: 90
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o→sO 
X       X 
X D X S X 


Timestep: 91
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o→sO 
X       X 
X D X S X 


Timestep: 92
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o→sO 
X       X 
X D X S X 


Timestep: 93
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o→sO 
X       X 
X D X S X 


Timestep: 94
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o→sO 
X       X 
X D X S X 


Timestep: 95
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X     ↓sX 
X D X S X 


Timestep: 96
Joint action taken: ('interact', 'interact') 	 Reward: 20 + shape * 3 
X X ø-X X 
O   ↑1  O 
X     ↓0X 
X D X S X 


Timestep: 97
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X   ←0  X 
X D X S X 


Timestep: 98
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X ←0    X 
X D X S X 


Timestep: 99
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X   →0  X 
X D X S X 


tot rew 60 tot rew shaped 57
../../thesis_data/dr_ppo/ppo_bc_train_simple/
SP envs: 28/30
Other agent actions took 6.1303606033325195 seconds
Total simulation time for 400 steps: 9.954830884933472 	 Other agent action time: 0 	 40.181496262824076 steps/s
Curr learning rate 0.000494949494949495 	 Curr reward per step 0.5042

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 150.47it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 141.48it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 164.86it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.85it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.31it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 170.18it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.55it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 146.83it/s]
-------------------------------------
| approxkl           | 0.004424344  |
| clipfrac           | 0.328448     |
| eplenmean          | 400          |
| eprewmean          | 204          |
| explained_variance | 0.311        |
| fps                | 1143         |
| nupdates           | 51           |
| policy_entropy     | 0.84089863   |
| policy_loss        | 0.0087494645 |
| serial_timesteps   | 20400        |
| time_elapsed       | 303          |
| time_remaining     | 1.49         |
| total_timesteps    | 612000       |
| true_eprew         | 148          |
| value_loss         | 126.18904    |
-------------------------------------
Current reward shaping 0.388
Current self-play randomization 0.9552
SP envs: 28/30
Other agent actions took 6.0229785442352295 seconds
Total simulation time for 400 steps: 9.756018877029419 	 Other agent action time: 0 	 41.00033067195077 steps/s
Curr learning rate 0.0004848484848484849 	 Curr reward per step 0.5144096666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 151.13it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 143.37it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 140.14it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.57it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.64it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.28it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 150.69it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 159.93it/s]
-------------------------------------
| approxkl           | 0.0032065653 |
| clipfrac           | 0.30672917   |
| eplenmean          | 400          |
| eprewmean          | 202          |
| explained_variance | 0.352        |
| fps                | 1164         |
| nupdates           | 52           |
| policy_entropy     | 0.78757596   |
| policy_loss        | 0.0065566683 |
| serial_timesteps   | 20800        |
| time_elapsed       | 313          |
| time_remaining     | 1.41         |
| total_timesteps    | 624000       |
| true_eprew         | 147          |
| value_loss         | 115.610634   |
-------------------------------------
Current reward shaping 0.376
Current self-play randomization 0.9504
SP envs: 28/30
Other agent actions took 6.244855880737305 seconds
Total simulation time for 400 steps: 10.422523736953735 	 Other agent action time: 0 	 38.378420629715045 steps/s
Curr learning rate 0.0004747474747474748 	 Curr reward per step 0.48376666666666673

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.55it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 145.97it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 130.90it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.15it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.53it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 169.42it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 159.96it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 165.20it/s]
-------------------------------------
| approxkl           | 0.004066728  |
| clipfrac           | 0.27260414   |
| eplenmean          | 400          |
| eprewmean          | 203          |
| explained_variance | 0.322        |
| fps                | 1094         |
| nupdates           | 53           |
| policy_entropy     | 0.7985036    |
| policy_loss        | 0.0063569536 |
| serial_timesteps   | 21200        |
| time_elapsed       | 324          |
| time_remaining     | 1.33         |
| total_timesteps    | 636000       |
| true_eprew         | 149          |
| value_loss         | 124.50858    |
-------------------------------------
Current reward shaping 0.364
Current self-play randomization 0.9456
SP envs: 27/30
Other agent actions took 6.071566581726074 seconds
Total simulation time for 400 steps: 9.781196117401123 	 Other agent action time: 0 	 40.89479396986884 steps/s
Curr learning rate 0.0004646464646464647 	 Curr reward per step 0.49409200000000003

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 143.07it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 170.56it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.54it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 130.25it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.62it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 153.65it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.25it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 156.60it/s]
-------------------------------------
| approxkl           | 0.0045371666 |
| clipfrac           | 0.36137497   |
| eplenmean          | 400          |
| eprewmean          | 198          |
| explained_variance | 0.287        |
| fps                | 1161         |
| nupdates           | 54           |
| policy_entropy     | 0.7793286    |
| policy_loss        | 0.00829669   |
| serial_timesteps   | 21600        |
| time_elapsed       | 335          |
| time_remaining     | 1.24         |
| total_timesteps    | 648000       |
| true_eprew         | 147          |
| value_loss         | 124.70558    |
-------------------------------------
Current reward shaping 0.352
Current self-play randomization 0.9408
SP envs: 28/30
Other agent actions took 6.096752882003784 seconds
Total simulation time for 400 steps: 9.852410793304443 	 Other agent action time: 0 	 40.5992003776207 steps/s
Curr learning rate 0.00045454545454545455 	 Curr reward per step 0.543256

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 117.17it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 108.33it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 101.76it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 112.20it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 112.72it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 143.90it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 158.55it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 154.44it/s]
-------------------------------------
| approxkl           | 0.0028248245 |
| clipfrac           | 0.2672708    |
| eplenmean          | 400          |
| eprewmean          | 204          |
| explained_variance | 0.243        |
| fps                | 1139         |
| nupdates           | 55           |
| policy_entropy     | 0.743909     |
| policy_loss        | 0.00563941   |
| serial_timesteps   | 22000        |
| time_elapsed       | 345          |
| time_remaining     | 1.15         |
| total_timesteps    | 660000       |
| true_eprew         | 153          |
| value_loss         | 115.43756    |
-------------------------------------
Current reward shaping 0.33999999999999997
Current self-play randomization 0.9359999999999999
SP envs: 27/30
Other agent actions took 6.000326633453369 seconds
Total simulation time for 400 steps: 9.798548936843872 	 Other agent action time: 0 	 40.82237100392955 steps/s
Curr learning rate 0.0004444444444444444 	 Curr reward per step 0.5350116666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.89it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.28it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 151.88it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 163.49it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.81it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.59it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 160.34it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 117.03it/s]
------------------------------------
| approxkl           | 0.004999261 |
| clipfrac           | 0.31351042  |
| eplenmean          | 400         |
| eprewmean          | 209         |
| explained_variance | 0.332       |
| fps                | 1160        |
| nupdates           | 56          |
| policy_entropy     | 0.7527563   |
| policy_loss        | 0.008420723 |
| serial_timesteps   | 22400       |
| time_elapsed       | 356         |
| time_remaining     | 1.06        |
| total_timesteps    | 672000      |
| true_eprew         | 158         |
| value_loss         | 111.15605   |
------------------------------------
Current reward shaping 0.32799999999999996
Current self-play randomization 0.9312
SP envs: 27/30
Other agent actions took 6.153593301773071 seconds
Total simulation time for 400 steps: 9.989553213119507 	 Other agent action time: 0 	 40.04183084731667 steps/s
Curr learning rate 0.00043434343434343433 	 Curr reward per step 0.5547500000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 145.90it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 138.62it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 138.01it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 143.67it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 158.26it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 126.51it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 135.76it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 139.22it/s]
-------------------------------------
| approxkl           | 0.0027683969 |
| clipfrac           | 0.24096873   |
| eplenmean          | 400          |
| eprewmean          | 216          |
| explained_variance | 0.275        |
| fps                | 1132         |
| nupdates           | 57           |
| policy_entropy     | 0.70323575   |
| policy_loss        | 0.0047279354 |
| serial_timesteps   | 22800        |
| time_elapsed       | 366          |
| time_remaining     | 0.964        |
| total_timesteps    | 684000       |
| true_eprew         | 165          |
| value_loss         | 105.3405     |
-------------------------------------
Current reward shaping 0.31599999999999995
Current self-play randomization 0.9264
SP envs: 29/30
Other agent actions took 6.05483865737915 seconds
Total simulation time for 400 steps: 9.88590121269226 	 Other agent action time: 0 	 40.461662664244514 steps/s
Curr learning rate 0.00042424242424242425 	 Curr reward per step 0.6052029999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.38it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 165.12it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 160.47it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 153.26it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 162.86it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 136.17it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 160.48it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 144.35it/s]
-------------------------------------
| approxkl           | 0.002043488  |
| clipfrac           | 0.21762499   |
| eplenmean          | 400          |
| eprewmean          | 228          |
| explained_variance | 0.29         |
| fps                | 1149         |
| nupdates           | 58           |
| policy_entropy     | 0.69369787   |
| policy_loss        | 0.0023845588 |
| serial_timesteps   | 23200        |
| time_elapsed       | 377          |
| time_remaining     | 0.866        |
| total_timesteps    | 696000       |
| true_eprew         | 175          |
| value_loss         | 89.665596    |
-------------------------------------
Current reward shaping 0.30400000000000005
Current self-play randomization 0.9216
SP envs: 28/30
Other agent actions took 6.022384166717529 seconds
Total simulation time for 400 steps: 9.894546508789062 	 Other agent action time: 0 	 40.42630954785958 steps/s
Curr learning rate 0.0004141414141414141 	 Curr reward per step 0.577644

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 154.72it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.23it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.67it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 157.31it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 156.79it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 157.41it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 156.70it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 149.29it/s]
-------------------------------------
| approxkl           | 0.0008970398 |
| clipfrac           | 0.145125     |
| eplenmean          | 400          |
| eprewmean          | 229          |
| explained_variance | 0.234        |
| fps                | 1150         |
| nupdates           | 59           |
| policy_entropy     | 0.710683     |
| policy_loss        | 0.0011101005 |
| serial_timesteps   | 23600        |
| time_elapsed       | 387          |
| time_remaining     | 0.765        |
| total_timesteps    | 708000       |
| true_eprew         | 178          |
| value_loss         | 101.386635   |
-------------------------------------
Current reward shaping 0.29200000000000004
Current self-play randomization 0.9168000000000001
SP envs: 30/30
Other agent actions took 0.8360230922698975 seconds
Total simulation time for 400 steps: 4.712164878845215 	 Other agent action time: 0 	 84.88667317133985 steps/s
Curr learning rate 0.00040404040404040404 	 Curr reward per step 0.5988323333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.76it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 143.32it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 143.45it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 156.93it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.63it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 153.78it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 157.25it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 152.37it/s]
-------------------------------------
| approxkl           | 0.0014914696 |
| clipfrac           | 0.19547914   |
| eplenmean          | 400          |
| eprewmean          | 238          |
| explained_variance | 0.337        |
| fps                | 2278         |
| nupdates           | 60           |
| policy_entropy     | 0.7088057    |
| policy_loss        | 0.002610716  |
| serial_timesteps   | 24000        |
| time_elapsed       | 392          |
| time_remaining     | 0.654        |
| total_timesteps    | 720000       |
| true_eprew         | 186          |
| value_loss         | 89.053566    |
-------------------------------------
Current reward shaping 0.28
Current self-play randomization 0.912
SP envs: 26/30
Other agent actions took 6.130849123001099 seconds
Total simulation time for 400 steps: 10.067901849746704 	 Other agent action time: 0 	 39.73022442705513 steps/s
Curr learning rate 0.00039393939393939396 	 Curr reward per step 0.5421733333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.82it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 152.96it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 154.44it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 159.35it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 145.50it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 160.02it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.67it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 142.46it/s]
-------------------------------------
| approxkl           | 0.0028941329 |
| clipfrac           | 0.24105212   |
| eplenmean          | 400          |
| eprewmean          | 231          |
| explained_variance | 0.209        |
| fps                | 1130         |
| nupdates           | 61           |
| policy_entropy     | 0.70588815   |
| policy_loss        | 0.0047725523 |
| serial_timesteps   | 24400        |
| time_elapsed       | 403          |
| time_remaining     | 0.55         |
| total_timesteps    | 732000       |
| true_eprew         | 183          |
| value_loss         | 115.55104    |
-------------------------------------
Current reward shaping 0.268
Current self-play randomization 0.9072
SP envs: 28/30
Other agent actions took 5.9747679233551025 seconds
Total simulation time for 400 steps: 9.738964557647705 	 Other agent action time: 0 	 41.072128113033585 steps/s
Curr learning rate 0.0003838383838383839 	 Curr reward per step 0.5489699999999998

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.93it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 153.62it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 150.96it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.91it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 153.52it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 133.02it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 156.43it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.32it/s]
-------------------------------------
| approxkl           | 0.0013940484 |
| clipfrac           | 0.1875625    |
| eplenmean          | 400          |
| eprewmean          | 228          |
| explained_variance | 0.324        |
| fps                | 1166         |
| nupdates           | 62           |
| policy_entropy     | 0.7349261    |
| policy_loss        | 0.0012927791 |
| serial_timesteps   | 24800        |
| time_elapsed       | 413          |
| time_remaining     | 0.444        |
| total_timesteps    | 744000       |
| true_eprew         | 182          |
| value_loss         | 96.369095    |
-------------------------------------
Current reward shaping 0.256
Current self-play randomization 0.9024
SP envs: 28/30
Other agent actions took 6.020804405212402 seconds
Total simulation time for 400 steps: 9.81584358215332 	 Other agent action time: 0 	 40.75044560889908 steps/s
Curr learning rate 0.0003737373737373737 	 Curr reward per step 0.5749120000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.27it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.66it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 167.73it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 146.53it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 166.02it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 148.94it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.79it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 154.15it/s]
-------------------------------------
| approxkl           | 0.0023539937 |
| clipfrac           | 0.22603121   |
| eplenmean          | 400          |
| eprewmean          | 224          |
| explained_variance | 0.324        |
| fps                | 1159         |
| nupdates           | 63           |
| policy_entropy     | 0.68175334   |
| policy_loss        | 0.004603383  |
| serial_timesteps   | 25200        |
| time_elapsed       | 424          |
| time_remaining     | 0.336        |
| total_timesteps    | 756000       |
| true_eprew         | 180          |
| value_loss         | 94.16719     |
-------------------------------------
Current reward shaping 0.244
Current self-play randomization 0.8976
SP envs: 30/30
Other agent actions took 0.7714698314666748 seconds
Total simulation time for 400 steps: 4.473322868347168 	 Other agent action time: 0 	 89.41898713154916 steps/s
Curr learning rate 0.0003636363636363636 	 Curr reward per step 0.6131129999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 150.41it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 157.03it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.05it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 163.88it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.82it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 167.74it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 153.83it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 158.05it/s]
---------------------------------------
| approxkl           | 0.00072512426  |
| clipfrac           | 0.13076045     |
| eplenmean          | 400            |
| eprewmean          | 228            |
| explained_variance | 0.222          |
| fps                | 2396           |
| nupdates           | 64             |
| policy_entropy     | 0.74327415     |
| policy_loss        | -9.7362536e-05 |
| serial_timesteps   | 25600          |
| time_elapsed       | 429            |
| time_remaining     | 0.223          |
| total_timesteps    | 768000         |
| true_eprew         | 185            |
| value_loss         | 86.40257       |
---------------------------------------
Current reward shaping 0.23199999999999998
Current self-play randomization 0.8928
SP envs: 26/30
Other agent actions took 6.212367534637451 seconds
Total simulation time for 400 steps: 10.262480020523071 	 Other agent action time: 0 	 38.97693337283713 steps/s
Curr learning rate 0.00035353535353535354 	 Curr reward per step 0.5002173333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.08it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 160.50it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 164.23it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.63it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 155.41it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 155.17it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 158.06it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 151.86it/s]
------------------------------------
| approxkl           | 0.001704799 |
| clipfrac           | 0.1743646   |
| eplenmean          | 400         |
| eprewmean          | 226         |
| explained_variance | 0.335       |
| fps                | 1110        |
| nupdates           | 65          |
| policy_entropy     | 0.69616646  |
| policy_loss        | 0.002283867 |
| serial_timesteps   | 26000       |
| time_elapsed       | 439         |
| time_remaining     | 0.113       |
| total_timesteps    | 780000      |
| true_eprew         | 185         |
| value_loss         | 112.4271    |
------------------------------------
Current reward shaping 0.21999999999999997
Current self-play randomization 0.888
SP envs: 27/30
Other agent actions took 6.0334367752075195 seconds
Total simulation time for 400 steps: 9.826504468917847 	 Other agent action time: 0 	 40.70623498572024 steps/s
Curr learning rate 0.0003434343434343434 	 Curr reward per step 0.4987283333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 134.32it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 161.27it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.16it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.03it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.74it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 159.08it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 161.91it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 133.52it/s]
-------------------------------------
| approxkl           | 0.0019535017 |
| clipfrac           | 0.22163539   |
| eplenmean          | 400          |
| eprewmean          | 215          |
| explained_variance | 0.315        |
| fps                | 1155         |
| nupdates           | 66           |
| policy_entropy     | 0.7227709    |
| policy_loss        | 0.0031678707 |
| serial_timesteps   | 26400        |
| time_elapsed       | 450          |
| time_remaining     | 0            |
| total_timesteps    | 792000       |
| true_eprew         | 178          |
| value_loss         | 112.55271    |
-------------------------------------
Current reward shaping 0.20799999999999996
Current self-play randomization 0.8832
LOADING BC MODEL FROM: seed3/worker3
Loading a model without an environment, this model cannot be trained until it has a valid environment.
Loaded MediumLevelPlanner from /home/ubuntu/human_aware_rl/overcooked_ai/overcooked_ai_py/data/planners/simple_am.pkl
TOT NUM UPDATES 66
SP envs: 26/30
Other agent actions took 6.029216051101685 seconds
Total simulation time for 400 steps: 9.842271327972412 	 Other agent action time: 0 	 40.64102549816651 steps/s
Curr learning rate 0.001 	 Curr reward per step 0.5262386666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 148.77it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 154.56it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 159.35it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.16it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 159.86it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 150.72it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 160.70it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 156.47it/s]
------------------------------------
| approxkl           | 0.023676645 |
| clipfrac           | 0.44249988  |
| eplenmean          | 400         |
| eprewmean          | 210         |
| explained_variance | 0.314       |
| fps                | 1154        |
| nupdates           | 1           |
| policy_entropy     | 0.6497589   |
| policy_loss        | 0.026827043 |
| serial_timesteps   | 400         |
| time_elapsed       | 10.4        |
| time_remaining     | 11.3        |
| total_timesteps    | 12000       |
| true_eprew         | 177         |
| value_loss         | 102.90285   |
------------------------------------
Current reward shaping 0.988
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7761068344116211 seconds
Total simulation time for 400 steps: 4.55118727684021 	 Other agent action time: 0 	 87.88915411929858 steps/s
Curr learning rate 0.00098989898989899 	 Curr reward per step 0.8421760000000004

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 152.16it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.48it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 164.14it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 149.11it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 132.20it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 140.87it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 165.16it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.93it/s]
------------------------------------
| approxkl           | 0.007469046 |
| clipfrac           | 0.33906248  |
| eplenmean          | 400         |
| eprewmean          | 274         |
| explained_variance | 0.314       |
| fps                | 2346        |
| nupdates           | 2           |
| policy_entropy     | 0.6820798   |
| policy_loss        | 0.013508538 |
| serial_timesteps   | 800         |
| time_elapsed       | 15.5        |
| time_remaining     | 8.27        |
| total_timesteps    | 24000       |
| true_eprew         | 178         |
| value_loss         | 126.509476  |
------------------------------------
Current reward shaping 0.976
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8175344467163086 seconds
Total simulation time for 400 steps: 4.500581741333008 	 Other agent action time: 0 	 88.87739918740499 steps/s
Curr learning rate 0.0009797979797979799 	 Curr reward per step 0.8911826666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 158.38it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 161.17it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 160.86it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 162.66it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.11it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 159.67it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 163.60it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 167.40it/s]
------------------------------------
| approxkl           | 0.005943063 |
| clipfrac           | 0.31023955  |
| eplenmean          | 400         |
| eprewmean          | 301         |
| explained_variance | 0.301       |
| fps                | 2388        |
| nupdates           | 3           |
| policy_entropy     | 0.6587226   |
| policy_loss        | 0.010252143 |
| serial_timesteps   | 1200        |
| time_elapsed       | 20.5        |
| time_remaining     | 7.19        |
| total_timesteps    | 36000       |
| true_eprew         | 182         |
| value_loss         | 123.59792   |
------------------------------------
Current reward shaping 0.964
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7692456245422363 seconds
Total simulation time for 400 steps: 4.562640905380249 	 Other agent action time: 0 	 87.66852537711691 steps/s
Curr learning rate 0.0009696969696969698 	 Curr reward per step 0.8334716666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.44it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 147.40it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 159.06it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 158.85it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 152.14it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 159.90it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 157.78it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 148.67it/s]
-------------------------------------
| approxkl           | 0.0048568537 |
| clipfrac           | 0.29813534   |
| eplenmean          | 400          |
| eprewmean          | 329          |
| explained_variance | 0.393        |
| fps                | 2348         |
| nupdates           | 4            |
| policy_entropy     | 0.6940482    |
| policy_loss        | 0.009425222  |
| serial_timesteps   | 1600         |
| time_elapsed       | 25.6         |
| time_remaining     | 6.62         |
| total_timesteps    | 48000        |
| true_eprew         | 182          |
| value_loss         | 126.41016    |
-------------------------------------
Current reward shaping 0.952
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.817232608795166 seconds
Total simulation time for 400 steps: 4.586530447006226 	 Other agent action time: 0 	 87.21189243627342 steps/s
Curr learning rate 0.0009595959595959597 	 Curr reward per step 0.7933453333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 154.95it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 160.76it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 143.91it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.72it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.73it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 158.52it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 159.83it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 128.64it/s]
------------------------------------
| approxkl           | 0.008529413 |
| clipfrac           | 0.3745625   |
| eplenmean          | 400         |
| eprewmean          | 338         |
| explained_variance | 0.363       |
| fps                | 2337        |
| nupdates           | 5           |
| policy_entropy     | 0.69453514  |
| policy_loss        | 0.015544454 |
| serial_timesteps   | 2000        |
| time_elapsed       | 30.8        |
| time_remaining     | 6.26        |
| total_timesteps    | 60000       |
| true_eprew         | 181         |
| value_loss         | 118.99016   |
------------------------------------
Current reward shaping 0.94
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7450463771820068 seconds
Total simulation time for 400 steps: 4.438851833343506 	 Other agent action time: 0 	 90.11339306153532 steps/s
Curr learning rate 0.0009494949494949496 	 Curr reward per step 0.796935

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.83it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.91it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 155.84it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.11it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.88it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 145.57it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 160.84it/s]
-------------------------------------
| approxkl           | 0.0064113424 |
| clipfrac           | 0.36997917   |
| eplenmean          | 400          |
| eprewmean          | 324          |
| explained_variance | 0.4          |
| fps                | 2416         |
| nupdates           | 6            |
| policy_entropy     | 0.75056285   |
| policy_loss        | 0.012895271  |
| serial_timesteps   | 2400         |
| time_elapsed       | 35.7         |
| time_remaining     | 5.96         |
| total_timesteps    | 72000        |
| true_eprew         | 174          |
| value_loss         | 126.53312    |
-------------------------------------
Current reward shaping 0.928
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7479794025421143 seconds
Total simulation time for 400 steps: 4.549111843109131 	 Other agent action time: 0 	 87.92925164192413 steps/s
Curr learning rate 0.0009393939393939395 	 Curr reward per step 0.8166986666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.58it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 167.54it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 165.09it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.14it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 166.90it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.54it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 160.03it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 162.80it/s]
-------------------------------------
| approxkl           | 0.0043081837 |
| clipfrac           | 0.27304167   |
| eplenmean          | 400          |
| eprewmean          | 325          |
| explained_variance | 0.283        |
| fps                | 2368         |
| nupdates           | 7            |
| policy_entropy     | 0.6482854    |
| policy_loss        | 0.006874443  |
| serial_timesteps   | 2800         |
| time_elapsed       | 40.8         |
| time_remaining     | 5.73         |
| total_timesteps    | 84000        |
| true_eprew         | 175          |
| value_loss         | 111.61295    |
-------------------------------------
Current reward shaping 0.916
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8031933307647705 seconds
Total simulation time for 400 steps: 4.61970329284668 	 Other agent action time: 0 	 86.58564731189011 steps/s
Curr learning rate 0.0009292929292929292 	 Curr reward per step 0.8171886666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 150.16it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 157.60it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.21it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 149.24it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 145.84it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 159.24it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 157.66it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 156.67it/s]
-------------------------------------
| approxkl           | 0.0022541624 |
| clipfrac           | 0.23227093   |
| eplenmean          | 400          |
| eprewmean          | 325          |
| explained_variance | 0.37         |
| fps                | 2323         |
| nupdates           | 8            |
| policy_entropy     | 0.71730584   |
| policy_loss        | 0.003786678  |
| serial_timesteps   | 3200         |
| time_elapsed       | 46           |
| time_remaining     | 5.56         |
| total_timesteps    | 96000        |
| true_eprew         | 176          |
| value_loss         | 120.03662    |
-------------------------------------
Current reward shaping 0.904
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7501919269561768 seconds
Total simulation time for 400 steps: 4.553729057312012 	 Other agent action time: 0 	 87.84009653752065 steps/s
Curr learning rate 0.0009191919191919192 	 Curr reward per step 0.8028240000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 136.40it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 140.86it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.74it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 159.87it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 154.15it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 157.72it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.05it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 158.69it/s]
-------------------------------------
| approxkl           | 0.0046121962 |
| clipfrac           | 0.2968853    |
| eplenmean          | 400          |
| eprewmean          | 323          |
| explained_variance | 0.421        |
| fps                | 2352         |
| nupdates           | 9            |
| policy_entropy     | 0.7118217    |
| policy_loss        | 0.00893504   |
| serial_timesteps   | 3600         |
| time_elapsed       | 51.1         |
| time_remaining     | 5.39         |
| total_timesteps    | 108000       |
| true_eprew         | 177          |
| value_loss         | 115.16805    |
-------------------------------------
Current reward shaping 0.892
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7994093894958496 seconds
Total simulation time for 400 steps: 4.530322313308716 	 Other agent action time: 0 	 88.29393856258771 steps/s
Curr learning rate 0.0009090909090909091 	 Curr reward per step 0.8025433333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 155.48it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 147.23it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.99it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.90it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.71it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 146.44it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 164.62it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 154.78it/s]
-------------------------------------
| approxkl           | 0.005525095  |
| clipfrac           | 0.30878127   |
| eplenmean          | 400          |
| eprewmean          | 325          |
| explained_variance | 0.395        |
| fps                | 2370         |
| nupdates           | 10           |
| policy_entropy     | 0.75915015   |
| policy_loss        | 0.0074395067 |
| serial_timesteps   | 4000         |
| time_elapsed       | 56.1         |
| time_remaining     | 5.24         |
| total_timesteps    | 120000       |
| true_eprew         | 179          |
| value_loss         | 109.66085    |
-------------------------------------
Current reward shaping 0.88
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7600946426391602 seconds
Total simulation time for 400 steps: 4.469129800796509 	 Other agent action time: 0 	 89.50288262576535 steps/s
Curr learning rate 0.000898989898989899 	 Curr reward per step 0.7935533333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 165.02it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 161.03it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 160.95it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.17it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.82it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 159.12it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.48it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 133.14it/s]
-------------------------------------
| approxkl           | 0.0038931712 |
| clipfrac           | 0.31061453   |
| eplenmean          | 400          |
| eprewmean          | 320          |
| explained_variance | 0.337        |
| fps                | 2400         |
| nupdates           | 11           |
| policy_entropy     | 0.7672024    |
| policy_loss        | 0.0068800636 |
| serial_timesteps   | 4400         |
| time_elapsed       | 61.1         |
| time_remaining     | 5.1          |
| total_timesteps    | 132000       |
| true_eprew         | 178          |
| value_loss         | 113.87007    |
-------------------------------------
Current reward shaping 0.868
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7614412307739258 seconds
Total simulation time for 400 steps: 4.470651626586914 	 Other agent action time: 0 	 89.47241552466414 steps/s
Curr learning rate 0.0008888888888888889 	 Curr reward per step 0.8064646666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 147.33it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 165.93it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.78it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 151.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.79it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 121.26it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 113.53it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 113.67it/s]
-------------------------------------
| approxkl           | 0.0059362296 |
| clipfrac           | 0.32932296   |
| eplenmean          | 400          |
| eprewmean          | 317          |
| explained_variance | 0.378        |
| fps                | 2364         |
| nupdates           | 12           |
| policy_entropy     | 0.6913741    |
| policy_loss        | 0.011273914  |
| serial_timesteps   | 4800         |
| time_elapsed       | 66.2         |
| time_remaining     | 4.97         |
| total_timesteps    | 144000       |
| true_eprew         | 177          |
| value_loss         | 107.35032    |
-------------------------------------
Current reward shaping 0.856
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8008160591125488 seconds
Total simulation time for 400 steps: 4.525958299636841 	 Other agent action time: 0 	 88.3790732301037 steps/s
Curr learning rate 0.0008787878787878789 	 Curr reward per step 0.7778346666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 164.42it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 153.42it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 163.48it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 133.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.83it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 157.56it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.19it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.12it/s]
------------------------------------
| approxkl           | 0.006643585 |
| clipfrac           | 0.35384378  |
| eplenmean          | 400         |
| eprewmean          | 319         |
| explained_variance | 0.423       |
| fps                | 2370        |
| nupdates           | 13          |
| policy_entropy     | 0.72278893  |
| policy_loss        | 0.013183226 |
| serial_timesteps   | 5200        |
| time_elapsed       | 71.3        |
| time_remaining     | 4.84        |
| total_timesteps    | 156000      |
| true_eprew         | 179         |
| value_loss         | 103.01412   |
------------------------------------
Current reward shaping 0.844
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7473204135894775 seconds
Total simulation time for 400 steps: 4.417006492614746 	 Other agent action time: 0 	 90.55906996487366 steps/s
Curr learning rate 0.0008686868686868688 	 Curr reward per step 0.8178623333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 155.85it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 160.13it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.52it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 170.36it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 154.96it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 148.54it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.00it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 147.38it/s]
------------------------------------
| approxkl           | 0.002006811 |
| clipfrac           | 0.22652085  |
| eplenmean          | 400         |
| eprewmean          | 322         |
| explained_variance | 0.29        |
| fps                | 2423        |
| nupdates           | 14          |
| policy_entropy     | 0.73110837  |
| policy_loss        | 0.003096053 |
| serial_timesteps   | 5600        |
| time_elapsed       | 76.2        |
| time_remaining     | 4.72        |
| total_timesteps    | 168000      |
| true_eprew         | 182         |
| value_loss         | 110.72865   |
------------------------------------
Current reward shaping 0.832
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7963154315948486 seconds
Total simulation time for 400 steps: 4.582711935043335 	 Other agent action time: 0 	 87.28456112225992 steps/s
Curr learning rate 0.0008585858585858587 	 Curr reward per step 0.8637146666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 155.32it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.70it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 171.59it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 157.17it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.52it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.47it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 167.81it/s]
-------------------------------------
| approxkl           | 0.0031967615 |
| clipfrac           | 0.23305206   |
| eplenmean          | 400          |
| eprewmean          | 328          |
| explained_variance | 0.225        |
| fps                | 2353         |
| nupdates           | 15           |
| policy_entropy     | 0.62335354   |
| policy_loss        | 0.005346668  |
| serial_timesteps   | 6000         |
| time_elapsed       | 81.3         |
| time_remaining     | 4.61         |
| total_timesteps    | 180000       |
| true_eprew         | 186          |
| value_loss         | 104.992355   |
-------------------------------------
Current reward shaping 0.8200000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7745771408081055 seconds
Total simulation time for 400 steps: 4.432617425918579 	 Other agent action time: 0 	 90.24013614644564 steps/s
Curr learning rate 0.0008484848484848486 	 Curr reward per step 0.8668399999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 150.02it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.20it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 164.64it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 127.15it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 159.66it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 162.49it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 151.62it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 152.86it/s]
-----------------------------------
| approxkl           | 0.00908714 |
| clipfrac           | 0.3067604  |
| eplenmean          | 400        |
| eprewmean          | 339        |
| explained_variance | 0.235      |
| fps                | 2407       |
| nupdates           | 16         |
| policy_entropy     | 0.62173456 |
| policy_loss        | 0.01190143 |
| serial_timesteps   | 6400       |
| time_elapsed       | 86.3       |
| time_remaining     | 4.5        |
| total_timesteps    | 192000     |
| true_eprew         | 194        |
| value_loss         | 102.54668  |
-----------------------------------
Current reward shaping 0.808
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7877519130706787 seconds
Total simulation time for 400 steps: 4.564852714538574 	 Other agent action time: 0 	 87.62604732591748 steps/s
Curr learning rate 0.0008383838383838385 	 Curr reward per step 0.7916166666666669

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 104.88it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 155.45it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 155.66it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.03it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 146.82it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 142.72it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 168.27it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 142.03it/s]
------------------------------------
| approxkl           | 0.004434633 |
| clipfrac           | 0.29534373  |
| eplenmean          | 400         |
| eprewmean          | 337         |
| explained_variance | 0.316       |
| fps                | 2332        |
| nupdates           | 17          |
| policy_entropy     | 0.67100644  |
| policy_loss        | 0.00796727  |
| serial_timesteps   | 6800        |
| time_elapsed       | 91.5        |
| time_remaining     | 4.39        |
| total_timesteps    | 204000      |
| true_eprew         | 194         |
| value_loss         | 106.08114   |
------------------------------------
Current reward shaping 0.796
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.757878303527832 seconds
Total simulation time for 400 steps: 4.45800256729126 	 Other agent action time: 0 	 89.72628300728978 steps/s
Curr learning rate 0.0008282828282828282 	 Curr reward per step 0.8233496666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.28it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 158.11it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 160.83it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 156.97it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.19it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 157.91it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.52it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 158.79it/s]
------------------------------------
| approxkl           | 0.006154064 |
| clipfrac           | 0.36213538  |
| eplenmean          | 400         |
| eprewmean          | 332         |
| explained_variance | 0.333       |
| fps                | 2407        |
| nupdates           | 18          |
| policy_entropy     | 0.7231851   |
| policy_loss        | 0.009577517 |
| serial_timesteps   | 7200        |
| time_elapsed       | 96.5        |
| time_remaining     | 4.29        |
| total_timesteps    | 216000      |
| true_eprew         | 193         |
| value_loss         | 110.84393   |
------------------------------------
Current reward shaping 0.784
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7677412033081055 seconds
Total simulation time for 400 steps: 4.513184547424316 	 Other agent action time: 0 	 88.62921420491897 steps/s
Curr learning rate 0.0008181818181818183 	 Curr reward per step 0.8481280000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 141.71it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 161.32it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 145.70it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 158.63it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 152.10it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 152.32it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 153.87it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 142.37it/s]
------------------------------------
| approxkl           | 0.016610192 |
| clipfrac           | 0.46029162  |
| eplenmean          | 400         |
| eprewmean          | 330         |
| explained_variance | 0.191       |
| fps                | 2362        |
| nupdates           | 19          |
| policy_entropy     | 0.69197047  |
| policy_loss        | 0.018809967 |
| serial_timesteps   | 7600        |
| time_elapsed       | 102         |
| time_remaining     | 4.19        |
| total_timesteps    | 228000      |
| true_eprew         | 193         |
| value_loss         | 107.995224  |
------------------------------------
Current reward shaping 0.772
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8268485069274902 seconds
Total simulation time for 400 steps: 4.609637975692749 	 Other agent action time: 0 	 86.77471031548565 steps/s
Curr learning rate 0.0008080808080808081 	 Curr reward per step 0.7308869999999998

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 144.17it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 156.46it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 165.41it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.91it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 158.88it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 161.02it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 161.82it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.76it/s]
------------------------------------
| approxkl           | 0.009964255 |
| clipfrac           | 0.43017697  |
| eplenmean          | 400         |
| eprewmean          | 316         |
| explained_variance | 0.501       |
| fps                | 2334        |
| nupdates           | 20          |
| policy_entropy     | 0.7502774   |
| policy_loss        | 0.014415665 |
| serial_timesteps   | 8000        |
| time_elapsed       | 107         |
| time_remaining     | 4.09        |
| total_timesteps    | 240000      |
| true_eprew         | 186         |
| value_loss         | 110.95571   |
------------------------------------
Current reward shaping 0.76
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7818706035614014 seconds
Total simulation time for 400 steps: 4.577276945114136 	 Other agent action time: 0 	 87.38820149979496 steps/s
Curr learning rate 0.000797979797979798 	 Curr reward per step 0.7924499999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.85it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 131.19it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 163.11it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 155.40it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 159.19it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 161.34it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 161.88it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 163.15it/s]
------------------------------------
| approxkl           | 0.004506352 |
| clipfrac           | 0.2991771   |
| eplenmean          | 400         |
| eprewmean          | 317         |
| explained_variance | 0.35        |
| fps                | 2342        |
| nupdates           | 21          |
| policy_entropy     | 0.6579977   |
| policy_loss        | 0.00827992  |
| serial_timesteps   | 8400        |
| time_elapsed       | 112         |
| time_remaining     | 3.99        |
| total_timesteps    | 252000      |
| true_eprew         | 187         |
| value_loss         | 104.06065   |
------------------------------------
Current reward shaping 0.748
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8096678256988525 seconds
Total simulation time for 400 steps: 4.534076452255249 	 Other agent action time: 0 	 88.22083266836844 steps/s
Curr learning rate 0.0007878787878787879 	 Curr reward per step 0.8420363333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.74it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.33it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 165.24it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.03it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 172.01it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 167.72it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 168.59it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 141.87it/s]
-------------------------------------
| approxkl           | 0.003712926  |
| clipfrac           | 0.29874998   |
| eplenmean          | 400          |
| eprewmean          | 319          |
| explained_variance | 0.22         |
| fps                | 2373         |
| nupdates           | 22           |
| policy_entropy     | 0.6976378    |
| policy_loss        | 0.0057812687 |
| serial_timesteps   | 8800         |
| time_elapsed       | 117          |
| time_remaining     | 3.9          |
| total_timesteps    | 264000       |
| true_eprew         | 189          |
| value_loss         | 105.74819    |
-------------------------------------
Current reward shaping 0.736
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7595038414001465 seconds
Total simulation time for 400 steps: 4.479633331298828 	 Other agent action time: 0 	 89.2930225349546 steps/s
Curr learning rate 0.0007777777777777778 	 Curr reward per step 0.8031573333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.98it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.25it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 165.01it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 150.50it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 165.52it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 153.03it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 159.80it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 165.65it/s]
------------------------------------
| approxkl           | 0.009964144 |
| clipfrac           | 0.38761464  |
| eplenmean          | 400         |
| eprewmean          | 323         |
| explained_variance | 0.329       |
| fps                | 2397        |
| nupdates           | 23          |
| policy_entropy     | 0.7123115   |
| policy_loss        | 0.011967336 |
| serial_timesteps   | 9200        |
| time_elapsed       | 122         |
| time_remaining     | 3.8         |
| total_timesteps    | 276000      |
| true_eprew         | 194         |
| value_loss         | 102.12467   |
------------------------------------
Current reward shaping 0.724
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7530417442321777 seconds
Total simulation time for 400 steps: 4.456522703170776 	 Other agent action time: 0 	 89.75607814482882 steps/s
Curr learning rate 0.0007676767676767678 	 Curr reward per step 0.838877

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 143.80it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 158.56it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 155.69it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.66it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.82it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.27it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 165.69it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.74it/s]
-------------------------------------
| approxkl           | 0.0043503693 |
| clipfrac           | 0.2994063    |
| eplenmean          | 400          |
| eprewmean          | 327          |
| explained_variance | 0.213        |
| fps                | 2404         |
| nupdates           | 24           |
| policy_entropy     | 0.70620555   |
| policy_loss        | 0.0071505206 |
| serial_timesteps   | 9600         |
| time_elapsed       | 127          |
| time_remaining     | 3.7          |
| total_timesteps    | 288000       |
| true_eprew         | 197          |
| value_loss         | 93.81247     |
-------------------------------------
Current reward shaping 0.712
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7983684539794922 seconds
Total simulation time for 400 steps: 4.593766689300537 	 Other agent action time: 0 	 87.07451358634528 steps/s
Curr learning rate 0.0007575757575757577 	 Curr reward per step 0.7968020000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 145.66it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 158.74it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.12it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.65it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 170.61it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.30it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.58it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 158.22it/s]
-------------------------------------
| approxkl           | 0.0041523026 |
| clipfrac           | 0.30706248   |
| eplenmean          | 400          |
| eprewmean          | 327          |
| explained_variance | 0.251        |
| fps                | 2346         |
| nupdates           | 25           |
| policy_entropy     | 0.7277831    |
| policy_loss        | 0.005937348  |
| serial_timesteps   | 10000        |
| time_elapsed       | 132          |
| time_remaining     | 3.61         |
| total_timesteps    | 300000       |
| true_eprew         | 199          |
| value_loss         | 104.29649    |
-------------------------------------
Current reward shaping 0.7
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7632248401641846 seconds
Total simulation time for 400 steps: 4.416117429733276 	 Other agent action time: 0 	 90.57730152437524 steps/s
Curr learning rate 0.0007474747474747475 	 Curr reward per step 0.8153499999999998

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 144.19it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 156.13it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 140.77it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 173.98it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.09it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.65it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 130.95it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.62it/s]
-------------------------------------
| approxkl           | 0.0075750463 |
| clipfrac           | 0.40456256   |
| eplenmean          | 400          |
| eprewmean          | 326          |
| explained_variance | 0.164        |
| fps                | 2415         |
| nupdates           | 26           |
| policy_entropy     | 0.72287446   |
| policy_loss        | 0.011333208  |
| serial_timesteps   | 10400        |
| time_elapsed       | 137          |
| time_remaining     | 3.51         |
| total_timesteps    | 312000       |
| true_eprew         | 199          |
| value_loss         | 104.74735    |
-------------------------------------
Current reward shaping 0.688
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7967948913574219 seconds
Total simulation time for 400 steps: 4.592839241027832 	 Other agent action time: 0 	 87.09209685085428 steps/s
Curr learning rate 0.0007373737373737374 	 Curr reward per step 0.7845133333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 163.26it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.62it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 150.77it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 124.85it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 135.79it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 167.87it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.47it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 163.27it/s]
-------------------------------------
| approxkl           | 0.0036173419 |
| clipfrac           | 0.33308333   |
| eplenmean          | 400          |
| eprewmean          | 321          |
| explained_variance | 0.16         |
| fps                | 2331         |
| nupdates           | 27           |
| policy_entropy     | 0.7676703    |
| policy_loss        | 0.005999615  |
| serial_timesteps   | 10800        |
| time_elapsed       | 142          |
| time_remaining     | 3.42         |
| total_timesteps    | 324000       |
| true_eprew         | 197          |
| value_loss         | 99.99051     |
-------------------------------------
Current reward shaping 0.6759999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7528340816497803 seconds
Total simulation time for 400 steps: 4.4590044021606445 	 Other agent action time: 0 	 89.70612359256182 steps/s
Curr learning rate 0.0007272727272727272 	 Curr reward per step 0.7907733333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 147.34it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 157.75it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 141.29it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 125.81it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 132.81it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 159.30it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 150.15it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 166.61it/s]
------------------------------------
| approxkl           | 0.003555355 |
| clipfrac           | 0.26194793  |
| eplenmean          | 400         |
| eprewmean          | 319         |
| explained_variance | 0.177       |
| fps                | 2381        |
| nupdates           | 28          |
| policy_entropy     | 0.67412174  |
| policy_loss        | 0.005868617 |
| serial_timesteps   | 11200       |
| time_elapsed       | 147         |
| time_remaining     | 3.33        |
| total_timesteps    | 336000      |
| true_eprew         | 197         |
| value_loss         | 99.23266    |
------------------------------------
Current reward shaping 0.6639999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8314712047576904 seconds
Total simulation time for 400 steps: 4.65397834777832 	 Other agent action time: 0 	 85.94797184455078 steps/s
Curr learning rate 0.0007171717171717171 	 Curr reward per step 0.7695726666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 113.54it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 115.43it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 155.06it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 163.07it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 166.06it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.31it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.28it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 152.90it/s]
-------------------------------------
| approxkl           | 0.00460763   |
| clipfrac           | 0.2783333    |
| eplenmean          | 400          |
| eprewmean          | 314          |
| explained_variance | 0.197        |
| fps                | 2294         |
| nupdates           | 29           |
| policy_entropy     | 0.67961025   |
| policy_loss        | 0.0072262897 |
| serial_timesteps   | 11600        |
| time_elapsed       | 152          |
| time_remaining     | 3.24         |
| total_timesteps    | 348000       |
| true_eprew         | 196          |
| value_loss         | 97.25974     |
-------------------------------------
Current reward shaping 0.652
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7333767414093018 seconds
Total simulation time for 400 steps: 4.386361598968506 	 Other agent action time: 0 	 91.19175220165701 steps/s
Curr learning rate 0.0007070707070707071 	 Curr reward per step 0.762081

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 151.93it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 149.50it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 156.17it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 158.96it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.94it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 152.28it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 154.97it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 156.10it/s]
-------------------------------------
| approxkl           | 0.002843422  |
| clipfrac           | 0.25750002   |
| eplenmean          | 400          |
| eprewmean          | 310          |
| explained_variance | 0.263        |
| fps                | 2433         |
| nupdates           | 30           |
| policy_entropy     | 0.7081755    |
| policy_loss        | 0.0045934776 |
| serial_timesteps   | 12000        |
| time_elapsed       | 157          |
| time_remaining     | 3.15         |
| total_timesteps    | 360000       |
| true_eprew         | 194          |
| value_loss         | 96.15856     |
-------------------------------------
Current reward shaping 0.64
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7613356113433838 seconds
Total simulation time for 400 steps: 4.578452825546265 	 Other agent action time: 0 	 87.36575765685107 steps/s
Curr learning rate 0.000696969696969697 	 Curr reward per step 0.7681733333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.10it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.48it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.97it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 163.31it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.67it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.57it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 157.53it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 134.04it/s]
------------------------------------
| approxkl           | 0.003708947 |
| clipfrac           | 0.32366663  |
| eplenmean          | 400         |
| eprewmean          | 308         |
| explained_variance | 0.231       |
| fps                | 2348        |
| nupdates           | 31          |
| policy_entropy     | 0.7349971   |
| policy_loss        | 0.007131492 |
| serial_timesteps   | 12400       |
| time_elapsed       | 162         |
| time_remaining     | 3.06        |
| total_timesteps    | 372000      |
| true_eprew         | 194         |
| value_loss         | 93.00782    |
------------------------------------
Current reward shaping 0.628
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.804771900177002 seconds
Total simulation time for 400 steps: 4.529871702194214 	 Other agent action time: 0 	 88.30272164358318 steps/s
Curr learning rate 0.0006868686868686869 	 Curr reward per step 0.7504506666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 143.88it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 144.90it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 156.06it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 139.83it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 135.06it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 149.74it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 158.20it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 153.63it/s]
-------------------------------------
| approxkl           | 0.0026145175 |
| clipfrac           | 0.2844792    |
| eplenmean          | 400          |
| eprewmean          | 305          |
| explained_variance | 0.229        |
| fps                | 2352         |
| nupdates           | 32           |
| policy_entropy     | 0.76147693   |
| policy_loss        | 0.004325279  |
| serial_timesteps   | 12800        |
| time_elapsed       | 168          |
| time_remaining     | 2.97         |
| total_timesteps    | 384000       |
| true_eprew         | 194          |
| value_loss         | 89.80794     |
-------------------------------------
Current reward shaping 0.616
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7350406646728516 seconds
Total simulation time for 400 steps: 4.420970678329468 	 Other agent action time: 0 	 90.47786766845199 steps/s
Curr learning rate 0.0006767676767676768 	 Curr reward per step 0.7324126666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 138.65it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.42it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 167.44it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 162.44it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.37it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 158.48it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.82it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 154.16it/s]
------------------------------------
| approxkl           | 0.003931122 |
| clipfrac           | 0.28191665  |
| eplenmean          | 400         |
| eprewmean          | 302         |
| explained_variance | 0.304       |
| fps                | 2421        |
| nupdates           | 33          |
| policy_entropy     | 0.6751236   |
| policy_loss        | 0.006488559 |
| serial_timesteps   | 13200       |
| time_elapsed       | 172         |
| time_remaining     | 2.87        |
| total_timesteps    | 396000      |
| true_eprew         | 193         |
| value_loss         | 92.94026    |
------------------------------------
Current reward shaping 0.604
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8113980293273926 seconds
Total simulation time for 400 steps: 4.6164610385894775 	 Other agent action time: 0 	 86.6464585439709 steps/s
Curr learning rate 0.0006666666666666668 	 Curr reward per step 0.747494

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 144.15it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 171.49it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.52it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 170.43it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 156.26it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 142.93it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.30it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 161.85it/s]
-------------------------------------
| approxkl           | 0.0035510093 |
| clipfrac           | 0.26261467   |
| eplenmean          | 400          |
| eprewmean          | 299          |
| explained_variance | 0.226        |
| fps                | 2328         |
| nupdates           | 34           |
| policy_entropy     | 0.66929185   |
| policy_loss        | 0.005124879  |
| serial_timesteps   | 13600        |
| time_elapsed       | 178          |
| time_remaining     | 2.79         |
| total_timesteps    | 408000       |
| true_eprew         | 193          |
| value_loss         | 92.72094     |
-------------------------------------
Current reward shaping 0.5920000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7524340152740479 seconds
Total simulation time for 400 steps: 4.479233026504517 	 Other agent action time: 0 	 89.30100256743066 steps/s
Curr learning rate 0.0006565656565656567 	 Curr reward per step 0.7792613333333337

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.54it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 157.48it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 129.51it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.06it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.12it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.07it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 149.89it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 167.87it/s]
-------------------------------------
| approxkl           | 0.0012682814 |
| clipfrac           | 0.18006253   |
| eplenmean          | 400          |
| eprewmean          | 302          |
| explained_variance | 0.184        |
| fps                | 2392         |
| nupdates           | 35           |
| policy_entropy     | 0.67596585   |
| policy_loss        | 0.0011876891 |
| serial_timesteps   | 14000        |
| time_elapsed       | 183          |
| time_remaining     | 2.7          |
| total_timesteps    | 420000       |
| true_eprew         | 196          |
| value_loss         | 91.97176     |
-------------------------------------
Current reward shaping 0.5800000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7674734592437744 seconds
Total simulation time for 400 steps: 4.506891965866089 	 Other agent action time: 0 	 88.75295947395358 steps/s
Curr learning rate 0.0006464646464646465 	 Curr reward per step 0.7437983333333336

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 145.61it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 157.87it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 153.37it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 160.39it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 159.50it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.75it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 158.82it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 165.03it/s]
-------------------------------------
| approxkl           | 0.0037115086 |
| clipfrac           | 0.34077075   |
| eplenmean          | 400          |
| eprewmean          | 299          |
| explained_variance | 0.242        |
| fps                | 2380         |
| nupdates           | 36           |
| policy_entropy     | 0.772784     |
| policy_loss        | 0.006372027  |
| serial_timesteps   | 14400        |
| time_elapsed       | 188          |
| time_remaining     | 2.61         |
| total_timesteps    | 432000       |
| true_eprew         | 196          |
| value_loss         | 90.088394    |
-------------------------------------
Current reward shaping 0.5680000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8156123161315918 seconds
Total simulation time for 400 steps: 4.6147401332855225 	 Other agent action time: 0 	 86.67877029842955 steps/s
Curr learning rate 0.0006363636363636364 	 Curr reward per step 0.7201493333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.88it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 155.66it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 159.95it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 160.48it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.40it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 147.89it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.44it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 165.19it/s]
-------------------------------------
| approxkl           | 0.002482654  |
| clipfrac           | 0.24279168   |
| eplenmean          | 400          |
| eprewmean          | 298          |
| explained_variance | 0.259        |
| fps                | 2330         |
| nupdates           | 37           |
| policy_entropy     | 0.6645097    |
| policy_loss        | 0.0053451145 |
| serial_timesteps   | 14800        |
| time_elapsed       | 193          |
| time_remaining     | 2.52         |
| total_timesteps    | 444000       |
| true_eprew         | 196          |
| value_loss         | 99.50711     |
-------------------------------------
Current reward shaping 0.556
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7780311107635498 seconds
Total simulation time for 400 steps: 4.538748502731323 	 Other agent action time: 0 	 88.13002081064603 steps/s
Curr learning rate 0.0006262626262626263 	 Curr reward per step 0.746725

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 145.96it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 153.78it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 158.28it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.66it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 166.00it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 162.41it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 163.64it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.13it/s]
-------------------------------------
| approxkl           | 0.0016088604 |
| clipfrac           | 0.22646873   |
| eplenmean          | 400          |
| eprewmean          | 296          |
| explained_variance | 0.21         |
| fps                | 2365         |
| nupdates           | 38           |
| policy_entropy     | 0.74441326   |
| policy_loss        | 0.0014879511 |
| serial_timesteps   | 15200        |
| time_elapsed       | 198          |
| time_remaining     | 2.43         |
| total_timesteps    | 456000       |
| true_eprew         | 196          |
| value_loss         | 95.07556     |
-------------------------------------
Current reward shaping 0.544
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8090236186981201 seconds
Total simulation time for 400 steps: 4.543604373931885 	 Other agent action time: 0 	 88.03583390643082 steps/s
Curr learning rate 0.0006161616161616161 	 Curr reward per step 0.7328746666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 163.12it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 165.73it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 160.87it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.08it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 173.12it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 152.80it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 147.48it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 150.11it/s]
-------------------------------------
| approxkl           | 0.0012144649 |
| clipfrac           | 0.1711979    |
| eplenmean          | 400          |
| eprewmean          | 294          |
| explained_variance | 0.348        |
| fps                | 2363         |
| nupdates           | 39           |
| policy_entropy     | 0.6818581    |
| policy_loss        | 0.001045217  |
| serial_timesteps   | 15600        |
| time_elapsed       | 203          |
| time_remaining     | 2.34         |
| total_timesteps    | 468000       |
| true_eprew         | 196          |
| value_loss         | 90.227776    |
-------------------------------------
Current reward shaping 0.532
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.76279616355896 seconds
Total simulation time for 400 steps: 4.521369218826294 	 Other agent action time: 0 	 88.46877585985698 steps/s
Curr learning rate 0.0006060606060606061 	 Curr reward per step 0.7446216666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 165.67it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 141.47it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.57it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 148.69it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 143.22it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 165.29it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 151.04it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 169.16it/s]
-------------------------------------
| approxkl           | 0.0013785098 |
| clipfrac           | 0.21062502   |
| eplenmean          | 400          |
| eprewmean          | 297          |
| explained_variance | 0.241        |
| fps                | 2368         |
| nupdates           | 40           |
| policy_entropy     | 0.7799822    |
| policy_loss        | 0.001428307  |
| serial_timesteps   | 16000        |
| time_elapsed       | 208          |
| time_remaining     | 2.25         |
| total_timesteps    | 480000       |
| true_eprew         | 200          |
| value_loss         | 81.917435    |
-------------------------------------
Current reward shaping 0.52
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7844903469085693 seconds
Total simulation time for 400 steps: 4.542139768600464 	 Other agent action time: 0 	 88.06422091305417 steps/s
Curr learning rate 0.000595959595959596 	 Curr reward per step 0.7310533333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 115.60it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 114.60it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 116.05it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 113.41it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 117.95it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 125.57it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 170.81it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 163.85it/s]
------------------------------------
| approxkl           | 0.00599174  |
| clipfrac           | 0.30920836  |
| eplenmean          | 400         |
| eprewmean          | 294         |
| explained_variance | 0.398       |
| fps                | 2306        |
| nupdates           | 41          |
| policy_entropy     | 0.66840297  |
| policy_loss        | 0.009840914 |
| serial_timesteps   | 16400       |
| time_elapsed       | 213         |
| time_remaining     | 2.17        |
| total_timesteps    | 492000      |
| true_eprew         | 199         |
| value_loss         | 78.95597    |
------------------------------------
Current reward shaping 0.508
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7536494731903076 seconds
Total simulation time for 400 steps: 4.449872732162476 	 Other agent action time: 0 	 89.8902112657983 steps/s
Curr learning rate 0.0005858585858585859 	 Curr reward per step 0.7497963333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.40it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 152.37it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 156.71it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.53it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 146.84it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 137.31it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 158.56it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.57it/s]
------------------------------------
| approxkl           | 0.004626644 |
| clipfrac           | 0.26253128  |
| eplenmean          | 400         |
| eprewmean          | 297         |
| explained_variance | 0.198       |
| fps                | 2398        |
| nupdates           | 42          |
| policy_entropy     | 0.6528071   |
| policy_loss        | 0.006573764 |
| serial_timesteps   | 16800       |
| time_elapsed       | 218         |
| time_remaining     | 2.08        |
| total_timesteps    | 504000      |
| true_eprew         | 203         |
| value_loss         | 81.09338    |
------------------------------------
Current reward shaping 0.496
Current self-play randomization 0.9984
SP envs: 30/30
Other agent actions took 0.758101224899292 seconds
Total simulation time for 400 steps: 4.528926134109497 	 Other agent action time: 0 	 88.32115785404618 steps/s
Curr learning rate 0.0005757575757575758 	 Curr reward per step 0.733324

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 132.55it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 167.55it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.51it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.79it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 172.77it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 170.69it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 141.22it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 133.13it/s]
-------------------------------------
| approxkl           | 0.0022884952 |
| clipfrac           | 0.23096874   |
| eplenmean          | 400          |
| eprewmean          | 296          |
| explained_variance | 0.32         |
| fps                | 2364         |
| nupdates           | 43           |
| policy_entropy     | 0.6786204    |
| policy_loss        | 0.0035999357 |
| serial_timesteps   | 17200        |
| time_elapsed       | 223          |
| time_remaining     | 1.99         |
| total_timesteps    | 516000       |
| true_eprew         | 204          |
| value_loss         | 79.22868     |
-------------------------------------
Current reward shaping 0.484
Current self-play randomization 0.9936
SP envs: 30/30
Other agent actions took 0.8023064136505127 seconds
Total simulation time for 400 steps: 4.5743443965911865 	 Other agent action time: 0 	 87.44422485943146 steps/s
Curr learning rate 0.0005656565656565657 	 Curr reward per step 0.7221676666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.59it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 160.81it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.59it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 138.39it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.17it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 168.79it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 161.92it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.33it/s]
-------------------------------------
| approxkl           | 0.0019900554 |
| clipfrac           | 0.24056256   |
| eplenmean          | 400          |
| eprewmean          | 296          |
| explained_variance | 0.312        |
| fps                | 2349         |
| nupdates           | 44           |
| policy_entropy     | 0.75208414   |
| policy_loss        | 0.0029059628 |
| serial_timesteps   | 17600        |
| time_elapsed       | 228          |
| time_remaining     | 1.9          |
| total_timesteps    | 528000       |
| true_eprew         | 205          |
| value_loss         | 83.693054    |
-------------------------------------
Current reward shaping 0.472
Current self-play randomization 0.9888
SP envs: 30/30
Other agent actions took 0.7549114227294922 seconds
Total simulation time for 400 steps: 4.501406908035278 	 Other agent action time: 0 	 88.86110679884911 steps/s
Curr learning rate 0.0005555555555555557 	 Curr reward per step 0.7273706666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.72it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.07it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 167.48it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.73it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.87it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 146.56it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 146.15it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 150.65it/s]
------------------------------------
| approxkl           | 0.008084868 |
| clipfrac           | 0.30801043  |
| eplenmean          | 400         |
| eprewmean          | 291         |
| explained_variance | 0.172       |
| fps                | 2383        |
| nupdates           | 45          |
| policy_entropy     | 0.65457135  |
| policy_loss        | 0.01158392  |
| serial_timesteps   | 18000       |
| time_elapsed       | 233         |
| time_remaining     | 1.82        |
| total_timesteps    | 540000      |
| true_eprew         | 203         |
| value_loss         | 89.14583    |
------------------------------------
Current reward shaping 0.45999999999999996
Current self-play randomization 0.984
SP envs: 30/30
Other agent actions took 0.8285694122314453 seconds
Total simulation time for 400 steps: 4.589472770690918 	 Other agent action time: 0 	 87.15598065086294 steps/s
Curr learning rate 0.0005454545454545455 	 Curr reward per step 0.7114433333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.81it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 161.04it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 169.35it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 145.76it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 151.68it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 147.21it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.88it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 160.66it/s]
-------------------------------------
| approxkl           | 0.0029620014 |
| clipfrac           | 0.27679163   |
| eplenmean          | 400          |
| eprewmean          | 289          |
| explained_variance | 0.326        |
| fps                | 2340         |
| nupdates           | 46           |
| policy_entropy     | 0.7624223    |
| policy_loss        | 0.0048761503 |
| serial_timesteps   | 18400        |
| time_elapsed       | 239          |
| time_remaining     | 1.73         |
| total_timesteps    | 552000       |
| true_eprew         | 204          |
| value_loss         | 77.51874     |
-------------------------------------
Current reward shaping 0.44799999999999995
Current self-play randomization 0.9792
SP envs: 29/30
Other agent actions took 5.922110319137573 seconds
Total simulation time for 400 steps: 9.71438217163086 	 Other agent action time: 0 	 41.17606173330606 steps/s
Curr learning rate 0.0005353535353535353 	 Curr reward per step 0.7038880000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 111.95it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 114.24it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 147.57it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.29it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.59it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.16it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 170.80it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 166.12it/s]
-------------------------------------
| approxkl           | 0.0013000766 |
| clipfrac           | 0.19933334   |
| eplenmean          | 400          |
| eprewmean          | 287          |
| explained_variance | 0.205        |
| fps                | 1166         |
| nupdates           | 47           |
| policy_entropy     | 0.7700744    |
| policy_loss        | 0.0017202552 |
| serial_timesteps   | 18800        |
| time_elapsed       | 249          |
| time_remaining     | 1.68         |
| total_timesteps    | 564000       |
| true_eprew         | 203          |
| value_loss         | 86.95118     |
-------------------------------------
Current reward shaping 0.43600000000000005
Current self-play randomization 0.9744
SP envs: 28/30
Other agent actions took 6.032492637634277 seconds
Total simulation time for 400 steps: 9.92018437385559 	 Other agent action time: 0 	 40.32183122061627 steps/s
Curr learning rate 0.0005252525252525252 	 Curr reward per step 0.6833930000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 153.89it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 167.99it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 164.60it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 125.18it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 151.29it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 121.64it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 112.79it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 115.36it/s]
-------------------------------------
| approxkl           | 0.0009539969 |
| clipfrac           | 0.14515626   |
| eplenmean          | 400          |
| eprewmean          | 281          |
| explained_variance | 0.376        |
| fps                | 1138         |
| nupdates           | 48           |
| policy_entropy     | 0.6439495    |
| policy_loss        | 0.0012667212 |
| serial_timesteps   | 19200        |
| time_elapsed       | 259          |
| time_remaining     | 1.62         |
| total_timesteps    | 576000       |
| true_eprew         | 201          |
| value_loss         | 84.702324    |
-------------------------------------
Current reward shaping 0.42400000000000004
Current self-play randomization 0.9696
SP envs: 29/30
Other agent actions took 5.9293906688690186 seconds
Total simulation time for 400 steps: 9.705081462860107 	 Other agent action time: 0 	 41.215522149993284 steps/s
Curr learning rate 0.0005151515151515151 	 Curr reward per step 0.7077080000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.77it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 152.01it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 149.91it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.75it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.53it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 162.03it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.33it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.30it/s]
--------------------------------------
| approxkl           | 0.00090064283 |
| clipfrac           | 0.15310416    |
| eplenmean          | 400           |
| eprewmean          | 279           |
| explained_variance | 0.219         |
| fps                | 1172          |
| nupdates           | 49            |
| policy_entropy     | 0.742756      |
| policy_loss        | 0.0008053092  |
| serial_timesteps   | 19600         |
| time_elapsed       | 270           |
| time_remaining     | 1.56          |
| total_timesteps    | 588000        |
| true_eprew         | 201           |
| value_loss         | 81.80408      |
--------------------------------------
Current reward shaping 0.41200000000000003
Current self-play randomization 0.9648
SP envs: 30/30
Other agent actions took 0.8115806579589844 seconds
Total simulation time for 400 steps: 4.519476652145386 	 Other agent action time: 0 	 88.50582286117594 steps/s
Curr learning rate 0.000505050505050505 	 Curr reward per step 0.6760053333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 161.45it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 153.34it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.30it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.44it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.53it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 156.22it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.97it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 167.40it/s]
-------------------------------------
| approxkl           | 0.0016829695 |
| clipfrac           | 0.20195833   |
| eplenmean          | 400          |
| eprewmean          | 274          |
| explained_variance | 0.242        |
| fps                | 2377         |
| nupdates           | 50           |
| policy_entropy     | 0.67191416   |
| policy_loss        | 0.002874916  |
| serial_timesteps   | 20000        |
| time_elapsed       | 275          |
| time_remaining     | 1.47         |
| total_timesteps    | 600000       |
| true_eprew         | 199          |
| value_loss         | 91.8646      |
-------------------------------------
Current reward shaping 0.4
Current self-play randomization 0.96
../../thesis_data/dr_ppo/ppo_bc_train_simple/
PPO agent on index 0:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑1O 
X ←0    X 
X D X S X 


Timestep: 2
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 3
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →0↑1O 
X       X 
X D X S X 


Timestep: 4
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   →0→1O 
X       X 
X D X S X 


Timestep: 5
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ←0  →1O 
X       X 
X D X S X 


Timestep: 6
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ←o  →1O 
X       X 
X D X S X 


Timestep: 7
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →o→1O 
X       X 
X D X S X 


Timestep: 8
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X     ↓1X 
X D X S X 


Timestep: 9
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑0  O 
X     ↓1X 
X D X S X 


Timestep: 10
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →0O 
X     ↓1X 
X D X S X 


Timestep: 11
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X   ←1  X 
X D X S X 


Timestep: 12
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X ←1    X 
X D X S X 


Timestep: 13
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ←1    X 
X D X S X 


Timestep: 14
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑0  O 
X ←1    X 
X D X S X 


Timestep: 15
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →0O 
X ←1    X 
X D X S X 


Timestep: 16
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
X ←1    X 
X D X S X 


Timestep: 17
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ↓1    X 
X D X S X 


Timestep: 18
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o  O 
X ↓1    X 
X D X S X 


Timestep: 19
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X X ø1X X 
O   ↑0  O 
X ↓1    X 
X D X S X 


Timestep: 20
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø2X X 
O     →0O 
X ↓1    X 
X D X S X 


Timestep: 21
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø3X X 
O     →oO 
X ←1    X 
X D X S X 


Timestep: 22
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø4X X 
O       O 
X ←1  ↓oX 
X D X S X 


Timestep: 23
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø5X X 
O     ↑oO 
X ←1    X 
X D X S X 


Timestep: 24
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø6X X 
O     ↑oO 
X ↓1    X 
X D X S X 


Timestep: 25
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 3 
X X ø7X X 
O       O 
X ↓d  ↓oX 
X D X S X 


Timestep: 26
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø8X X 
O       O 
X ↓d  ↓oX 
X D X S X 


Timestep: 27
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø9X X 
O ↑d    O 
X     ↓oX 
X D X S X 


Timestep: 28
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø10X X 
O ↑d    O 
X     ↓oX 
X D X S X 


Timestep: 29
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø11X X 
O ↑d    O 
X   ←o  X 
X D X S X 


Timestep: 30
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø12X X 
O ↑d    O 
X   ←o  X 
X D X S X 


Timestep: 31
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø13X X 
O   →d  O 
X   ←o  X 
X D X S X 


Timestep: 32
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø14X X 
O   →d  O 
X   ↑o  X 
X D X S X 


Timestep: 33
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø15X X 
O   →d  O 
X   ↑o  X 
X D X S X 


Timestep: 34
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø16X X 
O   →d  O 
X   ↑o  X 
X D X S X 


Timestep: 35
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø17X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 36
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø18X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 37
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø19X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 38
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d  O 
X   ↑o  X 
X D X S X 


Timestep: 39
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →dO 
X   ↑o  X 
X D X S X 


Timestep: 40
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o→dO 
X       X 
X D X S X 


Timestep: 41
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o→dO 
X       X 
X D X S X 


Timestep: 42
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o→dO 
X       X 
X D X S X 


Timestep: 43
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o→dO 
X       X 
X D X S X 


Timestep: 44
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ←o  ↑dO 
X       X 
X D X S X 


Timestep: 45
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   →o↑dO 
X       X 
X D X S X 


Timestep: 46
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   →o↑dO 
X       X 
X D X S X 


Timestep: 47
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   →o↑1O 
X       X 
X D X S X 


Timestep: 48
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   →o→1O 
X       X 
X D X S X 


Timestep: 49
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   →o→oO 
X       X 
X D X S X 


Timestep: 50
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O ←o  →oO 
X       X 
X D X S X 


Timestep: 51
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   →o→oO 
X       X 
X D X S X 


Timestep: 52
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O ←o  →oO 
X       X 
X D X S X 


Timestep: 53
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O →o  ←oO 
X       X 
X D X S X 


Timestep: 54
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ←oO 
X ↓o    X 
X D X S X 


Timestep: 55
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ←oO 
X   →o  X 
X D X S X 


Timestep: 56
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ←oO 
X   →o  X 
X D X S X 


Timestep: 57
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ←o  O 
X     →oX 
X D X S X 


Timestep: 58
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑o  O 
X   ←o  X 
X D X S X 


Timestep: 59
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑o  O 
X   ←o  X 
X D X S X 


Timestep: 60
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑o  O 
X   ↓o  X 
X D X S X 


Timestep: 61
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑o  O 
X   ↓0  X 
X D XoS X 


Timestep: 62
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →oO 
X ←0    X 
X D XoS X 


Timestep: 63
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →oO 
X ←0    X 
X D XoS X 


Timestep: 64
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →oO 
X   →0  X 
X D XoS X 


Timestep: 65
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →oO 
X ←0    X 
X D XoS X 


Timestep: 66
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑oO 
X ↓0    X 
X D XoS X 


Timestep: 67
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑oO 
X ↓d    X 
X D XoS X 


Timestep: 68
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     ↑oO 
X   →d  X 
X D XoS X 


Timestep: 69
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑d↑oO 
X       X 
X D XoS X 


Timestep: 70
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑d←oO 
X       X 
X D XoS X 


Timestep: 71
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 5 
X X P XdX 
O   ↑s←oO 
X       X 
X D XoS X 


Timestep: 72
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X P XdX 
O     ←oO 
X   ↓s  X 
X D XoS X 


Timestep: 73
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X P XdX 
O     ←oO 
X   ↑s  X 
X D XoS X 


Timestep: 74
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑s↑oO 
X       X 
X D XoS X 


Timestep: 75
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s↑oO 
X       X 
X D XoS X 


Timestep: 76
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s↑oO 
X       X 
X D XoS X 


Timestep: 77
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s↑oO 
X       X 
X D XoS X 


Timestep: 78
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s←oO 
X       X 
X D XoS X 


Timestep: 79
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P XdX 
O ←s  ←oO 
X       X 
X D XoS X 


Timestep: 80
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s←oO 
X       X 
X D XoS X 


Timestep: 81
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s←oO 
X       X 
X D XoS X 


Timestep: 82
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s↑oO 
X       X 
X D XoS X 


Timestep: 83
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s↑oO 
X       X 
X D XoS X 


Timestep: 84
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s↑oO 
X       X 
X D XoS X 


Timestep: 85
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s↑oO 
X       X 
X D XoS X 


Timestep: 86
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s→oO 
X       X 
X D XoS X 


Timestep: 87
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s←oO 
X       X 
X D XoS X 


Timestep: 88
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ←o  O 
X   ↓s  X 
X D XoS X 


Timestep: 89
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ←o  O 
X     →sX 
X D XoS X 


Timestep: 90
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑o  O 
X     →0Xs
X D XoS X 


Timestep: 91
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 3 
X X ø-XdX 
O   ↑1  O 
X   ←0  Xs
X D XoS X 


Timestep: 92
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   ↑1  O 
X   ↑0  Xs
X D XoS X 


Timestep: 93
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O     →1O 
X   ↑0  Xs
X D XoS X 


Timestep: 94
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O     →oO 
X ←0    Xs
X D XoS X 


Timestep: 95
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   ←o  O 
X ←0    Xs
X D XoS X 


Timestep: 96
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   ↑o  O 
X ←0    Xs
X D XoS X 


Timestep: 97
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   ↑o  O 
X ←0    Xs
X D XoS X 


Timestep: 98
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O   ↑o  O 
X ←0    Xs
X D XoS X 


Timestep: 99
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø-XdX 
O ←o    O 
X ←0    Xs
X D XoS X 


tot rew 80 tot rew shaped 76
PPO agent on index 1:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X ↑0    X 
X D X S X 


Timestep: 2
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X ↑0    X 
X D X S X 


Timestep: 3
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X ↑0    X 
X D X S X 


Timestep: 4
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X ↑0    X 
X D X S X 


Timestep: 5
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   →0  X 
X D X S X 


Timestep: 6
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑1  O 
X   →0  X 
X D X S X 


Timestep: 7
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X   →0  X 
X D X S X 


Timestep: 8
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X   →0  X 
X D X S X 


Timestep: 9
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X ←0    X 
X D X S X 


Timestep: 10
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ↑0↑o  O 
X       X 
X D X S X 


Timestep: 11
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O ↑0↑1  O 
X       X 
X D X S X 


Timestep: 12
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0  →1O 
X       X 
X D X S X 


Timestep: 13
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑0  →oO 
X       X 
X D X S X 


Timestep: 14
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←0←o  O 
X       X 
X D X S X 


Timestep: 15
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←o↑o  O 
X       X 
X D X S X 


Timestep: 16
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø1X X 
O ←o↑1  O 
X       X 
X D X S X 


Timestep: 17
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø2X X 
O ←o  →1O 
X       X 
X D X S X 


Timestep: 18
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø3X X 
O   →o→oO 
X       X 
X D X S X 


Timestep: 19
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø4X X 
O   →o←oO 
X       X 
X D X S X 


Timestep: 20
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø5X X 
O   →o  O 
X     ↓oX 
X D X S X 


Timestep: 21
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø6X X 
O   →o  O 
X   ←o  X 
X D X S X 


Timestep: 22
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø7X X 
O   →o  O 
X   ←o  X 
X D X S X 


Timestep: 23
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø8X X 
O     →oO 
X   ←o  X 
X D X S X 


Timestep: 24
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø9X X 
O     →oO 
X     →oX 
X D X S X 


Timestep: 25
Joint action taken: ('↓', '↓') 	 Reward: 0 + shape * 0 
X X ø10X X 
O     ↓oO 
X     ↓oX 
X D X S X 


Timestep: 26
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø11X X 
O     ↓oO 
X   ←o  X 
X D X S X 


Timestep: 27
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø12X X 
O     ↓oO 
X   ↓o  X 
X D X S X 


Timestep: 28
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø13X X 
O     ↓oO 
X ←o    X 
X D X S X 


Timestep: 29
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X ø14X X 
O     →oO 
X ↓o    X 
X D X S X 


Timestep: 30
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø15X X 
O     →oO 
X ↓o    X 
X D X S X 


Timestep: 31
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø16X X 
O     →oO 
X   →o  X 
X D X S X 


Timestep: 32
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø17X X 
O     →oO 
X   ↓o  X 
X D X S X 


Timestep: 33
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø18X X 
O     →oO 
X   ↓o  X 
X D X S X 


Timestep: 34
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø19X X 
O       O 
X   ↓o↓oX 
X D X S X 


Timestep: 35
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X   ↓o↓oX 
X D X S X 


Timestep: 36
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X     ↓oX 
X D X S X 


Timestep: 37
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ←o    O 
X     ↓oX 
X D X S X 


Timestep: 38
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ←o    O 
X   ←o  X 
X D X S X 


Timestep: 39
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   →o  O 
X   ←o  X 
X D X S X 


Timestep: 40
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
X   ←o  X 
X D X S X 


Timestep: 41
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
X   ←o  X 
X D X S X 


Timestep: 42
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X     ↓oX 
X D X S X 


Timestep: 43
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X     ↓oX 
X D X S X 


Timestep: 44
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X     ↓oX 
X D X S X 


Timestep: 45
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X   ←o  X 
X D X S X 


Timestep: 46
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X ←o    X 
X D X S X 


Timestep: 47
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑o  →oO 
X       X 
X D X S X 


Timestep: 48
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
X ↓o    X 
X D X S X 


Timestep: 49
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
X ←o    X 
X D X S X 


Timestep: 50
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
X   →o  X 
X D X S X 


Timestep: 51
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↓oO 
X   →o  X 
X D X S X 


Timestep: 52
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↓oO 
X   ↓o  X 
X D X S X 


Timestep: 53
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↓oO 
X     →oX 
X D X S X 


Timestep: 54
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↓oO 
X   ←o  X 
X D X S X 


Timestep: 55
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑oO 
X     →oX 
X D X S X 


Timestep: 56
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ←o  O 
X   ←o  X 
X D X S X 


Timestep: 57
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X   ←o  X 
X D X S X 


Timestep: 58
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X   ↓o  X 
X D X S X 


Timestep: 59
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X   ↓o  X 
X D X S X 


Timestep: 60
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X   ↑o  X 
X D X S X 


Timestep: 61
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
X   ↑o  X 
X D X S X 


Timestep: 62
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
X     →oX 
X D X S X 


Timestep: 63
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ←o  O 
X   ←o  X 
X D X S X 


Timestep: 64
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ←o  O 
X   ↑o  X 
X D X S X 


Timestep: 65
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ←o  O 
X   ↓o  X 
X D X S X 


Timestep: 66
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ←o  O 
X   ↓o  X 
X D X S X 


Timestep: 67
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X   ↓o  X 
X D X S X 


Timestep: 68
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X   ↓o  X 
X D X S X 


Timestep: 69
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X   ↓o  X 
X D X S X 


Timestep: 70
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X   ↓1  X 
X D XoS X 


Timestep: 71
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
X   ↓1  X 
X D XoS X 


Timestep: 72
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
X ←1    X 
X D XoS X 


Timestep: 73
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
X ←1    X 
X D XoS X 


Timestep: 74
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
X ↓1    X 
X D XoS X 


Timestep: 75
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
X ↓1    X 
X D XoS X 


Timestep: 76
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ↓1  ↓oX 
X D XoS X 


Timestep: 77
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ↓1  ↓oX 
X D XoS X 


Timestep: 78
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ↓1  ↓oX 
X D XoS X 


Timestep: 79
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø20X X 
O       O 
X ↓d  ↓oX 
X D XoS X 


Timestep: 80
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑oO 
X ↓d    X 
X D XoS X 


Timestep: 81
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑oO 
X   →d  X 
X D XoS X 


Timestep: 82
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d↑oO 
X       X 
X D XoS X 


Timestep: 83
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 5 
X X P X X 
O   ↑s↑oO 
X       X 
X D XoS X 


Timestep: 84
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →sO 
X     ↓oX 
X D XoS X 


Timestep: 85
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↓sO 
X     ↓oX 
X D XoS X 


Timestep: 86
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←s  O 
X     ↓oX 
X D XoS X 


Timestep: 87
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →sO 
X   ←o  X 
X D XoS X 


Timestep: 88
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X P X X 
O       O 
X   ←o↓sX 
X D XoS X 


Timestep: 89
Joint action taken: ('↑', 'interact') 	 Reward: 20 + shape * 0 
X X P X X 
O   ↑o  O 
X     ↓1X 
X D XoS X 


Timestep: 90
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑0  O 
X     ↓1X 
X D XoS X 


Timestep: 91
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →0O 
X     ↓1X 
X D XoS X 


Timestep: 92
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X     ↓1X 
X D XoS X 


Timestep: 93
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X   ←1  X 
X D XoS X 


Timestep: 94
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X   ←1  X 
X D XoS X 


Timestep: 95
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X   ←1  X 
X D XoS X 


Timestep: 96
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X   ↑1  X 
X D XoS X 


Timestep: 97
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X   ↑1  X 
X D XoS X 


Timestep: 98
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X   ↑1  X 
X D XoS X 


Timestep: 99
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X ←1    X 
X D XoS X 


tot rew 140 tot rew shaped 119
../../thesis_data/dr_ppo/ppo_bc_train_simple/
SP envs: 28/30
Other agent actions took 6.088596343994141 seconds
Total simulation time for 400 steps: 9.878948211669922 	 Other agent action time: 0 	 40.490140390399375 steps/s
Curr learning rate 0.000494949494949495 	 Curr reward per step 0.6721666666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 153.75it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 155.03it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 155.77it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 154.19it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 147.52it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 154.10it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 153.90it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 170.19it/s]
-------------------------------------
| approxkl           | 0.0032753902 |
| clipfrac           | 0.28598958   |
| eplenmean          | 400          |
| eprewmean          | 274          |
| explained_variance | 0.297        |
| fps                | 1151         |
| nupdates           | 51           |
| policy_entropy     | 0.7483938    |
| policy_loss        | 0.005350339  |
| serial_timesteps   | 20400        |
| time_elapsed       | 288          |
| time_remaining     | 1.41         |
| total_timesteps    | 612000       |
| true_eprew         | 201          |
| value_loss         | 85.36821     |
-------------------------------------
Current reward shaping 0.388
Current self-play randomization 0.9552
SP envs: 28/30
Other agent actions took 6.049863338470459 seconds
Total simulation time for 400 steps: 9.888420343399048 	 Other agent action time: 0 	 40.451354828076 steps/s
Curr learning rate 0.0004848484848484849 	 Curr reward per step 0.6559816666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 158.51it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.36it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 155.78it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.88it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 165.48it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 166.27it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 137.10it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.31it/s]
------------------------------------
| approxkl           | 0.008941017 |
| clipfrac           | 0.33380207  |
| eplenmean          | 400         |
| eprewmean          | 270         |
| explained_variance | 0.421       |
| fps                | 1151        |
| nupdates           | 52          |
| policy_entropy     | 0.77115124  |
| policy_loss        | 0.009091019 |
| serial_timesteps   | 20800       |
| time_elapsed       | 298         |
| time_remaining     | 1.34        |
| total_timesteps    | 624000      |
| true_eprew         | 199         |
| value_loss         | 82.18328    |
------------------------------------
Current reward shaping 0.376
Current self-play randomization 0.9504
SP envs: 28/30
Other agent actions took 6.032134532928467 seconds
Total simulation time for 400 steps: 9.844830513000488 	 Other agent action time: 0 	 40.63046077550895 steps/s
Curr learning rate 0.0004747474747474748 	 Curr reward per step 0.6282960000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.02it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 158.84it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 155.42it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 142.36it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 159.86it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 169.41it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.92it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 156.24it/s]
------------------------------------
| approxkl           | 0.010953766 |
| clipfrac           | 0.4346978   |
| eplenmean          | 400         |
| eprewmean          | 262         |
| explained_variance | 0.23        |
| fps                | 1156        |
| nupdates           | 53          |
| policy_entropy     | 0.66962016  |
| policy_loss        | 0.01774962  |
| serial_timesteps   | 21200       |
| time_elapsed       | 309         |
| time_remaining     | 1.26        |
| total_timesteps    | 636000      |
| true_eprew         | 195         |
| value_loss         | 95.68751    |
------------------------------------
Current reward shaping 0.364
Current self-play randomization 0.9456
SP envs: 28/30
Other agent actions took 5.974367618560791 seconds
Total simulation time for 400 steps: 9.794738292694092 	 Other agent action time: 0 	 40.838252952440854 steps/s
Curr learning rate 0.0004646464646464647 	 Curr reward per step 0.6154836666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 134.52it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 120.86it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 115.16it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 107.72it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 112.23it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 114.54it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 112.84it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 115.82it/s]
-------------------------------------
| approxkl           | 0.004424502  |
| clipfrac           | 0.34760413   |
| eplenmean          | 400          |
| eprewmean          | 252          |
| explained_variance | 0.261        |
| fps                | 1141         |
| nupdates           | 54           |
| policy_entropy     | 0.7983211    |
| policy_loss        | 0.0077183843 |
| serial_timesteps   | 21600        |
| time_elapsed       | 319          |
| time_remaining     | 1.18         |
| total_timesteps    | 648000       |
| true_eprew         | 188          |
| value_loss         | 92.39549     |
-------------------------------------
Current reward shaping 0.352
Current self-play randomization 0.9408
SP envs: 28/30
Other agent actions took 6.0244834423065186 seconds
Total simulation time for 400 steps: 9.775318622589111 	 Other agent action time: 0 	 40.91938231820572 steps/s
Curr learning rate 0.00045454545454545455 	 Curr reward per step 0.642152

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 144.41it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.86it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.84it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 143.10it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.03it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.24it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 164.57it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 144.98it/s]
-------------------------------------
| approxkl           | 0.0013857917 |
| clipfrac           | 0.20656255   |
| eplenmean          | 400          |
| eprewmean          | 254          |
| explained_variance | 0.257        |
| fps                | 1163         |
| nupdates           | 55           |
| policy_entropy     | 0.75154495   |
| policy_loss        | 0.0018294903 |
| serial_timesteps   | 22000        |
| time_elapsed       | 329          |
| time_remaining     | 1.1          |
| total_timesteps    | 660000       |
| true_eprew         | 191          |
| value_loss         | 84.59434     |
-------------------------------------
Current reward shaping 0.33999999999999997
Current self-play randomization 0.9359999999999999
SP envs: 28/30
Other agent actions took 5.973686218261719 seconds
Total simulation time for 400 steps: 9.69071364402771 	 Other agent action time: 0 	 41.27662984309892 steps/s
Curr learning rate 0.0004444444444444444 	 Curr reward per step 0.6137583333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.07it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.45it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 159.55it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 159.81it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 141.03it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 139.76it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 135.98it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 153.78it/s]
------------------------------------
| approxkl           | 0.004887403 |
| clipfrac           | 0.36141676  |
| eplenmean          | 400         |
| eprewmean          | 251         |
| explained_variance | 0.231       |
| fps                | 1170        |
| nupdates           | 56          |
| policy_entropy     | 0.7628921   |
| policy_loss        | 0.008449579 |
| serial_timesteps   | 22400       |
| time_elapsed       | 340         |
| time_remaining     | 1.01        |
| total_timesteps    | 672000      |
| true_eprew         | 191         |
| value_loss         | 92.90001    |
------------------------------------
Current reward shaping 0.32799999999999996
Current self-play randomization 0.9312
SP envs: 29/30
Other agent actions took 6.074293613433838 seconds
Total simulation time for 400 steps: 9.86100172996521 	 Other agent action time: 0 	 40.56383022269394 steps/s
Curr learning rate 0.00043434343434343433 	 Curr reward per step 0.6217506666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 156.89it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 155.65it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 136.74it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.17it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 162.81it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 148.98it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 141.04it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.03it/s]
-------------------------------------
| approxkl           | 0.004049996  |
| clipfrac           | 0.34794793   |
| eplenmean          | 400          |
| eprewmean          | 249          |
| explained_variance | 0.24         |
| fps                | 1152         |
| nupdates           | 57           |
| policy_entropy     | 0.750615     |
| policy_loss        | 0.0076912045 |
| serial_timesteps   | 22800        |
| time_elapsed       | 350          |
| time_remaining     | 0.921        |
| total_timesteps    | 684000       |
| true_eprew         | 191          |
| value_loss         | 91.28279     |
-------------------------------------
Current reward shaping 0.31599999999999995
Current self-play randomization 0.9264
SP envs: 27/30
Other agent actions took 6.040225028991699 seconds
Total simulation time for 400 steps: 9.796360969543457 	 Other agent action time: 0 	 40.831488472462986 steps/s
Curr learning rate 0.00042424242424242425 	 Curr reward per step 0.5402906666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 164.03it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.04it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 141.15it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 150.40it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 165.70it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 168.17it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 154.69it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 144.74it/s]
------------------------------------
| approxkl           | 0.010971933 |
| clipfrac           | 0.4326563   |
| eplenmean          | 400         |
| eprewmean          | 240         |
| explained_variance | 0.399       |
| fps                | 1160        |
| nupdates           | 58          |
| policy_entropy     | 0.7863292   |
| policy_loss        | 0.015813747 |
| serial_timesteps   | 23200       |
| time_elapsed       | 360         |
| time_remaining     | 0.829       |
| total_timesteps    | 696000      |
| true_eprew         | 185         |
| value_loss         | 101.879     |
------------------------------------
Current reward shaping 0.30400000000000005
Current self-play randomization 0.9216
SP envs: 26/30
Other agent actions took 6.005619525909424 seconds
Total simulation time for 400 steps: 9.744797229766846 	 Other agent action time: 0 	 41.047544712181804 steps/s
Curr learning rate 0.0004141414141414141 	 Curr reward per step 0.4299946666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 156.11it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 149.97it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 143.67it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 158.04it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.50it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 162.77it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 159.17it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 161.43it/s]
------------------------------------
| approxkl           | 0.03865583  |
| clipfrac           | 0.5372813   |
| eplenmean          | 400         |
| eprewmean          | 217         |
| explained_variance | 0.426       |
| fps                | 1166        |
| nupdates           | 59          |
| policy_entropy     | 0.7734136   |
| policy_loss        | 0.033037294 |
| serial_timesteps   | 23600       |
| time_elapsed       | 371         |
| time_remaining     | 0.733       |
| total_timesteps    | 708000      |
| true_eprew         | 168         |
| value_loss         | 120.58763   |
------------------------------------
Current reward shaping 0.29200000000000004
Current self-play randomization 0.9168000000000001
SP envs: 28/30
Other agent actions took 6.059781312942505 seconds
Total simulation time for 400 steps: 9.936887741088867 	 Other agent action time: 0 	 40.25405241784171 steps/s
Curr learning rate 0.00040404040404040404 	 Curr reward per step 0.5574933333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 131.18it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 145.24it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 130.66it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.31it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.58it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 149.71it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 173.68it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 172.44it/s]
-------------------------------------
| approxkl           | 0.0050912346 |
| clipfrac           | 0.3795833    |
| eplenmean          | 400          |
| eprewmean          | 209          |
| explained_variance | 0.255        |
| fps                | 1143         |
| nupdates           | 60           |
| policy_entropy     | 0.76683843   |
| policy_loss        | 0.009074187  |
| serial_timesteps   | 24000        |
| time_elapsed       | 381          |
| time_remaining     | 0.635        |
| total_timesteps    | 720000       |
| true_eprew         | 164          |
| value_loss         | 93.440445    |
-------------------------------------
Current reward shaping 0.28
Current self-play randomization 0.912
SP envs: 28/30
Other agent actions took 6.116045713424683 seconds
Total simulation time for 400 steps: 9.91677212715149 	 Other agent action time: 0 	 40.33570549683455 steps/s
Curr learning rate 0.00039393939393939396 	 Curr reward per step 0.5606366666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 125.53it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.48it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.01it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 150.48it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 151.43it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 167.57it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.98it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 170.34it/s]
-------------------------------------
| approxkl           | 0.001971366  |
| clipfrac           | 0.23882289   |
| eplenmean          | 400          |
| eprewmean          | 209          |
| explained_variance | 0.27         |
| fps                | 1147         |
| nupdates           | 61           |
| policy_entropy     | 0.7290394    |
| policy_loss        | 0.0026790737 |
| serial_timesteps   | 24400        |
| time_elapsed       | 392          |
| time_remaining     | 0.535        |
| total_timesteps    | 732000       |
| true_eprew         | 166          |
| value_loss         | 93.15643     |
-------------------------------------
Current reward shaping 0.268
Current self-play randomization 0.9072
SP envs: 30/30
Other agent actions took 0.7683594226837158 seconds
Total simulation time for 400 steps: 4.520388841629028 	 Other agent action time: 0 	 88.48796287530224 steps/s
Curr learning rate 0.0003838383838383839 	 Curr reward per step 0.5813763333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 158.84it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 155.60it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.77it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.65it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 169.13it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 147.19it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 136.46it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 160.60it/s]
-------------------------------------
| approxkl           | 0.002141921  |
| clipfrac           | 0.27119794   |
| eplenmean          | 400          |
| eprewmean          | 223          |
| explained_variance | 0.21         |
| fps                | 2368         |
| nupdates           | 62           |
| policy_entropy     | 0.731971     |
| policy_loss        | 0.0034666215 |
| serial_timesteps   | 24800        |
| time_elapsed       | 397          |
| time_remaining     | 0.427        |
| total_timesteps    | 744000       |
| true_eprew         | 178          |
| value_loss         | 79.47204     |
-------------------------------------
Current reward shaping 0.256
Current self-play randomization 0.9024
SP envs: 26/30
Other agent actions took 6.141299247741699 seconds
Total simulation time for 400 steps: 9.961893796920776 	 Other agent action time: 0 	 40.15300786720293 steps/s
Curr learning rate 0.0003737373737373737 	 Curr reward per step 0.5196106666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 152.80it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 151.31it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 167.91it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 157.84it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 157.58it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 155.16it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 157.33it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 163.11it/s]
-------------------------------------
| approxkl           | 0.0015056328 |
| clipfrac           | 0.19068749   |
| eplenmean          | 400          |
| eprewmean          | 222          |
| explained_variance | 0.236        |
| fps                | 1142         |
| nupdates           | 63           |
| policy_entropy     | 0.66750306   |
| policy_loss        | 0.0011168297 |
| serial_timesteps   | 25200        |
| time_elapsed       | 407          |
| time_remaining     | 0.323        |
| total_timesteps    | 756000       |
| true_eprew         | 179          |
| value_loss         | 95.48759     |
-------------------------------------
Current reward shaping 0.244
Current self-play randomization 0.8976
SP envs: 27/30
Other agent actions took 6.054646015167236 seconds
Total simulation time for 400 steps: 9.985548496246338 	 Other agent action time: 0 	 40.05788967429919 steps/s
Curr learning rate 0.0003636363636363636 	 Curr reward per step 0.526359

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 150.14it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 140.31it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 150.08it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 151.82it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 172.83it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 161.57it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 154.93it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 170.25it/s]
-------------------------------------
| approxkl           | 0.0030981398 |
| clipfrac           | 0.28240624   |
| eplenmean          | 400          |
| eprewmean          | 217          |
| explained_variance | 0.293        |
| fps                | 1139         |
| nupdates           | 64           |
| policy_entropy     | 0.76180625   |
| policy_loss        | 0.0041344697 |
| serial_timesteps   | 25600        |
| time_elapsed       | 418          |
| time_remaining     | 0.218        |
| total_timesteps    | 768000       |
| true_eprew         | 176          |
| value_loss         | 93.37183     |
-------------------------------------
Current reward shaping 0.23199999999999998
Current self-play randomization 0.8928
SP envs: 26/30
Other agent actions took 6.091789484024048 seconds
Total simulation time for 400 steps: 9.876929759979248 	 Other agent action time: 0 	 40.49841496502051 steps/s
Curr learning rate 0.00035353535353535354 	 Curr reward per step 0.5184266666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 150.53it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 159.95it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.67it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 158.82it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 166.13it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 147.80it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 163.82it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 146.46it/s]
-------------------------------------
| approxkl           | 0.001642956  |
| clipfrac           | 0.21533331   |
| eplenmean          | 400          |
| eprewmean          | 211          |
| explained_variance | 0.286        |
| fps                | 1152         |
| nupdates           | 65           |
| policy_entropy     | 0.6711528    |
| policy_loss        | 0.0021286998 |
| serial_timesteps   | 26000        |
| time_elapsed       | 428          |
| time_remaining     | 0.11         |
| total_timesteps    | 780000       |
| true_eprew         | 173          |
| value_loss         | 97.36464     |
-------------------------------------
Current reward shaping 0.21999999999999997
Current self-play randomization 0.888
SP envs: 28/30
Other agent actions took 6.050845623016357 seconds
Total simulation time for 400 steps: 9.820995330810547 	 Other agent action time: 0 	 40.729069358694744 steps/s
Curr learning rate 0.0003434343434343434 	 Curr reward per step 0.567785

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.25it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.03it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.31it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 132.03it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.21it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 169.75it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 143.46it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 151.35it/s]
-------------------------------------
| approxkl           | 0.0012424241 |
| clipfrac           | 0.17480208   |
| eplenmean          | 400          |
| eprewmean          | 216          |
| explained_variance | 0.253        |
| fps                | 1156         |
| nupdates           | 66           |
| policy_entropy     | 0.67168206   |
| policy_loss        | 0.0013559224 |
| serial_timesteps   | 26400        |
| time_elapsed       | 439          |
| time_remaining     | 0            |
| total_timesteps    | 792000       |
| true_eprew         | 178          |
| value_loss         | 81.23056     |
-------------------------------------
Current reward shaping 0.20799999999999996
Current self-play randomization 0.8832
LOADING BC MODEL FROM: seed1/worker1
Loading a model without an environment, this model cannot be trained until it has a valid environment.
Loaded MediumLevelPlanner from /home/ubuntu/human_aware_rl/overcooked_ai/overcooked_ai_py/data/planners/simple_am.pkl
TOT NUM UPDATES 66
SP envs: 23/30
Other agent actions took 6.0622334480285645 seconds
Total simulation time for 400 steps: 9.830382823944092 	 Other agent action time: 0 	 40.690175262118046 steps/s
Curr learning rate 0.001 	 Curr reward per step 0.46760666666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 141.21it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 153.95it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 160.26it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.22it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 159.07it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 167.57it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 161.67it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.07it/s]
------------------------------------
| approxkl           | 0.030894581 |
| clipfrac           | 0.45011464  |
| eplenmean          | 400         |
| eprewmean          | 187         |
| explained_variance | 0.323       |
| fps                | 1156        |
| nupdates           | 1           |
| policy_entropy     | 0.66925555  |
| policy_loss        | 0.02807134  |
| serial_timesteps   | 400         |
| time_elapsed       | 10.4        |
| time_remaining     | 11.2        |
| total_timesteps    | 12000       |
| true_eprew         | 157         |
| value_loss         | 102.52096   |
------------------------------------
Current reward shaping 0.988
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7926268577575684 seconds
Total simulation time for 400 steps: 4.488352298736572 	 Other agent action time: 0 	 89.11956401296666 steps/s
Curr learning rate 0.00098989898989899 	 Curr reward per step 0.9116090000000002

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 150.17it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 160.56it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 165.29it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 157.35it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 146.93it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 161.81it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 163.31it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 144.09it/s]
------------------------------------
| approxkl           | 0.01015774  |
| clipfrac           | 0.35229164  |
| eplenmean          | 400         |
| eprewmean          | 276         |
| explained_variance | 0.232       |
| fps                | 2383        |
| nupdates           | 2           |
| policy_entropy     | 0.6931343   |
| policy_loss        | 0.013480467 |
| serial_timesteps   | 800         |
| time_elapsed       | 15.4        |
| time_remaining     | 8.22        |
| total_timesteps    | 24000       |
| true_eprew         | 176         |
| value_loss         | 115.54044   |
------------------------------------
Current reward shaping 0.976
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7935123443603516 seconds
Total simulation time for 400 steps: 4.550627946853638 	 Other agent action time: 0 	 87.89995681289768 steps/s
Curr learning rate 0.0009797979797979799 	 Curr reward per step 0.8974439999999998

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 158.90it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 167.59it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 144.91it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 152.98it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 152.44it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 142.54it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 127.45it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 150.10it/s]
------------------------------------
| approxkl           | 0.005871657 |
| clipfrac           | 0.29304165  |
| eplenmean          | 400         |
| eprewmean          | 304         |
| explained_variance | 0.198       |
| fps                | 2345        |
| nupdates           | 3           |
| policy_entropy     | 0.65792286  |
| policy_loss        | 0.008639496 |
| serial_timesteps   | 1200        |
| time_elapsed       | 20.5        |
| time_remaining     | 7.18        |
| total_timesteps    | 36000       |
| true_eprew         | 181         |
| value_loss         | 123.9848    |
------------------------------------
Current reward shaping 0.964
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7727158069610596 seconds
Total simulation time for 400 steps: 4.531231880187988 	 Other agent action time: 0 	 88.27621507275525 steps/s
Curr learning rate 0.0009696969696969698 	 Curr reward per step 0.8481106666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 117.64it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 111.13it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 112.62it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 111.37it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 109.71it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 112.72it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 117.60it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 150.45it/s]
-------------------------------------
| approxkl           | 0.0037246428 |
| clipfrac           | 0.28359377   |
| eplenmean          | 400          |
| eprewmean          | 337          |
| explained_variance | 0.283        |
| fps                | 2287         |
| nupdates           | 4            |
| policy_entropy     | 0.788812     |
| policy_loss        | 0.0067036464 |
| serial_timesteps   | 1600         |
| time_elapsed       | 25.8         |
| time_remaining     | 6.66         |
| total_timesteps    | 48000        |
| true_eprew         | 187          |
| value_loss         | 115.48601    |
-------------------------------------
Current reward shaping 0.952
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7568709850311279 seconds
Total simulation time for 400 steps: 4.474868535995483 	 Other agent action time: 0 	 89.38810085311604 steps/s
Curr learning rate 0.0009595959595959597 	 Curr reward per step 0.8905513333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.62it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 148.43it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 144.69it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 148.94it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 126.60it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 139.35it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 142.46it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 144.87it/s]
------------------------------------
| approxkl           | 0.006625019 |
| clipfrac           | 0.39806244  |
| eplenmean          | 400         |
| eprewmean          | 353         |
| explained_variance | 0.218       |
| fps                | 2366        |
| nupdates           | 5           |
| policy_entropy     | 0.7846241   |
| policy_loss        | 0.010758035 |
| serial_timesteps   | 2000        |
| time_elapsed       | 30.8        |
| time_remaining     | 6.27        |
| total_timesteps    | 60000       |
| true_eprew         | 190         |
| value_loss         | 112.57439   |
------------------------------------
Current reward shaping 0.94
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7750997543334961 seconds
Total simulation time for 400 steps: 4.5501039028167725 	 Other agent action time: 0 	 87.91008041648836 steps/s
Curr learning rate 0.0009494949494949496 	 Curr reward per step 0.8541816666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 144.31it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 150.83it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 150.26it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 150.11it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 126.85it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 160.36it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 150.72it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 159.84it/s]
-------------------------------------
| approxkl           | 0.0026754811 |
| clipfrac           | 0.2440625    |
| eplenmean          | 400          |
| eprewmean          | 349          |
| explained_variance | 0.297        |
| fps                | 2342         |
| nupdates           | 6            |
| policy_entropy     | 0.6593469    |
| policy_loss        | 0.0041699545 |
| serial_timesteps   | 2400         |
| time_elapsed       | 36           |
| time_remaining     | 5.99         |
| total_timesteps    | 72000        |
| true_eprew         | 189          |
| value_loss         | 103.381996   |
-------------------------------------
Current reward shaping 0.928
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7970409393310547 seconds
Total simulation time for 400 steps: 4.589293003082275 	 Other agent action time: 0 	 87.15939464561333 steps/s
Curr learning rate 0.0009393939393939395 	 Curr reward per step 0.888224

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 155.88it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 137.12it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 135.13it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 154.10it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.84it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 161.04it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 157.90it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.35it/s]
------------------------------------
| approxkl           | 0.003167183 |
| clipfrac           | 0.25301045  |
| eplenmean          | 400         |
| eprewmean          | 351         |
| explained_variance | 0.204       |
| fps                | 2333        |
| nupdates           | 7           |
| policy_entropy     | 0.6757245   |
| policy_loss        | 0.005293104 |
| serial_timesteps   | 2800        |
| time_elapsed       | 41.1        |
| time_remaining     | 5.78        |
| total_timesteps    | 84000       |
| true_eprew         | 192         |
| value_loss         | 106.93939   |
------------------------------------
Current reward shaping 0.916
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7628993988037109 seconds
Total simulation time for 400 steps: 4.47707200050354 	 Other agent action time: 0 	 89.34410703133916 steps/s
Curr learning rate 0.0009292929292929292 	 Curr reward per step 0.8723406666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 154.97it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.56it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 149.71it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 158.19it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.57it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 149.53it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 157.82it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 144.26it/s]
------------------------------------
| approxkl           | 0.005197676 |
| clipfrac           | 0.34501037  |
| eplenmean          | 400         |
| eprewmean          | 349         |
| explained_variance | 0.208       |
| fps                | 2390        |
| nupdates           | 8           |
| policy_entropy     | 0.70243704  |
| policy_loss        | 0.00891163  |
| serial_timesteps   | 3200        |
| time_elapsed       | 46.1        |
| time_remaining     | 5.57        |
| total_timesteps    | 96000       |
| true_eprew         | 192         |
| value_loss         | 108.31187   |
------------------------------------
Current reward shaping 0.904
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8031313419342041 seconds
Total simulation time for 400 steps: 4.590858459472656 	 Other agent action time: 0 	 87.12967379219687 steps/s
Curr learning rate 0.0009191919191919192 	 Curr reward per step 0.8794073333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 134.60it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 157.61it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 150.55it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 148.47it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 153.04it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 154.40it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 159.08it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 145.03it/s]
-------------------------------------
| approxkl           | 0.0037742674 |
| clipfrac           | 0.28143752   |
| eplenmean          | 400          |
| eprewmean          | 352          |
| explained_variance | 0.198        |
| fps                | 2329         |
| nupdates           | 9            |
| policy_entropy     | 0.6696526    |
| policy_loss        | 0.006594504  |
| serial_timesteps   | 3600         |
| time_elapsed       | 51.3         |
| time_remaining     | 5.41         |
| total_timesteps    | 108000       |
| true_eprew         | 195          |
| value_loss         | 118.7422     |
-------------------------------------
Current reward shaping 0.892
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7638373374938965 seconds
Total simulation time for 400 steps: 4.530635356903076 	 Other agent action time: 0 	 88.28783790567968 steps/s
Curr learning rate 0.0009090909090909091 	 Curr reward per step 0.8641743333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 145.95it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 139.32it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 153.48it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 159.93it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 159.70it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 157.12it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 150.69it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 125.82it/s]
-------------------------------------
| approxkl           | 0.0036710366 |
| clipfrac           | 0.3238542    |
| eplenmean          | 400          |
| eprewmean          | 350          |
| explained_variance | 0.241        |
| fps                | 2353         |
| nupdates           | 10           |
| policy_entropy     | 0.8080433    |
| policy_loss        | 0.005566505  |
| serial_timesteps   | 4000         |
| time_elapsed       | 56.4         |
| time_remaining     | 5.26         |
| total_timesteps    | 120000       |
| true_eprew         | 195          |
| value_loss         | 110.487976   |
-------------------------------------
Current reward shaping 0.88
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7695910930633545 seconds
Total simulation time for 400 steps: 4.489189624786377 	 Other agent action time: 0 	 89.10294138422242 steps/s
Curr learning rate 0.000898989898989899 	 Curr reward per step 0.8798133333333336

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 153.96it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.99it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 157.41it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 138.00it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 151.57it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 154.66it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 160.31it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 163.41it/s]
-------------------------------------
| approxkl           | 0.0025254637 |
| clipfrac           | 0.2381042    |
| eplenmean          | 400          |
| eprewmean          | 350          |
| explained_variance | 0.258        |
| fps                | 2383         |
| nupdates           | 11           |
| policy_entropy     | 0.685481     |
| policy_loss        | 0.004018933  |
| serial_timesteps   | 4400         |
| time_elapsed       | 61.4         |
| time_remaining     | 5.12         |
| total_timesteps    | 132000       |
| true_eprew         | 196          |
| value_loss         | 110.80536    |
-------------------------------------
Current reward shaping 0.868
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8040838241577148 seconds
Total simulation time for 400 steps: 4.517305850982666 	 Other agent action time: 0 	 88.5483545270654 steps/s
Curr learning rate 0.0008888888888888889 	 Curr reward per step 0.8736680000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 146.13it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 147.68it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 152.86it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 157.17it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 140.91it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 136.76it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.53it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.29it/s]
-------------------------------------
| approxkl           | 0.0027693734 |
| clipfrac           | 0.2905729    |
| eplenmean          | 400          |
| eprewmean          | 349          |
| explained_variance | 0.264        |
| fps                | 2363         |
| nupdates           | 12           |
| policy_entropy     | 0.8129834    |
| policy_loss        | 0.004607559  |
| serial_timesteps   | 4800         |
| time_elapsed       | 66.5         |
| time_remaining     | 4.99         |
| total_timesteps    | 144000       |
| true_eprew         | 197          |
| value_loss         | 106.35311    |
-------------------------------------
Current reward shaping 0.856
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7585492134094238 seconds
Total simulation time for 400 steps: 4.443833351135254 	 Other agent action time: 0 	 90.01237634120845 steps/s
Curr learning rate 0.0008787878787878789 	 Curr reward per step 0.8746880000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 150.16it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 154.71it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 160.66it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 134.21it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 145.20it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 148.25it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 165.91it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 159.80it/s]
-------------------------------------
| approxkl           | 0.0032653592 |
| clipfrac           | 0.23352084   |
| eplenmean          | 400          |
| eprewmean          | 350          |
| explained_variance | 0.21         |
| fps                | 2400         |
| nupdates           | 13           |
| policy_entropy     | 0.6613133    |
| policy_loss        | 0.0056607085 |
| serial_timesteps   | 5200         |
| time_elapsed       | 71.5         |
| time_remaining     | 4.86         |
| total_timesteps    | 156000       |
| true_eprew         | 199          |
| value_loss         | 108.03416    |
-------------------------------------
Current reward shaping 0.844
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7972488403320312 seconds
Total simulation time for 400 steps: 4.522838592529297 	 Other agent action time: 0 	 88.44003424325362 steps/s
Curr learning rate 0.0008686868686868688 	 Curr reward per step 0.819113333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.54it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 159.09it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 156.40it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 156.81it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 159.23it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.55it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 157.43it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 155.59it/s]
-------------------------------------
| approxkl           | 0.0026132385 |
| clipfrac           | 0.30584377   |
| eplenmean          | 400          |
| eprewmean          | 344          |
| explained_variance | 0.236        |
| fps                | 2369         |
| nupdates           | 14           |
| policy_entropy     | 0.8398479    |
| policy_loss        | 0.004382598  |
| serial_timesteps   | 5600         |
| time_elapsed       | 76.6         |
| time_remaining     | 4.74         |
| total_timesteps    | 168000       |
| true_eprew         | 196          |
| value_loss         | 102.03801    |
-------------------------------------
Current reward shaping 0.832
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7559621334075928 seconds
Total simulation time for 400 steps: 4.432324647903442 	 Other agent action time: 0 	 90.2460969751406 steps/s
Curr learning rate 0.0008585858585858587 	 Curr reward per step 0.8285386666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 154.40it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 160.66it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.07it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 155.23it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 156.70it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 149.55it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 155.08it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 151.95it/s]
------------------------------------
| approxkl           | 0.014016318 |
| clipfrac           | 0.5402397   |
| eplenmean          | 400         |
| eprewmean          | 338         |
| explained_variance | 0.195       |
| fps                | 2412        |
| nupdates           | 15          |
| policy_entropy     | 0.88154525  |
| policy_loss        | 0.02101117  |
| serial_timesteps   | 6000        |
| time_elapsed       | 81.5        |
| time_remaining     | 4.62        |
| total_timesteps    | 180000      |
| true_eprew         | 194         |
| value_loss         | 102.01376   |
------------------------------------
Current reward shaping 0.8200000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7559940814971924 seconds
Total simulation time for 400 steps: 4.469247102737427 	 Other agent action time: 0 	 89.50053349142384 steps/s
Curr learning rate 0.0008484848484848486 	 Curr reward per step 0.7892983333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 136.57it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 157.49it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 158.87it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 154.52it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 115.55it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 110.66it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 112.29it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 113.05it/s]
------------------------------------
| approxkl           | 0.011103783 |
| clipfrac           | 0.44431248  |
| eplenmean          | 400         |
| eprewmean          | 326         |
| explained_variance | 0.264       |
| fps                | 2344        |
| nupdates           | 16          |
| policy_entropy     | 0.81710947  |
| policy_loss        | 0.014692107 |
| serial_timesteps   | 6400        |
| time_elapsed       | 86.7        |
| time_remaining     | 4.51        |
| total_timesteps    | 192000      |
| true_eprew         | 188         |
| value_loss         | 102.06643   |
------------------------------------
Current reward shaping 0.808
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7725298404693604 seconds
Total simulation time for 400 steps: 4.453353643417358 	 Other agent action time: 0 	 89.81994964429842 steps/s
Curr learning rate 0.0008383838383838385 	 Curr reward per step 0.8432986666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 156.78it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 159.75it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.73it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 142.87it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 126.91it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 156.48it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 151.61it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 153.34it/s]
-------------------------------------
| approxkl           | 0.0018740657 |
| clipfrac           | 0.2551042    |
| eplenmean          | 400          |
| eprewmean          | 328          |
| explained_variance | 0.227        |
| fps                | 2393         |
| nupdates           | 17           |
| policy_entropy     | 0.81448746   |
| policy_loss        | 0.00279583   |
| serial_timesteps   | 6800         |
| time_elapsed       | 91.7         |
| time_remaining     | 4.4          |
| total_timesteps    | 204000       |
| true_eprew         | 191          |
| value_loss         | 98.33775     |
-------------------------------------
Current reward shaping 0.796
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.755556583404541 seconds
Total simulation time for 400 steps: 4.463598966598511 	 Other agent action time: 0 	 89.61378542141306 steps/s
Curr learning rate 0.0008282828282828282 	 Curr reward per step 0.8263763333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 156.27it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.57it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 157.85it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 144.20it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 152.25it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.10it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 156.23it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 162.39it/s]
-------------------------------------
| approxkl           | 0.002387091  |
| clipfrac           | 0.23728128   |
| eplenmean          | 400          |
| eprewmean          | 329          |
| explained_variance | 0.186        |
| fps                | 2398         |
| nupdates           | 18           |
| policy_entropy     | 0.7239737    |
| policy_loss        | 0.0034893104 |
| serial_timesteps   | 7200         |
| time_elapsed       | 96.7         |
| time_remaining     | 4.3          |
| total_timesteps    | 216000       |
| true_eprew         | 193          |
| value_loss         | 102.8765     |
-------------------------------------
Current reward shaping 0.784
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8024985790252686 seconds
Total simulation time for 400 steps: 4.554140329360962 	 Other agent action time: 0 	 87.83216393688248 steps/s
Curr learning rate 0.0008181818181818183 	 Curr reward per step 0.8213693333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 150.63it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.41it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.32it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 160.28it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.44it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 156.60it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 148.22it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 137.83it/s]
-------------------------------------
| approxkl           | 0.0030076592 |
| clipfrac           | 0.2589896    |
| eplenmean          | 400          |
| eprewmean          | 329          |
| explained_variance | 0.271        |
| fps                | 2353         |
| nupdates           | 19           |
| policy_entropy     | 0.7075595    |
| policy_loss        | 0.0046707643 |
| serial_timesteps   | 7600         |
| time_elapsed       | 102          |
| time_remaining     | 4.2          |
| total_timesteps    | 228000       |
| true_eprew         | 194          |
| value_loss         | 101.303696   |
-------------------------------------
Current reward shaping 0.772
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7597894668579102 seconds
Total simulation time for 400 steps: 4.504332065582275 	 Other agent action time: 0 	 88.80339952207586 steps/s
Curr learning rate 0.0008080808080808081 	 Curr reward per step 0.7942219999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.31it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 148.52it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 165.16it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 157.63it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 165.44it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 154.49it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 158.93it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 148.65it/s]
-------------------------------------
| approxkl           | 0.0014535803 |
| clipfrac           | 0.18825      |
| eplenmean          | 400          |
| eprewmean          | 326          |
| explained_variance | 0.303        |
| fps                | 2380         |
| nupdates           | 20           |
| policy_entropy     | 0.6928091    |
| policy_loss        | 0.0020522766 |
| serial_timesteps   | 8000         |
| time_elapsed       | 107          |
| time_remaining     | 4.09         |
| total_timesteps    | 240000       |
| true_eprew         | 194          |
| value_loss         | 98.53169     |
-------------------------------------
Current reward shaping 0.76
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8005547523498535 seconds
Total simulation time for 400 steps: 4.548377513885498 	 Other agent action time: 0 	 87.94344769730776 steps/s
Curr learning rate 0.000797979797979798 	 Curr reward per step 0.8247333333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 123.15it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 151.01it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.05it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.07it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.01it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 170.00it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.12it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.13it/s]
------------------------------------
| approxkl           | 0.009946653 |
| clipfrac           | 0.35482293  |
| eplenmean          | 400         |
| eprewmean          | 325         |
| explained_variance | 0.249       |
| fps                | 2356        |
| nupdates           | 21          |
| policy_entropy     | 0.70898956  |
| policy_loss        | 0.014366294 |
| serial_timesteps   | 8400        |
| time_elapsed       | 112         |
| time_remaining     | 4           |
| total_timesteps    | 252000      |
| true_eprew         | 194         |
| value_loss         | 93.471695   |
------------------------------------
Current reward shaping 0.748
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7534942626953125 seconds
Total simulation time for 400 steps: 4.499788045883179 	 Other agent action time: 0 	 88.89307583408443 steps/s
Curr learning rate 0.0007878787878787879 	 Curr reward per step 0.7744033333333331

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 151.59it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.47it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 141.23it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 170.02it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 155.03it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 157.20it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 160.76it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 161.75it/s]
------------------------------------
| approxkl           | 0.008014373 |
| clipfrac           | 0.34246877  |
| eplenmean          | 400         |
| eprewmean          | 320         |
| explained_variance | 0.31        |
| fps                | 2378        |
| nupdates           | 22          |
| policy_entropy     | 0.7021358   |
| policy_loss        | 0.011954684 |
| serial_timesteps   | 8800        |
| time_elapsed       | 117         |
| time_remaining     | 3.9         |
| total_timesteps    | 264000      |
| true_eprew         | 193         |
| value_loss         | 87.08046    |
------------------------------------
Current reward shaping 0.736
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7435812950134277 seconds
Total simulation time for 400 steps: 4.42391037940979 	 Other agent action time: 0 	 90.41774486701185 steps/s
Curr learning rate 0.0007777777777777778 	 Curr reward per step 0.7874533333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 158.86it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 156.33it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.25it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 150.83it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 165.08it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 160.52it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 159.62it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 160.52it/s]
------------------------------------
| approxkl           | 0.012944214 |
| clipfrac           | 0.47740626  |
| eplenmean          | 400         |
| eprewmean          | 318         |
| explained_variance | 0.225       |
| fps                | 2422        |
| nupdates           | 23          |
| policy_entropy     | 0.8446163   |
| policy_loss        | 0.01807563  |
| serial_timesteps   | 9200        |
| time_elapsed       | 122         |
| time_remaining     | 3.8         |
| total_timesteps    | 276000      |
| true_eprew         | 192         |
| value_loss         | 96.05797    |
------------------------------------
Current reward shaping 0.724
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8027346134185791 seconds
Total simulation time for 400 steps: 4.646209239959717 	 Other agent action time: 0 	 86.0916888029494 steps/s
Curr learning rate 0.0007676767676767678 	 Curr reward per step 0.749517

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 142.70it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 138.91it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 147.28it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 149.38it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 142.00it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 129.35it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 138.39it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 159.84it/s]
------------------------------------
| approxkl           | 0.005898771 |
| clipfrac           | 0.30117708  |
| eplenmean          | 400         |
| eprewmean          | 310         |
| explained_variance | 0.307       |
| fps                | 2292        |
| nupdates           | 24          |
| policy_entropy     | 0.7526366   |
| policy_loss        | 0.008530761 |
| serial_timesteps   | 9600        |
| time_elapsed       | 127         |
| time_remaining     | 3.71        |
| total_timesteps    | 288000      |
| true_eprew         | 189         |
| value_loss         | 91.18029    |
------------------------------------
Current reward shaping 0.712
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7595603466033936 seconds
Total simulation time for 400 steps: 4.448620319366455 	 Other agent action time: 0 	 89.91551790982368 steps/s
Curr learning rate 0.0007575757575757577 	 Curr reward per step 0.7491366666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.00it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.99it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.62it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 155.19it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 155.91it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.43it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 163.26it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 161.23it/s]
-------------------------------------
| approxkl           | 0.0052557345 |
| clipfrac           | 0.39245838   |
| eplenmean          | 400          |
| eprewmean          | 305          |
| explained_variance | 0.289        |
| fps                | 2414         |
| nupdates           | 25           |
| policy_entropy     | 0.8403858    |
| policy_loss        | 0.008782448  |
| serial_timesteps   | 10000        |
| time_elapsed       | 132          |
| time_remaining     | 3.61         |
| total_timesteps    | 300000       |
| true_eprew         | 187          |
| value_loss         | 92.76817     |
-------------------------------------
Current reward shaping 0.7
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7922487258911133 seconds
Total simulation time for 400 steps: 4.52969765663147 	 Other agent action time: 0 	 88.30611451834996 steps/s
Curr learning rate 0.0007474747474747475 	 Curr reward per step 0.7676916666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 156.97it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.41it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 158.74it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 158.33it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.91it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 158.67it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 161.53it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 143.14it/s]
------------------------------------
| approxkl           | 0.003400463 |
| clipfrac           | 0.27435416  |
| eplenmean          | 400         |
| eprewmean          | 302         |
| explained_variance | 0.3         |
| fps                | 2369        |
| nupdates           | 26          |
| policy_entropy     | 0.7405281   |
| policy_loss        | 0.004924071 |
| serial_timesteps   | 10400       |
| time_elapsed       | 137         |
| time_remaining     | 3.52        |
| total_timesteps    | 312000      |
| true_eprew         | 186         |
| value_loss         | 90.32881    |
------------------------------------
Current reward shaping 0.688
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7663145065307617 seconds
Total simulation time for 400 steps: 4.52816367149353 	 Other agent action time: 0 	 88.3360295737869 steps/s
Curr learning rate 0.0007373737373737374 	 Curr reward per step 0.7563879999999998

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 142.69it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 148.47it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.14it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 142.08it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.03it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 150.38it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 164.08it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 161.31it/s]
------------------------------------
| approxkl           | 0.01528894  |
| clipfrac           | 0.48981243  |
| eplenmean          | 400         |
| eprewmean          | 302         |
| explained_variance | 0.343       |
| fps                | 2363        |
| nupdates           | 27          |
| policy_entropy     | 0.87875205  |
| policy_loss        | 0.020817013 |
| serial_timesteps   | 10800       |
| time_elapsed       | 142         |
| time_remaining     | 3.42        |
| total_timesteps    | 324000      |
| true_eprew         | 188         |
| value_loss         | 87.309105   |
------------------------------------
Current reward shaping 0.6759999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7588675022125244 seconds
Total simulation time for 400 steps: 4.469850778579712 	 Other agent action time: 0 	 89.4884459939621 steps/s
Curr learning rate 0.0007272727272727272 	 Curr reward per step 0.7092926666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 151.45it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 154.80it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 158.67it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 125.94it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 158.55it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 158.16it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.00it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.06it/s]
-------------------------------------
| approxkl           | 0.003475529  |
| clipfrac           | 0.28798956   |
| eplenmean          | 400          |
| eprewmean          | 298          |
| explained_variance | 0.249        |
| fps                | 2390         |
| nupdates           | 28           |
| policy_entropy     | 0.791243     |
| policy_loss        | 0.0055517717 |
| serial_timesteps   | 11200        |
| time_elapsed       | 147          |
| time_remaining     | 3.33         |
| total_timesteps    | 336000       |
| true_eprew         | 186          |
| value_loss         | 87.70973     |
-------------------------------------
Current reward shaping 0.6639999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8035223484039307 seconds
Total simulation time for 400 steps: 4.649608135223389 	 Other agent action time: 0 	 86.02875519116884 steps/s
Curr learning rate 0.0007171717171717171 	 Curr reward per step 0.6985239999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 152.57it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 160.96it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.42it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 158.12it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 162.82it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 159.96it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 145.26it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.92it/s]
------------------------------------
| approxkl           | 0.005154254 |
| clipfrac           | 0.37806255  |
| eplenmean          | 400         |
| eprewmean          | 291         |
| explained_variance | 0.278       |
| fps                | 2315        |
| nupdates           | 29          |
| policy_entropy     | 0.8440485   |
| policy_loss        | 0.008674392 |
| serial_timesteps   | 11600       |
| time_elapsed       | 152         |
| time_remaining     | 3.24        |
| total_timesteps    | 348000      |
| true_eprew         | 183         |
| value_loss         | 90.02426    |
------------------------------------
Current reward shaping 0.652
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7757785320281982 seconds
Total simulation time for 400 steps: 4.5499513149261475 	 Other agent action time: 0 	 87.91302858292069 steps/s
Curr learning rate 0.0007070707070707071 	 Curr reward per step 0.6332176666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 152.24it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 161.83it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 126.63it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 153.46it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.75it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 165.80it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.28it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.40it/s]
------------------------------------
| approxkl           | 0.045400195 |
| clipfrac           | 0.47310418  |
| eplenmean          | 400         |
| eprewmean          | 275         |
| explained_variance | 0.251       |
| fps                | 2352        |
| nupdates           | 30          |
| policy_entropy     | 0.7781144   |
| policy_loss        | 0.027473778 |
| serial_timesteps   | 12000       |
| time_elapsed       | 158         |
| time_remaining     | 3.15        |
| total_timesteps    | 360000      |
| true_eprew         | 174         |
| value_loss         | 92.12519    |
------------------------------------
Current reward shaping 0.64
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8230001926422119 seconds
Total simulation time for 400 steps: 4.645275115966797 	 Other agent action time: 0 	 86.10900108480445 steps/s
Curr learning rate 0.000696969696969697 	 Curr reward per step 0.6174666666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 132.23it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.17it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 159.59it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 143.96it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 155.46it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 142.84it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 155.31it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 156.87it/s]
-------------------------------------
| approxkl           | 0.0031111261 |
| clipfrac           | 0.27058333   |
| eplenmean          | 400          |
| eprewmean          | 263          |
| explained_variance | 0.292        |
| fps                | 2305         |
| nupdates           | 31           |
| policy_entropy     | 0.7406994    |
| policy_loss        | 0.00486163   |
| serial_timesteps   | 12400        |
| time_elapsed       | 163          |
| time_remaining     | 3.06         |
| total_timesteps    | 372000       |
| true_eprew         | 167          |
| value_loss         | 92.10293     |
-------------------------------------
Current reward shaping 0.628
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7647190093994141 seconds
Total simulation time for 400 steps: 4.469873428344727 	 Other agent action time: 0 	 89.48799253766053 steps/s
Curr learning rate 0.0006868686868686869 	 Curr reward per step 0.5932183333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 161.46it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 160.43it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 141.33it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 158.70it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.21it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.56it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 160.60it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 167.44it/s]
-------------------------------------
| approxkl           | 0.0044652116 |
| clipfrac           | 0.41634384   |
| eplenmean          | 400          |
| eprewmean          | 250          |
| explained_variance | 0.287        |
| fps                | 2399         |
| nupdates           | 32           |
| policy_entropy     | 0.84835356   |
| policy_loss        | 0.008680515  |
| serial_timesteps   | 12800        |
| time_elapsed       | 168          |
| time_remaining     | 2.97         |
| total_timesteps    | 384000       |
| true_eprew         | 161          |
| value_loss         | 88.25493     |
-------------------------------------
Current reward shaping 0.616
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.745182991027832 seconds
Total simulation time for 400 steps: 4.428642272949219 	 Other agent action time: 0 	 90.32113576733377 steps/s
Curr learning rate 0.0006767676767676768 	 Curr reward per step 0.6517393333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 128.29it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 115.20it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 112.33it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 112.95it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 114.04it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 115.23it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 113.21it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 114.40it/s]
-------------------------------------
| approxkl           | 0.0017516909 |
| clipfrac           | 0.21364585   |
| eplenmean          | 400          |
| eprewmean          | 249          |
| explained_variance | 0.279        |
| fps                | 2329         |
| nupdates           | 33           |
| policy_entropy     | 0.8336795    |
| policy_loss        | 0.0022794046 |
| serial_timesteps   | 13200        |
| time_elapsed       | 173          |
| time_remaining     | 2.88         |
| total_timesteps    | 396000       |
| true_eprew         | 162          |
| value_loss         | 84.77259     |
-------------------------------------
Current reward shaping 0.604
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7726261615753174 seconds
Total simulation time for 400 steps: 4.539828062057495 	 Other agent action time: 0 	 88.10906372051369 steps/s
Curr learning rate 0.0006666666666666668 	 Curr reward per step 0.6476746666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 158.01it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.92it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 159.85it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.72it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 153.75it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 146.35it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 150.24it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 160.39it/s]
-------------------------------------
| approxkl           | 0.0009235116 |
| clipfrac           | 0.15651041   |
| eplenmean          | 400          |
| eprewmean          | 249          |
| explained_variance | 0.396        |
| fps                | 2362         |
| nupdates           | 34           |
| policy_entropy     | 0.8374573    |
| policy_loss        | 0.0004790604 |
| serial_timesteps   | 13600        |
| time_elapsed       | 178          |
| time_remaining     | 2.79         |
| total_timesteps    | 408000       |
| true_eprew         | 163          |
| value_loss         | 82.71683     |
-------------------------------------
Current reward shaping 0.5920000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7711925506591797 seconds
Total simulation time for 400 steps: 4.448950290679932 	 Other agent action time: 0 	 89.90884902399486 steps/s
Curr learning rate 0.0006565656565656567 	 Curr reward per step 0.6385853333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 136.76it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 151.65it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 157.24it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 143.40it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 151.53it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 154.92it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 145.23it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 165.48it/s]
-------------------------------------
| approxkl           | 0.0034411822 |
| clipfrac           | 0.29921868   |
| eplenmean          | 400          |
| eprewmean          | 256          |
| explained_variance | 0.43         |
| fps                | 2395         |
| nupdates           | 35           |
| policy_entropy     | 0.8010971    |
| policy_loss        | 0.006487433  |
| serial_timesteps   | 14000        |
| time_elapsed       | 183          |
| time_remaining     | 2.7          |
| total_timesteps    | 420000       |
| true_eprew         | 168          |
| value_loss         | 85.11611     |
-------------------------------------
Current reward shaping 0.5800000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8069467544555664 seconds
Total simulation time for 400 steps: 4.573236465454102 	 Other agent action time: 0 	 87.465409458175 steps/s
Curr learning rate 0.0006464646464646465 	 Curr reward per step 0.6444066666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 165.98it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 165.90it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 171.36it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.83it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 162.34it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 146.21it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.81it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 153.37it/s]
------------------------------------
| approxkl           | 0.005878742 |
| clipfrac           | 0.3347813   |
| eplenmean          | 400         |
| eprewmean          | 258         |
| explained_variance | 0.296       |
| fps                | 2352        |
| nupdates           | 36          |
| policy_entropy     | 0.73603225  |
| policy_loss        | 0.009865226 |
| serial_timesteps   | 14400       |
| time_elapsed       | 188         |
| time_remaining     | 2.61        |
| total_timesteps    | 432000      |
| true_eprew         | 170         |
| value_loss         | 84.269745   |
------------------------------------
Current reward shaping 0.5680000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.761481761932373 seconds
Total simulation time for 400 steps: 4.490317344665527 	 Other agent action time: 0 	 89.08056364327965 steps/s
Curr learning rate 0.0006363636363636364 	 Curr reward per step 0.6623446666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.02it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.16it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 167.89it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.16it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 170.92it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.34it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 164.54it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.89it/s]
--------------------------------------
| approxkl           | 0.0010423162  |
| clipfrac           | 0.16517708    |
| eplenmean          | 400           |
| eprewmean          | 257           |
| explained_variance | 0.264         |
| fps                | 2398          |
| nupdates           | 37            |
| policy_entropy     | 0.75996155    |
| policy_loss        | 0.00041728118 |
| serial_timesteps   | 14800         |
| time_elapsed       | 193           |
| time_remaining     | 2.52          |
| total_timesteps    | 444000        |
| true_eprew         | 171           |
| value_loss         | 80.62998      |
--------------------------------------
Current reward shaping 0.556
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8104152679443359 seconds
Total simulation time for 400 steps: 4.6453094482421875 	 Other agent action time: 0 	 86.10836467554651 steps/s
Curr learning rate 0.0006262626262626263 	 Curr reward per step 0.6190493333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.04it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.94it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 164.82it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.09it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.47it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 162.70it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 151.47it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 160.65it/s]
-------------------------------------
| approxkl           | 0.0011758984 |
| clipfrac           | 0.18205206   |
| eplenmean          | 400          |
| eprewmean          | 256          |
| explained_variance | 0.325        |
| fps                | 2317         |
| nupdates           | 38           |
| policy_entropy     | 0.8079444    |
| policy_loss        | 0.0010408952 |
| serial_timesteps   | 15200        |
| time_elapsed       | 198          |
| time_remaining     | 2.44         |
| total_timesteps    | 456000       |
| true_eprew         | 171          |
| value_loss         | 87.21167     |
-------------------------------------
Current reward shaping 0.544
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7542204856872559 seconds
Total simulation time for 400 steps: 4.488957405090332 	 Other agent action time: 0 	 89.10755079707661 steps/s
Curr learning rate 0.0006161616161616161 	 Curr reward per step 0.635848

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 151.66it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 154.77it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 140.35it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.85it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 141.74it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 162.22it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 157.25it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 148.36it/s]
------------------------------------
| approxkl           | 0.005097117 |
| clipfrac           | 0.30743748  |
| eplenmean          | 400         |
| eprewmean          | 257         |
| explained_variance | 0.276       |
| fps                | 2378        |
| nupdates           | 39          |
| policy_entropy     | 0.8069822   |
| policy_loss        | 0.008190701 |
| serial_timesteps   | 15600       |
| time_elapsed       | 203         |
| time_remaining     | 2.35        |
| total_timesteps    | 468000      |
| true_eprew         | 173         |
| value_loss         | 84.46952    |
------------------------------------
Current reward shaping 0.532
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.750870943069458 seconds
Total simulation time for 400 steps: 4.535733461380005 	 Other agent action time: 0 	 88.18860354247961 steps/s
Curr learning rate 0.0006060606060606061 	 Curr reward per step 0.6545719999999998

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 151.58it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 156.49it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 151.48it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 155.93it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 152.62it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 152.94it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 154.82it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 161.37it/s]
-------------------------------------
| approxkl           | 0.0035357638 |
| clipfrac           | 0.22725001   |
| eplenmean          | 400          |
| eprewmean          | 256          |
| explained_variance | 0.252        |
| fps                | 2361         |
| nupdates           | 40           |
| policy_entropy     | 0.75976765   |
| policy_loss        | 0.0041344524 |
| serial_timesteps   | 16000        |
| time_elapsed       | 208          |
| time_remaining     | 2.26         |
| total_timesteps    | 480000       |
| true_eprew         | 174          |
| value_loss         | 79.85725     |
-------------------------------------
Current reward shaping 0.52
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8014485836029053 seconds
Total simulation time for 400 steps: 4.547368049621582 	 Other agent action time: 0 	 87.96297014781699 steps/s
Curr learning rate 0.000595959595959596 	 Curr reward per step 0.6518266666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 146.86it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.67it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 152.98it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 153.53it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.18it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 160.63it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 159.63it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 142.50it/s]
-------------------------------------
| approxkl           | 0.0031109743 |
| clipfrac           | 0.3009792    |
| eplenmean          | 400          |
| eprewmean          | 259          |
| explained_variance | 0.229        |
| fps                | 2355         |
| nupdates           | 41           |
| policy_entropy     | 0.8604296    |
| policy_loss        | 0.0054939818 |
| serial_timesteps   | 16400        |
| time_elapsed       | 214          |
| time_remaining     | 2.17         |
| total_timesteps    | 492000       |
| true_eprew         | 177          |
| value_loss         | 76.90556     |
-------------------------------------
Current reward shaping 0.508
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7453970909118652 seconds
Total simulation time for 400 steps: 4.467554092407227 	 Other agent action time: 0 	 89.53445033375529 steps/s
Curr learning rate 0.0005858585858585859 	 Curr reward per step 0.6177906666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 145.82it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.91it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.11it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 147.54it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 152.56it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 146.12it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 155.29it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.23it/s]
-------------------------------------
| approxkl           | 0.0031151962 |
| clipfrac           | 0.2933125    |
| eplenmean          | 400          |
| eprewmean          | 256          |
| explained_variance | 0.49         |
| fps                | 2388         |
| nupdates           | 42           |
| policy_entropy     | 0.8558308    |
| policy_loss        | 0.0042050714 |
| serial_timesteps   | 16800        |
| time_elapsed       | 219          |
| time_remaining     | 2.08         |
| total_timesteps    | 504000       |
| true_eprew         | 176          |
| value_loss         | 69.43981     |
-------------------------------------
Current reward shaping 0.496
Current self-play randomization 0.9984
SP envs: 30/30
Other agent actions took 0.8144798278808594 seconds
Total simulation time for 400 steps: 4.52812647819519 	 Other agent action time: 0 	 88.33675515164299 steps/s
Curr learning rate 0.0005757575757575758 	 Curr reward per step 0.5775666666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 161.82it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.73it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.80it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 155.95it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 162.08it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 162.71it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 146.83it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 162.96it/s]
-------------------------------------
| approxkl           | 0.0021894432 |
| clipfrac           | 0.21743746   |
| eplenmean          | 400          |
| eprewmean          | 249          |
| explained_variance | 0.316        |
| fps                | 2373         |
| nupdates           | 43           |
| policy_entropy     | 0.76780343   |
| policy_loss        | 0.0028526953 |
| serial_timesteps   | 17200        |
| time_elapsed       | 224          |
| time_remaining     | 1.99         |
| total_timesteps    | 516000       |
| true_eprew         | 173          |
| value_loss         | 77.373764    |
-------------------------------------
Current reward shaping 0.484
Current self-play randomization 0.9936
SP envs: 28/30
Other agent actions took 6.092252731323242 seconds
Total simulation time for 400 steps: 9.94091796875 	 Other agent action time: 0 	 40.23773269806965 steps/s
Curr learning rate 0.0005656565656565657 	 Curr reward per step 0.5501376666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 155.10it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 155.96it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 159.57it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 147.35it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 162.34it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 161.22it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 161.75it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.26it/s]
-------------------------------------
| approxkl           | 0.0021063422 |
| clipfrac           | 0.23134372   |
| eplenmean          | 400          |
| eprewmean          | 235          |
| explained_variance | 0.234        |
| fps                | 1145         |
| nupdates           | 44           |
| policy_entropy     | 0.8252424    |
| policy_loss        | 0.0031320795 |
| serial_timesteps   | 17600        |
| time_elapsed       | 234          |
| time_remaining     | 1.95         |
| total_timesteps    | 528000       |
| true_eprew         | 164          |
| value_loss         | 92.516815    |
-------------------------------------
Current reward shaping 0.472
Current self-play randomization 0.9888
SP envs: 30/30
Other agent actions took 0.7462592124938965 seconds
Total simulation time for 400 steps: 4.494735956192017 	 Other agent action time: 0 	 88.99299177940674 steps/s
Curr learning rate 0.0005555555555555557 	 Curr reward per step 0.4966100000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 146.28it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 143.29it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 157.18it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 156.71it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 154.27it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 162.97it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.43it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 150.46it/s]
-------------------------------------
| approxkl           | 0.0133281965 |
| clipfrac           | 0.46608338   |
| eplenmean          | 400          |
| eprewmean          | 222          |
| explained_variance | 0.417        |
| fps                | 2379         |
| nupdates           | 45           |
| policy_entropy     | 0.8434644    |
| policy_loss        | 0.017556664  |
| serial_timesteps   | 18000        |
| time_elapsed       | 239          |
| time_remaining     | 1.86         |
| total_timesteps    | 540000       |
| true_eprew         | 156          |
| value_loss         | 89.17704     |
-------------------------------------
Current reward shaping 0.45999999999999996
Current self-play randomization 0.984
SP envs: 29/30
Other agent actions took 6.061458587646484 seconds
Total simulation time for 400 steps: 9.853366613388062 	 Other agent action time: 0 	 40.59526207585823 steps/s
Curr learning rate 0.0005454545454545455 	 Curr reward per step 0.5008583333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 163.81it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 158.33it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 158.57it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 163.33it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.17it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 165.65it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.19it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.15it/s]
-------------------------------------
| approxkl           | 0.004767515  |
| clipfrac           | 0.30934376   |
| eplenmean          | 400          |
| eprewmean          | 210          |
| explained_variance | 0.36         |
| fps                | 1156         |
| nupdates           | 46           |
| policy_entropy     | 0.7715685    |
| policy_loss        | 0.0072262855 |
| serial_timesteps   | 18400        |
| time_elapsed       | 250          |
| time_remaining     | 1.81         |
| total_timesteps    | 552000       |
| true_eprew         | 148          |
| value_loss         | 93.67351     |
-------------------------------------
Current reward shaping 0.44799999999999995
Current self-play randomization 0.9792
SP envs: 30/30
Other agent actions took 0.7501955032348633 seconds
Total simulation time for 400 steps: 4.481918573379517 	 Other agent action time: 0 	 89.24749377996544 steps/s
Curr learning rate 0.0005353535353535353 	 Curr reward per step 0.46947466666666654

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.87it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.43it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 158.09it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.46it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.08it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 156.25it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.99it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 126.76it/s]
------------------------------------
| approxkl           | 0.022151299 |
| clipfrac           | 0.40695834  |
| eplenmean          | 400         |
| eprewmean          | 199         |
| explained_variance | 0.28        |
| fps                | 2387        |
| nupdates           | 47          |
| policy_entropy     | 0.7212616   |
| policy_loss        | 0.017569002 |
| serial_timesteps   | 18800       |
| time_elapsed       | 255         |
| time_remaining     | 1.72        |
| total_timesteps    | 564000      |
| true_eprew         | 141         |
| value_loss         | 104.38674   |
------------------------------------
Current reward shaping 0.43600000000000005
Current self-play randomization 0.9744
SP envs: 30/30
Other agent actions took 0.800386905670166 seconds
Total simulation time for 400 steps: 4.573858261108398 	 Other agent action time: 0 	 87.45351892541302 steps/s
Curr learning rate 0.0005252525252525252 	 Curr reward per step 0.5408103333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 155.67it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.57it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.97it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 162.96it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.57it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 143.82it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 161.97it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 158.09it/s]
------------------------------------
| approxkl           | 0.005727976 |
| clipfrac           | 0.41061458  |
| eplenmean          | 400         |
| eprewmean          | 202         |
| explained_variance | 0.173       |
| fps                | 2353        |
| nupdates           | 48          |
| policy_entropy     | 0.81122077  |
| policy_loss        | 0.009325348 |
| serial_timesteps   | 19200       |
| time_elapsed       | 260         |
| time_remaining     | 1.62        |
| total_timesteps    | 576000      |
| true_eprew         | 145         |
| value_loss         | 89.317764   |
------------------------------------
Current reward shaping 0.42400000000000004
Current self-play randomization 0.9696
SP envs: 29/30
Other agent actions took 5.990200519561768 seconds
Total simulation time for 400 steps: 9.787884712219238 	 Other agent action time: 0 	 40.866848329408526 steps/s
Curr learning rate 0.0005151515151515151 	 Curr reward per step 0.46248866666666677

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 134.61it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 141.78it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.06it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.44it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 151.37it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.58it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 141.13it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 144.57it/s]
-------------------------------------
| approxkl           | 0.0034978266 |
| clipfrac           | 0.24116662   |
| eplenmean          | 400          |
| eprewmean          | 198          |
| explained_variance | 0.324        |
| fps                | 1159         |
| nupdates           | 49           |
| policy_entropy     | 0.6351498    |
| policy_loss        | 0.005360448  |
| serial_timesteps   | 19600        |
| time_elapsed       | 270          |
| time_remaining     | 1.56         |
| total_timesteps    | 588000       |
| true_eprew         | 143          |
| value_loss         | 92.79024     |
-------------------------------------
Current reward shaping 0.41200000000000003
Current self-play randomization 0.9648
SP envs: 30/30
Other agent actions took 0.763852596282959 seconds
Total simulation time for 400 steps: 4.53294038772583 	 Other agent action time: 0 	 88.2429429434168 steps/s
Curr learning rate 0.000505050505050505 	 Curr reward per step 0.47172433333333325

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 144.88it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 150.06it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 154.00it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 156.46it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.30it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 130.51it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 144.81it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 165.33it/s]
------------------------------------
| approxkl           | 0.003176169 |
| clipfrac           | 0.26070836  |
| eplenmean          | 400         |
| eprewmean          | 197         |
| explained_variance | 0.275       |
| fps                | 2351        |
| nupdates           | 50          |
| policy_entropy     | 0.74636996  |
| policy_loss        | 0.004777772 |
| serial_timesteps   | 20000       |
| time_elapsed       | 275         |
| time_remaining     | 1.47        |
| total_timesteps    | 600000      |
| true_eprew         | 143         |
| value_loss         | 96.276535   |
------------------------------------
Current reward shaping 0.4
Current self-play randomization 0.96
../../thesis_data/dr_ppo/ppo_bc_train_simple/
PPO agent on index 0:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X ↑0    X 
X D X S X 


Timestep: 2
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X ↑0    X 
X D X S X 


Timestep: 3
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X ↑0    X 
X D X S X 


Timestep: 4
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X ↑0    X 
X D X S X 


Timestep: 5
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X ←0    X 
X D X S X 


Timestep: 6
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X ←0    X 
X D X S X 


Timestep: 7
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X ↓0    X 
X D X S X 


Timestep: 8
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑1  O 
X ↓d    X 
X D X S X 


Timestep: 9
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 10
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 11
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 12
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 13
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     ↑1O 
X ↓d    X 
X D X S X 


Timestep: 14
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←1  O 
X ↓d    X 
X D X S X 


Timestep: 15
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←1  O 
X ↓d    X 
X D X S X 


Timestep: 16
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←1  O 
X ↓d    X 
X D X S X 


Timestep: 17
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←1  O 
X ↓d    X 
X D X S X 


Timestep: 18
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X ←d    X 
X D X S X 


Timestep: 19
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←1  O 
Xd←0    X 
X D X S X 


Timestep: 20
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←1  O 
Xd  →0  X 
X D X S X 


Timestep: 21
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←1    O 
Xd←0    X 
X D X S X 


Timestep: 22
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →1  O 
Xd←0    X 
X D X S X 


Timestep: 23
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →1  O 
Xd←0    X 
X D X S X 


Timestep: 24
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →1  O 
Xd  →0  X 
X D X S X 


Timestep: 25
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →1  O 
Xd←0    X 
X D X S X 


Timestep: 26
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
Xd  →0  X 
X D X S X 


Timestep: 27
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
Xd  →0  X 
X D X S X 


Timestep: 28
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
Xd←0    X 
X D X S X 


Timestep: 29
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
Xd←0    X 
X D X S X 


Timestep: 30
Joint action taken: ('↓', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
Xd↓0    X 
X D X S X 


Timestep: 31
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
Xd↓d    X 
X D X S X 


Timestep: 32
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
Xd←d    X 
X D X S X 


Timestep: 33
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
Xd←d    X 
X D X S X 


Timestep: 34
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     ↑oO 
Xd←d    X 
X D X S X 


Timestep: 35
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
Xd  →d  X 
X D X S X 


Timestep: 36
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
Xd←d    X 
X D X S X 


Timestep: 37
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
Xd↓d    X 
X D X S X 


Timestep: 38
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
Xd↓d    X 
X D X S X 


Timestep: 39
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
Xd↓d    X 
X D X S X 


Timestep: 40
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
Xd↓d    X 
X D X S X 


Timestep: 41
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑1  O 
Xd↓d    X 
X D X S X 


Timestep: 42
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←1    O 
Xd↓d    X 
X D X S X 


Timestep: 43
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←1    O 
Xd↓d    X 
X D X S X 


Timestep: 44
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←1    O 
Xd↓d    X 
X D X S X 


Timestep: 45
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←1    O 
Xd↓d    X 
X D X S X 


Timestep: 46
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   →1  O 
Xd←d    X 
X D X S X 


Timestep: 47
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø=X X 
O       O 
Xd←d↓1  X 
X D X S X 


Timestep: 48
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O       O 
Xd  →d→1X 
X D X S X 


Timestep: 49
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O       O 
Xd  →d←1X 
X D X S X 


Timestep: 50
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O       O 
Xd  →d←1X 
X D X S X 


Timestep: 51
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O       O 
Xd  →d←1X 
X D X S X 


Timestep: 52
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O       O 
Xd←d←1  X 
X D X S X 


Timestep: 53
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O       O 
Xd←d←1  X 
X D X S X 


Timestep: 54
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑1  O 
Xd←d    X 
X D X S X 


Timestep: 55
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑1  O 
Xd↓d    X 
X D X S X 


Timestep: 56
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑1  O 
Xd↓d    X 
X D X S X 


Timestep: 57
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
Xd↓d    X 
X D X S X 


Timestep: 58
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
Xd↓d    X 
X D X S X 


Timestep: 59
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
Xd↓d    X 
X D X S X 


Timestep: 60
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
Xd↓d    X 
X D X S X 


Timestep: 61
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     ↑oO 
Xd↓d    X 
X D X S X 


Timestep: 62
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     ↑oO 
Xd←d    X 
X D X S X 


Timestep: 63
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     ↑oO 
Xd←d    X 
X D X S X 


Timestep: 64
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     ↑oO 
Xd←d    X 
X D X S X 


Timestep: 65
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
Xd↓d    X 
X D X S X 


Timestep: 66
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o  O 
Xd↓d    X 
X D X S X 


Timestep: 67
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o  O 
Xd↓d    X 
X D X S X 


Timestep: 68
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø1X X 
O   ↑1  O 
Xd↓d    X 
X D X S X 


Timestep: 69
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø2X X 
O   ↑1  O 
Xd↓d    X 
X D X S X 


Timestep: 70
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø3X X 
O     →1O 
Xd↓d    X 
X D X S X 


Timestep: 71
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø4X X 
O     →1O 
Xd↓d    X 
X D X S X 


Timestep: 72
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø5X X 
O     →1O 
Xd↓d    X 
X D X S X 


Timestep: 73
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø6X X 
O     →1O 
Xd↓d    X 
X D X S X 


Timestep: 74
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø7X X 
O     →oO 
Xd↓d    X 
X D X S X 


Timestep: 75
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø8X X 
O     ↑oO 
Xd↓d    X 
X D X S X 


Timestep: 76
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø9X X 
O     ↑oO 
Xd↓d    X 
X D X S X 


Timestep: 77
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø10X X 
O     ↑oO 
Xd↓d    X 
X D X S X 


Timestep: 78
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø11X X 
O     ↑oO 
Xd↓d    X 
X D X S X 


Timestep: 79
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø12X X 
O   ←o  O 
Xd↓d    X 
X D X S X 


Timestep: 80
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø13X X 
O   ←o  O 
Xd↓d    X 
X D X S X 


Timestep: 81
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø14X X 
O   ←o  O 
Xd↓d    X 
X D X S X 


Timestep: 82
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø15X X 
O   ←o  O 
Xd↓d    X 
X D X S X 


Timestep: 83
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø16X X 
O   ↑o  O 
Xd↓d    X 
X D X S X 


Timestep: 84
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø17X X 
O   ↑o  O 
Xd↓d    X 
X D X S X 


Timestep: 85
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø18X X 
O   ↑o  O 
Xd↓d    X 
X D X S X 


Timestep: 86
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø19X X 
O   ↑o  O 
Xd↓d    X 
X D X S X 


Timestep: 87
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
Xd↓d    X 
X D X S X 


Timestep: 88
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
Xd↓d    X 
X D X S X 


Timestep: 89
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
Xd↓d    X 
X D X S X 


Timestep: 90
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
Xd↓d    X 
X D X S X 


Timestep: 91
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
Xd↓d    X 
X D X S X 


Timestep: 92
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
Xd↓d    X 
X D X S X 


Timestep: 93
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     ↑oO 
Xd↓d    X 
X D X S X 


Timestep: 94
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
Xd  →d  X 
X D X S X 


Timestep: 95
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
Xd  →d  X 
X D X S X 


Timestep: 96
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
Xd  →d  X 
X D X S X 


Timestep: 97
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d→oO 
Xd      X 
X D X S X 


Timestep: 98
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d→oO 
Xd      X 
X D X S X 


Timestep: 99
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 5 
X X P X X 
O   ↑s↑oO 
Xd      X 
X D X S X 


tot rew 60 tot rew shaped 51
PPO agent on index 1:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 2
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 3
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 4
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0  ↑1O 
X       X 
X D X S X 


Timestep: 5
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O   →0↑1O 
X       X 
X D X S X 


Timestep: 6
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   →0←1O 
X       X 
X D X S X 


Timestep: 7
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O ←0  ←1O 
X       X 
X D X S X 


Timestep: 8
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     ←1O 
X ↓0    X 
X D X S X 


Timestep: 9
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←1  O 
X   →0  X 
X D X S X 


Timestep: 10
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X   →0  X 
X D X S X 


Timestep: 11
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X   →0  X 
X D X S X 


Timestep: 12
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   →0  X 
X D X S X 


Timestep: 13
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X     →0X 
X D X S X 


Timestep: 14
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑1↑0O 
X       X 
X D X S X 


Timestep: 15
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →1↑0O 
X       X 
X D X S X 


Timestep: 16
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑1↑0O 
X       X 
X D X S X 


Timestep: 17
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →1→0O 
X       X 
X D X S X 


Timestep: 18
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →1→0O 
X       X 
X D X S X 


Timestep: 19
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←1  →oO 
X       X 
X D X S X 


Timestep: 20
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X ↓1    X 
X D X S X 


Timestep: 21
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O     ↑oO 
X ↓d    X 
X D X S X 


Timestep: 22
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 23
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 24
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 25
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 26
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑0  O 
X ↓d    X 
X D X S X 


Timestep: 27
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑0  O 
X ↓d    X 
X D X S X 


Timestep: 28
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →0O 
X ↓d    X 
X D X S X 


Timestep: 29
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →0O 
X ↓d    X 
X D X S X 


Timestep: 30
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
X ↓d    X 
X D X S X 


Timestep: 31
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
X ←d    X 
X D X S X 


Timestep: 32
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ←d    X 
X D X S X 


Timestep: 33
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o  O 
X ←d    X 
X D X S X 


Timestep: 34
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o  O 
X ←d    X 
X D X S X 


Timestep: 35
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o  O 
X ←d    X 
X D X S X 


Timestep: 36
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 37
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 38
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø1X X 
O   ↑0  O 
X ↓d    X 
X D X S X 


Timestep: 39
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø2X X 
O ←0    O 
X ↓d    X 
X D X S X 


Timestep: 40
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø3X X 
O ←0    O 
X ↓d    X 
X D X S X 


Timestep: 41
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø4X X 
O   →0  O 
X ↓d    X 
X D X S X 


Timestep: 42
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø5X X 
O   →0  O 
X ↓d    X 
X D X S X 


Timestep: 43
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø6X X 
O   →0  O 
X ↓d    X 
X D X S X 


Timestep: 44
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø7X X 
O   →0  O 
X ↓d    X 
X D X S X 


Timestep: 45
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø8X X 
O ←0    O 
X   →d  X 
X D X S X 


Timestep: 46
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø9X X 
O ←0    O 
X   →d  X 
X D X S X 


Timestep: 47
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø10X X 
O ←0    O 
X   →d  X 
X D X S X 


Timestep: 48
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø11X X 
O ←0    O 
X   →d  X 
X D X S X 


Timestep: 49
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø12X X 
O   →0  O 
X   →d  X 
X D X S X 


Timestep: 50
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø13X X 
O   →0  O 
X   →d  X 
X D X S X 


Timestep: 51
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø14X X 
O   →0  O 
X   →d  X 
X D X S X 


Timestep: 52
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø15X X 
O   →0  O 
X   →d  X 
X D X S X 


Timestep: 53
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø16X X 
O ←0    O 
X   →d  X 
X D X S X 


Timestep: 54
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø17X X 
O ←o    O 
X   →d  X 
X D X S X 


Timestep: 55
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø18X X 
O   →o  O 
X   →d  X 
X D X S X 


Timestep: 56
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø19X X 
O   →o  O 
X   →d  X 
X D X S X 


Timestep: 57
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
X   →d  X 
X D X S X 


Timestep: 58
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
X   →d  X 
X D X S X 


Timestep: 59
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
X   →d  X 
X D X S X 


Timestep: 60
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
X   →d  X 
X D X S X 


Timestep: 61
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d→oO 
X       X 
X D X S X 


Timestep: 62
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d→oO 
X       X 
X D X S X 


Timestep: 63
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 64
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 5 
X X P X X 
O   ↑s↑oO 
X       X 
X D X S X 


Timestep: 65
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 66
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 67
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s←oO 
X       X 
X D X S X 


Timestep: 68
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X P X X 
O     ←oO 
X   ↓s  X 
X D X S X 


Timestep: 69
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s←oO 
X       X 
X D X S X 


Timestep: 70
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X P X X 
O     ←oO 
X   ↓s  X 
X D X S X 


Timestep: 71
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s↑oO 
X       X 
X D X S X 


Timestep: 72
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 73
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 74
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 75
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 76
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s←oO 
X       X 
X D X S X 


Timestep: 77
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X P X X 
O     ←oO 
X   ↓s  X 
X D X S X 


Timestep: 78
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s←oO 
X       X 
X D X S X 


Timestep: 79
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s←oO 
X       X 
X D X S X 


Timestep: 80
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   ↓s  X 
X D X S X 


Timestep: 81
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   ↑s  X 
X D X S X 


Timestep: 82
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X   ↑s  X 
X D X S X 


Timestep: 83
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑0  O 
X   ↑s  X 
X D X S X 


Timestep: 84
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑0  O 
X   ↑s  X 
X D X S X 


Timestep: 85
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑0  O 
X   ↑s  X 
X D X S X 


Timestep: 86
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←0    O 
X     →sX 
X D X S X 


Timestep: 87
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←0    O 
X   ←s  X 
X D X S X 


Timestep: 88
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o↑s  O 
X       X 
X D X S X 


Timestep: 89
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o  →sO 
X       X 
X D X S X 


Timestep: 90
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →o  O 
X     ↓sX 
X D X S X 


Timestep: 91
Joint action taken: ('stay', 'interact') 	 Reward: 20 + shape * 0 
X X ø-X X 
O   →o  O 
X     ↓1X 
X D X S X 


Timestep: 92
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X     ↓1X 
X D X S X 


Timestep: 93
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o    O 
X     ↓1X 
X D X S X 


Timestep: 94
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →o  O 
X     ↓1X 
X D X S X 


Timestep: 95
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →o  O 
X     ↓1X 
X D X S X 


Timestep: 96
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X     ↓1X 
X D X S X 


Timestep: 97
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     ↑oO 
X   ←1  X 
X D X S X 


Timestep: 98
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X ←1    X 
X D X S X 


Timestep: 99
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X ←1    X 
X D X S X 


tot rew 100 tot rew shaped 96
../../thesis_data/dr_ppo/ppo_bc_train_simple/
SP envs: 30/30
Other agent actions took 0.7942330837249756 seconds
Total simulation time for 400 steps: 4.503757953643799 	 Other agent action time: 0 	 88.81471964459747 steps/s
Curr learning rate 0.000494949494949495 	 Curr reward per step 0.48690000000000005

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 158.71it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 150.83it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 156.42it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.10it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.29it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 157.22it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 154.22it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 152.16it/s]
-------------------------------------
| approxkl           | 0.0025203228 |
| clipfrac           | 0.2084271    |
| eplenmean          | 400          |
| eprewmean          | 192          |
| explained_variance | 0.366        |
| fps                | 2380         |
| nupdates           | 51           |
| policy_entropy     | 0.67783725   |
| policy_loss        | 0.0038367235 |
| serial_timesteps   | 20400        |
| time_elapsed       | 283          |
| time_remaining     | 1.39         |
| total_timesteps    | 612000       |
| true_eprew         | 140          |
| value_loss         | 86.58284     |
-------------------------------------
Current reward shaping 0.388
Current self-play randomization 0.9552
SP envs: 29/30
Other agent actions took 6.108391284942627 seconds
Total simulation time for 400 steps: 9.966971158981323 	 Other agent action time: 0 	 40.13255317183862 steps/s
Curr learning rate 0.0004848484848484849 	 Curr reward per step 0.5122756666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 144.32it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.10it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 142.05it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 162.39it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 148.13it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 160.98it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.52it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.25it/s]
-------------------------------------
| approxkl           | 0.0012963375 |
| clipfrac           | 0.16936463   |
| eplenmean          | 400          |
| eprewmean          | 195          |
| explained_variance | 0.33         |
| fps                | 1141         |
| nupdates           | 52           |
| policy_entropy     | 0.75997597   |
| policy_loss        | 0.0010474556 |
| serial_timesteps   | 20800        |
| time_elapsed       | 293          |
| time_remaining     | 1.32         |
| total_timesteps    | 624000       |
| true_eprew         | 143          |
| value_loss         | 86.40179     |
-------------------------------------
Current reward shaping 0.376
Current self-play randomization 0.9504
SP envs: 27/30
Other agent actions took 6.093583106994629 seconds
Total simulation time for 400 steps: 9.94426941871643 	 Other agent action time: 0 	 40.22417164675236 steps/s
Curr learning rate 0.0004747474747474748 	 Curr reward per step 0.4226046666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 154.67it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 160.48it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 150.63it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 128.69it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 144.93it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 156.81it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 144.10it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.58it/s]
-------------------------------------
| approxkl           | 0.001727314  |
| clipfrac           | 0.18654165   |
| eplenmean          | 400          |
| eprewmean          | 191          |
| explained_variance | 0.4          |
| fps                | 1142         |
| nupdates           | 53           |
| policy_entropy     | 0.6813583    |
| policy_loss        | 0.0013578476 |
| serial_timesteps   | 21200        |
| time_elapsed       | 304          |
| time_remaining     | 1.24         |
| total_timesteps    | 636000       |
| true_eprew         | 142          |
| value_loss         | 96.22037     |
-------------------------------------
Current reward shaping 0.364
Current self-play randomization 0.9456
SP envs: 28/30
Other agent actions took 5.977584600448608 seconds
Total simulation time for 400 steps: 9.783780813217163 	 Other agent action time: 0 	 40.883990313808916 steps/s
Curr learning rate 0.0004646464646464647 	 Curr reward per step 0.5271286666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 156.88it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.21it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 153.56it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 157.56it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 151.73it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 140.54it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 156.52it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 158.01it/s]
-------------------------------------
| approxkl           | 0.0010584983 |
| clipfrac           | 0.13953124   |
| eplenmean          | 400          |
| eprewmean          | 195          |
| explained_variance | 0.253        |
| fps                | 1161         |
| nupdates           | 54           |
| policy_entropy     | 0.6547593    |
| policy_loss        | 0.0008271278 |
| serial_timesteps   | 21600        |
| time_elapsed       | 314          |
| time_remaining     | 1.16         |
| total_timesteps    | 648000       |
| true_eprew         | 146          |
| value_loss         | 83.59702     |
-------------------------------------
Current reward shaping 0.352
Current self-play randomization 0.9408
SP envs: 29/30
Other agent actions took 6.041220188140869 seconds
Total simulation time for 400 steps: 9.81679081916809 	 Other agent action time: 0 	 40.746513536681164 steps/s
Curr learning rate 0.00045454545454545455 	 Curr reward per step 0.5435733333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 110.04it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 102.72it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 100.36it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 101.20it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 102.37it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 103.87it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 102.96it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 104.09it/s]
-------------------------------------
| approxkl           | 0.0024693827 |
| clipfrac           | 0.2446146    |
| eplenmean          | 400          |
| eprewmean          | 201          |
| explained_variance | 0.318        |
| fps                | 1129         |
| nupdates           | 55           |
| policy_entropy     | 0.788808     |
| policy_loss        | 0.0035089173 |
| serial_timesteps   | 22000        |
| time_elapsed       | 325          |
| time_remaining     | 1.08         |
| total_timesteps    | 660000       |
| true_eprew         | 151          |
| value_loss         | 80.21064     |
-------------------------------------
Current reward shaping 0.33999999999999997
Current self-play randomization 0.9359999999999999
SP envs: 30/30
Other agent actions took 0.7544059753417969 seconds
Total simulation time for 400 steps: 4.457014799118042 	 Other agent action time: 0 	 89.74616823779726 steps/s
Curr learning rate 0.0004444444444444444 	 Curr reward per step 0.5943333333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 135.59it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.09it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 145.87it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 156.28it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.40it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 147.04it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 159.69it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 140.74it/s]
--------------------------------------
| approxkl           | 0.0008317883  |
| clipfrac           | 0.14760414    |
| eplenmean          | 400           |
| eprewmean          | 216           |
| explained_variance | 0.232         |
| fps                | 2390          |
| nupdates           | 56            |
| policy_entropy     | 0.7968895     |
| policy_loss        | 0.00031295483 |
| serial_timesteps   | 22400         |
| time_elapsed       | 330           |
| time_remaining     | 0.982         |
| total_timesteps    | 672000        |
| true_eprew         | 165           |
| value_loss         | 68.75021      |
--------------------------------------
Current reward shaping 0.32799999999999996
Current self-play randomization 0.9312
SP envs: 26/30
Other agent actions took 6.067095518112183 seconds
Total simulation time for 400 steps: 9.891516923904419 	 Other agent action time: 0 	 40.438691363236366 steps/s
Curr learning rate 0.00043434343434343433 	 Curr reward per step 0.5224786666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.98it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 153.83it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 157.88it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 137.47it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 159.73it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 150.51it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 164.79it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 161.26it/s]
--------------------------------------
| approxkl           | 0.00087227486 |
| clipfrac           | 0.12776043    |
| eplenmean          | 400           |
| eprewmean          | 221           |
| explained_variance | 0.376         |
| fps                | 1149          |
| nupdates           | 57            |
| policy_entropy     | 0.7187263     |
| policy_loss        | 5.633911e-06  |
| serial_timesteps   | 22800         |
| time_elapsed       | 340           |
| time_remaining     | 0.895         |
| total_timesteps    | 684000        |
| true_eprew         | 169           |
| value_loss         | 81.81107      |
--------------------------------------
Current reward shaping 0.31599999999999995
Current self-play randomization 0.9264
SP envs: 29/30
Other agent actions took 6.068723678588867 seconds
Total simulation time for 400 steps: 9.873127937316895 	 Other agent action time: 0 	 40.5140095965072 steps/s
Curr learning rate 0.00042424242424242425 	 Curr reward per step 0.5563516666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.75it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 161.01it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 160.67it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 158.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 140.41it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 140.94it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 145.34it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 151.87it/s]
-------------------------------------
| approxkl           | 0.0021546031 |
| clipfrac           | 0.23746875   |
| eplenmean          | 400          |
| eprewmean          | 222          |
| explained_variance | 0.301        |
| fps                | 1150         |
| nupdates           | 58           |
| policy_entropy     | 0.79034144   |
| policy_loss        | 0.0032117541 |
| serial_timesteps   | 23200        |
| time_elapsed       | 351          |
| time_remaining     | 0.806        |
| total_timesteps    | 696000       |
| true_eprew         | 172          |
| value_loss         | 74.59105     |
-------------------------------------
Current reward shaping 0.30400000000000005
Current self-play randomization 0.9216
SP envs: 27/30
Other agent actions took 5.8786914348602295 seconds
Total simulation time for 400 steps: 9.645919561386108 	 Other agent action time: 0 	 41.46831180318493 steps/s
Curr learning rate 0.0004141414141414141 	 Curr reward per step 0.55398

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 143.28it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.02it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.51it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 163.11it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 156.79it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 152.92it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 151.58it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 163.84it/s]
-------------------------------------
| approxkl           | 0.0024045226 |
| clipfrac           | 0.23253123   |
| eplenmean          | 400          |
| eprewmean          | 219          |
| explained_variance | 0.363        |
| fps                | 1178         |
| nupdates           | 59           |
| policy_entropy     | 0.7125502    |
| policy_loss        | 0.0040031564 |
| serial_timesteps   | 23600        |
| time_elapsed       | 361          |
| time_remaining     | 0.714        |
| total_timesteps    | 708000       |
| true_eprew         | 171          |
| value_loss         | 77.89168     |
-------------------------------------
Current reward shaping 0.29200000000000004
Current self-play randomization 0.9168000000000001
SP envs: 28/30
Other agent actions took 6.029114007949829 seconds
Total simulation time for 400 steps: 9.851139783859253 	 Other agent action time: 0 	 40.604438549880896 steps/s
Curr learning rate 0.00040404040404040404 	 Curr reward per step 0.5836613333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 150.22it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.55it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 158.91it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.89it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.10it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 155.60it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 147.71it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 163.67it/s]
-------------------------------------
| approxkl           | 0.0014399517 |
| clipfrac           | 0.20837498   |
| eplenmean          | 400          |
| eprewmean          | 226          |
| explained_variance | 0.366        |
| fps                | 1155         |
| nupdates           | 60           |
| policy_entropy     | 0.78087515   |
| policy_loss        | 0.0014199475 |
| serial_timesteps   | 24000        |
| time_elapsed       | 371          |
| time_remaining     | 0.619        |
| total_timesteps    | 720000       |
| true_eprew         | 179          |
| value_loss         | 65.48482     |
-------------------------------------
Current reward shaping 0.28
Current self-play randomization 0.912
SP envs: 28/30
Other agent actions took 6.046235084533691 seconds
Total simulation time for 400 steps: 9.913356304168701 	 Other agent action time: 0 	 40.349603880554014 steps/s
Curr learning rate 0.00039393939393939396 	 Curr reward per step 0.56257

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 118.52it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 144.49it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 155.73it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 147.54it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.62it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 155.63it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 157.51it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 149.97it/s]
---------------------------------------
| approxkl           | 0.00087651855  |
| clipfrac           | 0.13227083     |
| eplenmean          | 400            |
| eprewmean          | 227            |
| explained_variance | 0.223          |
| fps                | 1144           |
| nupdates           | 61             |
| policy_entropy     | 0.67525375     |
| policy_loss        | -0.00018197371 |
| serial_timesteps   | 24400          |
| time_elapsed       | 382            |
| time_remaining     | 0.522          |
| total_timesteps    | 732000         |
| true_eprew         | 181            |
| value_loss         | 77.081696      |
---------------------------------------
Current reward shaping 0.268
Current self-play randomization 0.9072
SP envs: 27/30
Other agent actions took 6.0705602169036865 seconds
Total simulation time for 400 steps: 9.937248706817627 	 Other agent action time: 0 	 40.252590208955205 steps/s
Curr learning rate 0.0003838383838383839 	 Curr reward per step 0.5272923333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 143.97it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 158.76it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 115.60it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 113.27it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 106.89it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 114.03it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 113.40it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 114.65it/s]
--------------------------------------
| approxkl           | 0.00064447377 |
| clipfrac           | 0.12104167    |
| eplenmean          | 400           |
| eprewmean          | 219           |
| explained_variance | 0.499         |
| fps                | 1128          |
| nupdates           | 62            |
| policy_entropy     | 0.8018039     |
| policy_loss        | -0.0004566825 |
| serial_timesteps   | 24800         |
| time_elapsed       | 392           |
| time_remaining     | 0.422         |
| total_timesteps    | 744000        |
| true_eprew         | 176           |
| value_loss         | 71.64491      |
--------------------------------------
Current reward shaping 0.256
Current self-play randomization 0.9024
SP envs: 27/30
Other agent actions took 5.974480867385864 seconds
Total simulation time for 400 steps: 9.71007752418518 	 Other agent action time: 0 	 41.19431580270168 steps/s
Curr learning rate 0.0003737373737373737 	 Curr reward per step 0.56304

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 141.38it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.65it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.62it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.22it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 159.74it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 162.70it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 151.14it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 158.63it/s]
--------------------------------------
| approxkl           | 0.00062453025 |
| clipfrac           | 0.11611457    |
| eplenmean          | 400           |
| eprewmean          | 224           |
| explained_variance | 0.224         |
| fps                | 1171          |
| nupdates           | 63            |
| policy_entropy     | 0.79412764    |
| policy_loss        | 0.00018516017 |
| serial_timesteps   | 25200         |
| time_elapsed       | 403           |
| time_remaining     | 0.32          |
| total_timesteps    | 756000        |
| true_eprew         | 181           |
| value_loss         | 78.2772       |
--------------------------------------
Current reward shaping 0.244
Current self-play randomization 0.8976
SP envs: 27/30
Other agent actions took 6.030796051025391 seconds
Total simulation time for 400 steps: 9.823453903198242 	 Other agent action time: 0 	 40.71887585992246 steps/s
Curr learning rate 0.0003636363636363636 	 Curr reward per step 0.5431686666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 152.20it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 126.81it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 140.77it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.70it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 145.44it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 146.41it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 149.57it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.34it/s]
---------------------------------------
| approxkl           | 0.00074442325  |
| clipfrac           | 0.124625005    |
| eplenmean          | 400            |
| eprewmean          | 218            |
| explained_variance | 0.241          |
| fps                | 1154           |
| nupdates           | 64             |
| policy_entropy     | 0.79708827     |
| policy_loss        | -0.00011263008 |
| serial_timesteps   | 25600          |
| time_elapsed       | 413            |
| time_remaining     | 0.215          |
| total_timesteps    | 768000         |
| true_eprew         | 178            |
| value_loss         | 77.88753       |
---------------------------------------
Current reward shaping 0.23199999999999998
Current self-play randomization 0.8928
SP envs: 29/30
Other agent actions took 6.125627517700195 seconds
Total simulation time for 400 steps: 9.868339776992798 	 Other agent action time: 0 	 40.53366716583536 steps/s
Curr learning rate 0.00035353535353535354 	 Curr reward per step 0.57901

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 155.35it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 160.97it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.10it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 149.50it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 165.17it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 166.98it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 154.48it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 167.23it/s]
-------------------------------------
| approxkl           | 0.0038121287 |
| clipfrac           | 0.26986462   |
| eplenmean          | 400          |
| eprewmean          | 226          |
| explained_variance | 0.292        |
| fps                | 1154         |
| nupdates           | 65           |
| policy_entropy     | 0.7889105    |
| policy_loss        | 0.0056742667 |
| serial_timesteps   | 26000        |
| time_elapsed       | 423          |
| time_remaining     | 0.109        |
| total_timesteps    | 780000       |
| true_eprew         | 186          |
| value_loss         | 67.54009     |
-------------------------------------
Current reward shaping 0.21999999999999997
Current self-play randomization 0.888
SP envs: 29/30
Other agent actions took 5.980889558792114 seconds
Total simulation time for 400 steps: 9.705026149749756 	 Other agent action time: 0 	 41.21575705494766 steps/s
Curr learning rate 0.0003434343434343434 	 Curr reward per step 0.5509316666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 132.36it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 158.32it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 157.93it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 153.74it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 151.71it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 154.63it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 149.64it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 159.75it/s]
------------------------------------
| approxkl           | 0.001123439 |
| clipfrac           | 0.14241664  |
| eplenmean          | 400         |
| eprewmean          | 224         |
| explained_variance | 0.29        |
| fps                | 1169        |
| nupdates           | 66          |
| policy_entropy     | 0.68598676  |
| policy_loss        | 0.000891444 |
| serial_timesteps   | 26400       |
| time_elapsed       | 434         |
| time_remaining     | 0           |
| total_timesteps    | 792000      |
| true_eprew         | 186         |
| value_loss         | 72.61058    |
------------------------------------
Current reward shaping 0.20799999999999996
Current self-play randomization 0.8832
LOADING BC MODEL FROM: seed4/worker4
Loading a model without an environment, this model cannot be trained until it has a valid environment.
Loaded MediumLevelPlanner from /home/ubuntu/human_aware_rl/overcooked_ai/overcooked_ai_py/data/planners/simple_am.pkl
TOT NUM UPDATES 66
SP envs: 27/30
Other agent actions took 6.024054288864136 seconds
Total simulation time for 400 steps: 9.853477239608765 	 Other agent action time: 0 	 40.59480630777629 steps/s
Curr learning rate 0.001 	 Curr reward per step 0.5365679999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.18it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.33it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 155.55it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.58it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 154.49it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 156.73it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 158.84it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 137.65it/s]
------------------------------------
| approxkl           | 0.015084861 |
| clipfrac           | 0.44448954  |
| eplenmean          | 400         |
| eprewmean          | 215         |
| explained_variance | 0.222       |
| fps                | 1153        |
| nupdates           | 1           |
| policy_entropy     | 0.7652129   |
| policy_loss        | 0.01956614  |
| serial_timesteps   | 400         |
| time_elapsed       | 10.4        |
| time_remaining     | 11.3        |
| total_timesteps    | 12000       |
| true_eprew         | 181         |
| value_loss         | 81.189545   |
------------------------------------
Current reward shaping 0.988
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7652268409729004 seconds
Total simulation time for 400 steps: 4.556285381317139 	 Other agent action time: 0 	 87.79081346400811 steps/s
Curr learning rate 0.00098989898989899 	 Curr reward per step 0.8995283333333336

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 164.14it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.59it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 147.40it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 163.41it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.95it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 152.83it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 143.21it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 161.83it/s]
-------------------------------------
| approxkl           | 0.006757184  |
| clipfrac           | 0.32054156   |
| eplenmean          | 400          |
| eprewmean          | 287          |
| explained_variance | 0.247        |
| fps                | 2355         |
| nupdates           | 2            |
| policy_entropy     | 0.69699556   |
| policy_loss        | 0.0096106855 |
| serial_timesteps   | 800          |
| time_elapsed       | 15.5         |
| time_remaining     | 8.26         |
| total_timesteps    | 24000        |
| true_eprew         | 187          |
| value_loss         | 117.642685   |
-------------------------------------
Current reward shaping 0.976
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8105289936065674 seconds
Total simulation time for 400 steps: 4.51322340965271 	 Other agent action time: 0 	 88.62845104111072 steps/s
Curr learning rate 0.0009797979797979799 	 Curr reward per step 0.8893093333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 150.24it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 158.45it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 138.71it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 146.68it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 154.08it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 177.04it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.17it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 169.16it/s]
------------------------------------
| approxkl           | 0.008564595 |
| clipfrac           | 0.37230211  |
| eplenmean          | 400         |
| eprewmean          | 310         |
| explained_variance | 0.281       |
| fps                | 2375        |
| nupdates           | 3           |
| policy_entropy     | 0.78631955  |
| policy_loss        | 0.011049335 |
| serial_timesteps   | 1200        |
| time_elapsed       | 20.5        |
| time_remaining     | 7.19        |
| total_timesteps    | 36000       |
| true_eprew         | 189         |
| value_loss         | 118.96755   |
------------------------------------
Current reward shaping 0.964
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7557835578918457 seconds
Total simulation time for 400 steps: 4.527623653411865 	 Other agent action time: 0 	 88.34656557608834 steps/s
Curr learning rate 0.0009696969696969698 	 Curr reward per step 0.8221243333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 164.70it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.15it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 155.97it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.05it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 148.27it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.95it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 149.74it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 142.92it/s]
-------------------------------------
| approxkl           | 0.0071340636 |
| clipfrac           | 0.34569794   |
| eplenmean          | 400          |
| eprewmean          | 337          |
| explained_variance | 0.316        |
| fps                | 2367         |
| nupdates           | 4            |
| policy_entropy     | 0.7476548    |
| policy_loss        | 0.008711592  |
| serial_timesteps   | 1600         |
| time_elapsed       | 25.6         |
| time_remaining     | 6.62         |
| total_timesteps    | 48000        |
| true_eprew         | 190          |
| value_loss         | 112.17589    |
-------------------------------------
Current reward shaping 0.952
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.795149564743042 seconds
Total simulation time for 400 steps: 4.5483973026275635 	 Other agent action time: 0 	 87.94306508117134 steps/s
Curr learning rate 0.0009595959595959597 	 Curr reward per step 0.8365946666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 156.40it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 167.19it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.00it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 162.25it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 171.19it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 165.79it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.73it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 179.01it/s]
-------------------------------------
| approxkl           | 0.0045224163 |
| clipfrac           | 0.30377084   |
| eplenmean          | 400          |
| eprewmean          | 342          |
| explained_variance | 0.297        |
| fps                | 2373         |
| nupdates           | 5            |
| policy_entropy     | 0.7644488    |
| policy_loss        | 0.007292344  |
| serial_timesteps   | 2000         |
| time_elapsed       | 30.7         |
| time_remaining     | 6.24         |
| total_timesteps    | 60000        |
| true_eprew         | 186          |
| value_loss         | 108.38682    |
-------------------------------------
Current reward shaping 0.94
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7527375221252441 seconds
Total simulation time for 400 steps: 4.479378938674927 	 Other agent action time: 0 	 89.29809365901214 steps/s
Curr learning rate 0.0009494949494949496 	 Curr reward per step 0.8670233333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 146.47it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.99it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 163.87it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 170.76it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 166.36it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 170.11it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.91it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 160.41it/s]
------------------------------------
| approxkl           | 0.017147955 |
| clipfrac           | 0.43927082  |
| eplenmean          | 400         |
| eprewmean          | 339         |
| explained_variance | 0.318       |
| fps                | 2399        |
| nupdates           | 6           |
| policy_entropy     | 0.7869762   |
| policy_loss        | 0.019694703 |
| serial_timesteps   | 2400        |
| time_elapsed       | 35.7        |
| time_remaining     | 5.95        |
| total_timesteps    | 72000       |
| true_eprew         | 185         |
| value_loss         | 107.254845  |
------------------------------------
Current reward shaping 0.928
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.761070966720581 seconds
Total simulation time for 400 steps: 4.482558965682983 	 Other agent action time: 0 	 89.23474360566591 steps/s
Curr learning rate 0.0009393939393939395 	 Curr reward per step 0.8848373333333331

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 114.61it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 117.17it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 117.46it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 116.25it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 132.29it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 117.44it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 125.69it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 123.99it/s]
-------------------------------------
| approxkl           | 0.0055344333 |
| clipfrac           | 0.35741666   |
| eplenmean          | 400          |
| eprewmean          | 344          |
| explained_variance | 0.448        |
| fps                | 2316         |
| nupdates           | 7            |
| policy_entropy     | 0.7210133    |
| policy_loss        | 0.00939968   |
| serial_timesteps   | 2800         |
| time_elapsed       | 40.9         |
| time_remaining     | 5.74         |
| total_timesteps    | 84000        |
| true_eprew         | 190          |
| value_loss         | 112.7155     |
-------------------------------------
Current reward shaping 0.916
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.773179292678833 seconds
Total simulation time for 400 steps: 4.554614543914795 	 Other agent action time: 0 	 87.82301908169619 steps/s
Curr learning rate 0.0009292929292929292 	 Curr reward per step 0.8945543333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.84it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.62it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 151.70it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.89it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 158.75it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 170.71it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 174.82it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 159.49it/s]
-------------------------------------
| approxkl           | 0.0064692283 |
| clipfrac           | 0.36878124   |
| eplenmean          | 400          |
| eprewmean          | 350          |
| explained_variance | 0.265        |
| fps                | 2367         |
| nupdates           | 8            |
| policy_entropy     | 0.8058364    |
| policy_loss        | 0.009999633  |
| serial_timesteps   | 3200         |
| time_elapsed       | 45.9         |
| time_remaining     | 5.55         |
| total_timesteps    | 96000        |
| true_eprew         | 195          |
| value_loss         | 116.89402    |
-------------------------------------
Current reward shaping 0.904
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7457122802734375 seconds
Total simulation time for 400 steps: 4.466700792312622 	 Other agent action time: 0 	 89.55155462582509 steps/s
Curr learning rate 0.0009191919191919192 	 Curr reward per step 0.8726086666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.54it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.47it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.95it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 137.96it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 146.89it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 151.12it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 133.80it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 147.32it/s]
------------------------------------
| approxkl           | 0.006331312 |
| clipfrac           | 0.380323    |
| eplenmean          | 400         |
| eprewmean          | 353         |
| explained_variance | 0.349       |
| fps                | 2385        |
| nupdates           | 9           |
| policy_entropy     | 0.7948568   |
| policy_loss        | 0.011948771 |
| serial_timesteps   | 3600        |
| time_elapsed       | 51          |
| time_remaining     | 5.38        |
| total_timesteps    | 108000      |
| true_eprew         | 197         |
| value_loss         | 115.17914   |
------------------------------------
Current reward shaping 0.892
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.808056116104126 seconds
Total simulation time for 400 steps: 4.638284921646118 	 Other agent action time: 0 	 86.23877289928123 steps/s
Curr learning rate 0.0009090909090909091 	 Curr reward per step 0.8578873333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 143.64it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 156.19it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.57it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 156.95it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.05it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 141.31it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 147.29it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 169.55it/s]
------------------------------------
| approxkl           | 0.006521725 |
| clipfrac           | 0.38403124  |
| eplenmean          | 400         |
| eprewmean          | 350         |
| explained_variance | 0.277       |
| fps                | 2317        |
| nupdates           | 10          |
| policy_entropy     | 0.8054349   |
| policy_loss        | 0.01108617  |
| serial_timesteps   | 4000        |
| time_elapsed       | 56.1        |
| time_remaining     | 5.24        |
| total_timesteps    | 120000      |
| true_eprew         | 197         |
| value_loss         | 116.12228   |
------------------------------------
Current reward shaping 0.88
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7553417682647705 seconds
Total simulation time for 400 steps: 4.463479995727539 	 Other agent action time: 0 	 89.61617401285132 steps/s
Curr learning rate 0.000898989898989899 	 Curr reward per step 0.82876

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.85it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 170.60it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.02it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 162.77it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.99it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.25it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 173.26it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 133.51it/s]
------------------------------------
| approxkl           | 0.014595416 |
| clipfrac           | 0.4662396   |
| eplenmean          | 400         |
| eprewmean          | 344         |
| explained_variance | 0.376       |
| fps                | 2410        |
| nupdates           | 11          |
| policy_entropy     | 0.81277466  |
| policy_loss        | 0.022270393 |
| serial_timesteps   | 4400        |
| time_elapsed       | 61.1        |
| time_remaining     | 5.09        |
| total_timesteps    | 132000      |
| true_eprew         | 194         |
| value_loss         | 107.331116  |
------------------------------------
Current reward shaping 0.868
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8121016025543213 seconds
Total simulation time for 400 steps: 4.616449594497681 	 Other agent action time: 0 	 86.64667333891344 steps/s
Curr learning rate 0.0008888888888888889 	 Curr reward per step 0.7430526666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.48it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 174.48it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 171.67it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 171.32it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 166.38it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 173.67it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 158.74it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 158.81it/s]
-------------------------------------
| approxkl           | 0.0078107915 |
| clipfrac           | 0.40727082   |
| eplenmean          | 400          |
| eprewmean          | 329          |
| explained_variance | 0.382        |
| fps                | 2341         |
| nupdates           | 12           |
| policy_entropy     | 0.8183969    |
| policy_loss        | 0.012194361  |
| serial_timesteps   | 4800         |
| time_elapsed       | 66.2         |
| time_remaining     | 4.97         |
| total_timesteps    | 144000       |
| true_eprew         | 188          |
| value_loss         | 96.1554      |
-------------------------------------
Current reward shaping 0.856
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7367827892303467 seconds
Total simulation time for 400 steps: 4.479821681976318 	 Other agent action time: 0 	 89.2892682780927 steps/s
Curr learning rate 0.0008787878787878789 	 Curr reward per step 0.7622840000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.68it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.30it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 167.29it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.37it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 162.54it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 166.14it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 155.80it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 146.90it/s]
-----------------------------------
| approxkl           | 0.00284747 |
| clipfrac           | 0.2950521  |
| eplenmean          | 400        |
| eprewmean          | 315        |
| explained_variance | 0.439      |
| fps                | 2398       |
| nupdates           | 13         |
| policy_entropy     | 0.7862165  |
| policy_loss        | 0.00492374 |
| serial_timesteps   | 5200       |
| time_elapsed       | 71.2       |
| time_remaining     | 4.84       |
| total_timesteps    | 156000     |
| true_eprew         | 181        |
| value_loss         | 92.31315   |
-----------------------------------
Current reward shaping 0.844
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7781219482421875 seconds
Total simulation time for 400 steps: 4.559230804443359 	 Other agent action time: 0 	 87.73409751710001 steps/s
Curr learning rate 0.0008686868686868688 	 Curr reward per step 0.8259633333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 156.67it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.84it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.86it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 151.19it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 162.40it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 165.72it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 141.99it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 162.61it/s]
-------------------------------------
| approxkl           | 0.004752736  |
| clipfrac           | 0.32301039   |
| eplenmean          | 400          |
| eprewmean          | 313          |
| explained_variance | 0.279        |
| fps                | 2355         |
| nupdates           | 14           |
| policy_entropy     | 0.7727934    |
| policy_loss        | 0.0077033928 |
| serial_timesteps   | 5600         |
| time_elapsed       | 76.3         |
| time_remaining     | 4.73         |
| total_timesteps    | 168000       |
| true_eprew         | 180          |
| value_loss         | 97.31046     |
-------------------------------------
Current reward shaping 0.832
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8085892200469971 seconds
Total simulation time for 400 steps: 4.600335597991943 	 Other agent action time: 0 	 86.95017819452148 steps/s
Curr learning rate 0.0008585858585858587 	 Curr reward per step 0.8009200000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 165.14it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.88it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.69it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.21it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.69it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 170.45it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 154.45it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 180.20it/s]
------------------------------------
| approxkl           | 0.013086705 |
| clipfrac           | 0.4227709   |
| eplenmean          | 400         |
| eprewmean          | 317         |
| explained_variance | 0.322       |
| fps                | 2352        |
| nupdates           | 15          |
| policy_entropy     | 0.8214625   |
| policy_loss        | 0.017930763 |
| serial_timesteps   | 6000        |
| time_elapsed       | 81.4        |
| time_remaining     | 4.62        |
| total_timesteps    | 180000      |
| true_eprew         | 183         |
| value_loss         | 100.78745   |
------------------------------------
Current reward shaping 0.8200000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7516841888427734 seconds
Total simulation time for 400 steps: 4.499231576919556 	 Other agent action time: 0 	 88.90407020877642 steps/s
Curr learning rate 0.0008484848484848486 	 Curr reward per step 0.7265699999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 154.59it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 165.86it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 140.84it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 150.41it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.67it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 166.70it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.21it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 171.49it/s]
-------------------------------------
| approxkl           | 0.0050060274 |
| clipfrac           | 0.33264583   |
| eplenmean          | 400          |
| eprewmean          | 311          |
| explained_variance | 0.409        |
| fps                | 2386         |
| nupdates           | 16           |
| policy_entropy     | 0.7872044    |
| policy_loss        | 0.008413264  |
| serial_timesteps   | 6400         |
| time_elapsed       | 86.5         |
| time_remaining     | 4.5          |
| total_timesteps    | 192000       |
| true_eprew         | 180          |
| value_loss         | 97.249214    |
-------------------------------------
Current reward shaping 0.808
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8087337017059326 seconds
Total simulation time for 400 steps: 4.580623388290405 	 Other agent action time: 0 	 87.32435873740086 steps/s
Curr learning rate 0.0008383838383838385 	 Curr reward per step 0.7278486666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.99it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 158.93it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.48it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.53it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.44it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 168.98it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 161.39it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.97it/s]
-------------------------------------
| approxkl           | 0.0078882575 |
| clipfrac           | 0.3864896    |
| eplenmean          | 400          |
| eprewmean          | 305          |
| explained_variance | 0.33         |
| fps                | 2356         |
| nupdates           | 17           |
| policy_entropy     | 0.7947576    |
| policy_loss        | 0.011948388  |
| serial_timesteps   | 6800         |
| time_elapsed       | 91.6         |
| time_remaining     | 4.4          |
| total_timesteps    | 204000       |
| true_eprew         | 178          |
| value_loss         | 98.43211     |
-------------------------------------
Current reward shaping 0.796
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.749330997467041 seconds
Total simulation time for 400 steps: 4.449648857116699 	 Other agent action time: 0 	 89.89473390922662 steps/s
Curr learning rate 0.0008282828282828282 	 Curr reward per step 0.7292959999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.22it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 170.45it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.10it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 133.78it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 170.09it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.26it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 163.43it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 153.18it/s]
------------------------------------
| approxkl           | 0.008741551 |
| clipfrac           | 0.34909374  |
| eplenmean          | 400         |
| eprewmean          | 295         |
| explained_variance | 0.377       |
| fps                | 2407        |
| nupdates           | 18          |
| policy_entropy     | 0.8079731   |
| policy_loss        | 0.011006594 |
| serial_timesteps   | 7200        |
| time_elapsed       | 96.6        |
| time_remaining     | 4.29        |
| total_timesteps    | 216000      |
| true_eprew         | 172         |
| value_loss         | 89.25862    |
------------------------------------
Current reward shaping 0.784
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7500123977661133 seconds
Total simulation time for 400 steps: 4.405015468597412 	 Other agent action time: 0 	 90.80558351078 steps/s
Curr learning rate 0.0008181818181818183 	 Curr reward per step 0.7464799999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.97it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 143.43it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 154.40it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 165.10it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.13it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 148.81it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 99.16it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 98.79it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 102.44it/s]
-------------------------------------
| approxkl           | 0.0037027553 |
| clipfrac           | 0.26366672   |
| eplenmean          | 400          |
| eprewmean          | 294          |
| explained_variance | 0.381        |
| fps                | 2389         |
| nupdates           | 19           |
| policy_entropy     | 0.76040256   |
| policy_loss        | 0.0052907635 |
| serial_timesteps   | 7600         |
| time_elapsed       | 102          |
| time_remaining     | 4.19         |
| total_timesteps    | 228000       |
| true_eprew         | 173          |
| value_loss         | 97.05458     |
-------------------------------------
Current reward shaping 0.772
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8025951385498047 seconds
Total simulation time for 400 steps: 4.530831575393677 	 Other agent action time: 0 	 88.2840143898407 steps/s
Curr learning rate 0.0008080808080808081 	 Curr reward per step 0.7477929999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.60it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 156.56it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 165.76it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.52it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 170.91it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 159.70it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 173.40it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 179.29it/s]
-------------------------------------
| approxkl           | 0.0059752953 |
| clipfrac           | 0.32398954   |
| eplenmean          | 400          |
| eprewmean          | 298          |
| explained_variance | 0.347        |
| fps                | 2382         |
| nupdates           | 20           |
| policy_entropy     | 0.7462881    |
| policy_loss        | 0.008670856  |
| serial_timesteps   | 8000         |
| time_elapsed       | 107          |
| time_remaining     | 4.09         |
| total_timesteps    | 240000       |
| true_eprew         | 177          |
| value_loss         | 92.41603     |
-------------------------------------
Current reward shaping 0.76
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7613532543182373 seconds
Total simulation time for 400 steps: 4.5178868770599365 	 Other agent action time: 0 	 88.5369667025183 steps/s
Curr learning rate 0.000797979797979798 	 Curr reward per step 0.7120233333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.39it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.25it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.46it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 171.23it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.18it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 162.41it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 143.01it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.39it/s]
------------------------------------
| approxkl           | 0.007858118 |
| clipfrac           | 0.4379167   |
| eplenmean          | 400         |
| eprewmean          | 293         |
| explained_variance | 0.364       |
| fps                | 2378        |
| nupdates           | 21          |
| policy_entropy     | 0.8119237   |
| policy_loss        | 0.012898083 |
| serial_timesteps   | 8400        |
| time_elapsed       | 112         |
| time_remaining     | 3.99        |
| total_timesteps    | 252000      |
| true_eprew         | 176         |
| value_loss         | 95.68248    |
------------------------------------
Current reward shaping 0.748
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7972006797790527 seconds
Total simulation time for 400 steps: 4.5525476932525635 	 Other agent action time: 0 	 87.86289061679668 steps/s
Curr learning rate 0.0007878787878787879 	 Curr reward per step 0.7222616666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 163.36it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 170.09it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.04it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 153.02it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 170.18it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 170.79it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 171.89it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 169.88it/s]
------------------------------------
| approxkl           | 0.010332758 |
| clipfrac           | 0.39541674  |
| eplenmean          | 400         |
| eprewmean          | 288         |
| explained_variance | 0.357       |
| fps                | 2371        |
| nupdates           | 22          |
| policy_entropy     | 0.80339926  |
| policy_loss        | 0.015482096 |
| serial_timesteps   | 8800        |
| time_elapsed       | 117         |
| time_remaining     | 3.89        |
| total_timesteps    | 264000      |
| true_eprew         | 174         |
| value_loss         | 96.804405   |
------------------------------------
Current reward shaping 0.736
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7550272941589355 seconds
Total simulation time for 400 steps: 4.474315643310547 	 Other agent action time: 0 	 89.39914657072337 steps/s
Curr learning rate 0.0007777777777777778 	 Curr reward per step 0.741856

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 161.95it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 144.16it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 159.76it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.88it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 170.16it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 143.29it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 153.11it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.31it/s]
------------------------------------
| approxkl           | 0.008884231 |
| clipfrac           | 0.35548967  |
| eplenmean          | 400         |
| eprewmean          | 291         |
| explained_variance | 0.431       |
| fps                | 2392        |
| nupdates           | 23          |
| policy_entropy     | 0.75922984  |
| policy_loss        | 0.011248888 |
| serial_timesteps   | 9200        |
| time_elapsed       | 122         |
| time_remaining     | 3.79        |
| total_timesteps    | 276000      |
| true_eprew         | 177         |
| value_loss         | 90.65685    |
------------------------------------
Current reward shaping 0.724
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7805747985839844 seconds
Total simulation time for 400 steps: 4.635836839675903 	 Other agent action time: 0 	 86.28431367053126 steps/s
Curr learning rate 0.0007676767676767678 	 Curr reward per step 0.4821766666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 115.53it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 117.40it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 112.02it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 128.24it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.87it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.34it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 171.52it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 169.88it/s]
------------------------------------
| approxkl           | 0.009667705 |
| clipfrac           | 0.41192707  |
| eplenmean          | 400         |
| eprewmean          | 262         |
| explained_variance | 0.317       |
| fps                | 2291        |
| nupdates           | 24          |
| policy_entropy     | 0.7547134   |
| policy_loss        | 0.012930719 |
| serial_timesteps   | 9600        |
| time_elapsed       | 127         |
| time_remaining     | 3.7         |
| total_timesteps    | 288000      |
| true_eprew         | 160         |
| value_loss         | 113.29291   |
------------------------------------
Current reward shaping 0.712
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.770683765411377 seconds
Total simulation time for 400 steps: 4.54987645149231 	 Other agent action time: 0 	 87.91447509938526 steps/s
Curr learning rate 0.0007575757575757577 	 Curr reward per step 0.46549733333333326

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.64it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 165.73it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 163.77it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 155.33it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 170.01it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 166.33it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 173.12it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 163.57it/s]
------------------------------------
| approxkl           | 0.008358786 |
| clipfrac           | 0.41162497  |
| eplenmean          | 400         |
| eprewmean          | 229         |
| explained_variance | 0.338       |
| fps                | 2371        |
| nupdates           | 25          |
| policy_entropy     | 0.75056994  |
| policy_loss        | 0.013027941 |
| serial_timesteps   | 10000       |
| time_elapsed       | 132         |
| time_remaining     | 3.61        |
| total_timesteps    | 300000      |
| true_eprew         | 140         |
| value_loss         | 108.142044  |
------------------------------------
Current reward shaping 0.7
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7706072330474854 seconds
Total simulation time for 400 steps: 4.5911359786987305 	 Other agent action time: 0 	 87.12440708701735 steps/s
Curr learning rate 0.0007474747474747475 	 Curr reward per step 0.40145

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.33it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 170.12it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 157.24it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 175.27it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 184.44it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 151.60it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.75it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 160.22it/s]
------------------------------------
| approxkl           | 0.042051993 |
| clipfrac           | 0.50815624  |
| eplenmean          | 400         |
| eprewmean          | 194         |
| explained_variance | 0.316       |
| fps                | 2353        |
| nupdates           | 26          |
| policy_entropy     | 0.70427096  |
| policy_loss        | 0.034797844 |
| serial_timesteps   | 10400       |
| time_elapsed       | 137         |
| time_remaining     | 3.52        |
| total_timesteps    | 312000      |
| true_eprew         | 118         |
| value_loss         | 123.750046  |
------------------------------------
Current reward shaping 0.688
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8033421039581299 seconds
Total simulation time for 400 steps: 4.584180116653442 	 Other agent action time: 0 	 87.25660637697833 steps/s
Curr learning rate 0.0007373737373737374 	 Curr reward per step 0.22575466666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.25it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.28it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 164.82it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 169.05it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.81it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 147.96it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 179.59it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 169.11it/s]
------------------------------------
| approxkl           | 0.07271177  |
| clipfrac           | 0.5504584   |
| eplenmean          | 400         |
| eprewmean          | 149         |
| explained_variance | 0.484       |
| fps                | 2351        |
| nupdates           | 27          |
| policy_entropy     | 0.56700623  |
| policy_loss        | 0.052266784 |
| serial_timesteps   | 10800       |
| time_elapsed       | 142         |
| time_remaining     | 3.42        |
| total_timesteps    | 324000      |
| true_eprew         | 90.6        |
| value_loss         | 158.15825   |
------------------------------------
Current reward shaping 0.6759999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7463295459747314 seconds
Total simulation time for 400 steps: 4.446745872497559 	 Other agent action time: 0 	 89.95342020193658 steps/s
Curr learning rate 0.0007272727272727272 	 Curr reward per step 0.30349233333333325

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 161.28it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 151.30it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 181.12it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 173.69it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.56it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.98it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 171.53it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.94it/s]
------------------------------------
| approxkl           | 0.005386447 |
| clipfrac           | 0.2684062   |
| eplenmean          | 400         |
| eprewmean          | 130         |
| explained_variance | 0.426       |
| fps                | 2423        |
| nupdates           | 28          |
| policy_entropy     | 0.56763685  |
| policy_loss        | 0.007953341 |
| serial_timesteps   | 11200       |
| time_elapsed       | 147         |
| time_remaining     | 3.33        |
| total_timesteps    | 336000      |
| true_eprew         | 78.8        |
| value_loss         | 142.88243   |
------------------------------------
Current reward shaping 0.6639999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8117120265960693 seconds
Total simulation time for 400 steps: 4.577090740203857 	 Other agent action time: 0 	 87.39175662097198 steps/s
Curr learning rate 0.0007171717171717171 	 Curr reward per step 0.27101866666666663

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.32it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 169.40it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 169.06it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.57it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 173.80it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 159.17it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.55it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.12it/s]
------------------------------------
| approxkl           | 0.01051723  |
| clipfrac           | 0.37282297  |
| eplenmean          | 400         |
| eprewmean          | 113         |
| explained_variance | 0.379       |
| fps                | 2361        |
| nupdates           | 29          |
| policy_entropy     | 0.6023712   |
| policy_loss        | 0.015689585 |
| serial_timesteps   | 11600       |
| time_elapsed       | 152         |
| time_remaining     | 3.24        |
| total_timesteps    | 348000      |
| true_eprew         | 67.8        |
| value_loss         | 152.0109    |
------------------------------------
Current reward shaping 0.652
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7594623565673828 seconds
Total simulation time for 400 steps: 4.435962200164795 	 Other agent action time: 0 	 90.17209388870359 steps/s
Curr learning rate 0.0007070707070707071 	 Curr reward per step 0.3186083333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 151.60it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 144.78it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 158.62it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 155.21it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 172.24it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 149.70it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.30it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 162.56it/s]
------------------------------------
| approxkl           | 0.01091733  |
| clipfrac           | 0.33495829  |
| eplenmean          | 400         |
| eprewmean          | 116         |
| explained_variance | 0.391       |
| fps                | 2413        |
| nupdates           | 30          |
| policy_entropy     | 0.56662285  |
| policy_loss        | 0.014527781 |
| serial_timesteps   | 12000       |
| time_elapsed       | 157         |
| time_remaining     | 3.15        |
| total_timesteps    | 360000      |
| true_eprew         | 70.2        |
| value_loss         | 146.83798   |
------------------------------------
Current reward shaping 0.64
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7572324275970459 seconds
Total simulation time for 400 steps: 4.5091774463653564 	 Other agent action time: 0 	 88.70797495947335 steps/s
Curr learning rate 0.000696969696969697 	 Curr reward per step 0.45765333333333325

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 161.27it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.40it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 167.39it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 157.21it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.88it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 169.96it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.03it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 166.86it/s]
-------------------------------------
| approxkl           | 0.0019911171 |
| clipfrac           | 0.17305206   |
| eplenmean          | 400          |
| eprewmean          | 138          |
| explained_variance | 0.383        |
| fps                | 2389         |
| nupdates           | 31           |
| policy_entropy     | 0.51057243   |
| policy_loss        | 0.0034683212 |
| serial_timesteps   | 12400        |
| time_elapsed       | 162          |
| time_remaining     | 3.05         |
| total_timesteps    | 372000       |
| true_eprew         | 85.2         |
| value_loss         | 113.33934    |
-------------------------------------
Current reward shaping 0.628
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7985756397247314 seconds
Total simulation time for 400 steps: 4.5888307094573975 	 Other agent action time: 0 	 87.16817536450318 steps/s
Curr learning rate 0.0006868686868686869 	 Curr reward per step 0.42575199999999996

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.15it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.65it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.96it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.67it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 148.81it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.67it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 144.00it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.90it/s]
-------------------------------------
| approxkl           | 0.0019110121 |
| clipfrac           | 0.17610416   |
| eplenmean          | 400          |
| eprewmean          | 155          |
| explained_variance | 0.321        |
| fps                | 2346         |
| nupdates           | 32           |
| policy_entropy     | 0.53460205   |
| policy_loss        | 0.0029445281 |
| serial_timesteps   | 12800        |
| time_elapsed       | 167          |
| time_remaining     | 2.96         |
| total_timesteps    | 384000       |
| true_eprew         | 96.2         |
| value_loss         | 118.408424   |
-------------------------------------
Current reward shaping 0.616
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7691390514373779 seconds
Total simulation time for 400 steps: 4.480627775192261 	 Other agent action time: 0 	 89.27320457518618 steps/s
Curr learning rate 0.0006767676767676768 	 Curr reward per step 0.4259506666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 164.38it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 152.77it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 175.68it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 176.03it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.62it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 165.62it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.36it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 165.94it/s]
-------------------------------------
| approxkl           | 0.0024304145 |
| clipfrac           | 0.18611458   |
| eplenmean          | 400          |
| eprewmean          | 169          |
| explained_variance | 0.446        |
| fps                | 2405         |
| nupdates           | 33           |
| policy_entropy     | 0.5571768    |
| policy_loss        | 0.003807854  |
| serial_timesteps   | 13200        |
| time_elapsed       | 172          |
| time_remaining     | 2.87         |
| total_timesteps    | 396000       |
| true_eprew         | 106          |
| value_loss         | 114.04916    |
-------------------------------------
Current reward shaping 0.604
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8116312026977539 seconds
Total simulation time for 400 steps: 4.590428352355957 	 Other agent action time: 0 	 87.13783753856151 steps/s
Curr learning rate 0.0006666666666666668 	 Curr reward per step 0.4555016666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 148.82it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.62it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 169.05it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 133.67it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 151.35it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 152.60it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.05it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 166.63it/s]
-------------------------------------
| approxkl           | 0.0028142696 |
| clipfrac           | 0.16734372   |
| eplenmean          | 400          |
| eprewmean          | 175          |
| explained_variance | 0.33         |
| fps                | 2340         |
| nupdates           | 34           |
| policy_entropy     | 0.5080867    |
| policy_loss        | 0.0035960735 |
| serial_timesteps   | 13600        |
| time_elapsed       | 178          |
| time_remaining     | 2.78         |
| total_timesteps    | 408000       |
| true_eprew         | 111          |
| value_loss         | 112.19358    |
-------------------------------------
Current reward shaping 0.5920000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7511999607086182 seconds
Total simulation time for 400 steps: 4.4708287715911865 	 Other agent action time: 0 	 89.46887041205972 steps/s
Curr learning rate 0.0006565656565656567 	 Curr reward per step 0.48269466666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 144.64it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 169.24it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.16it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 139.85it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 144.40it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 161.26it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.27it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.53it/s]
-------------------------------------
| approxkl           | 0.0010690151 |
| clipfrac           | 0.12778124   |
| eplenmean          | 400          |
| eprewmean          | 180          |
| explained_variance | 0.35         |
| fps                | 2394         |
| nupdates           | 35           |
| policy_entropy     | 0.5052123    |
| policy_loss        | 0.0012926126 |
| serial_timesteps   | 14000        |
| time_elapsed       | 183          |
| time_remaining     | 2.69         |
| total_timesteps    | 420000       |
| true_eprew         | 116          |
| value_loss         | 106.55757    |
-------------------------------------
Current reward shaping 0.5800000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7608349323272705 seconds
Total simulation time for 400 steps: 4.49070143699646 	 Other agent action time: 0 	 89.07294453036143 steps/s
Curr learning rate 0.0006464646464646465 	 Curr reward per step 0.462085

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.18it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 136.52it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 147.32it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 169.26it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.95it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 132.95it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 118.89it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 116.52it/s]
-------------------------------------
| approxkl           | 0.0033137412 |
| clipfrac           | 0.29203123   |
| eplenmean          | 400          |
| eprewmean          | 187          |
| explained_variance | 0.389        |
| fps                | 2358         |
| nupdates           | 36           |
| policy_entropy     | 0.7033092    |
| policy_loss        | 0.004947752  |
| serial_timesteps   | 14400        |
| time_elapsed       | 188          |
| time_remaining     | 2.61         |
| total_timesteps    | 432000       |
| true_eprew         | 121          |
| value_loss         | 106.40415    |
-------------------------------------
Current reward shaping 0.5680000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7825140953063965 seconds
Total simulation time for 400 steps: 4.483751535415649 	 Other agent action time: 0 	 89.21100931675946 steps/s
Curr learning rate 0.0006363636363636364 	 Curr reward per step 0.468452

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 146.91it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.30it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 169.88it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.21it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 175.96it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 175.83it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.85it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 163.38it/s]
-------------------------------------
| approxkl           | 0.0020444987 |
| clipfrac           | 0.19976042   |
| eplenmean          | 400          |
| eprewmean          | 190          |
| explained_variance | 0.389        |
| fps                | 2402         |
| nupdates           | 37           |
| policy_entropy     | 0.65490264   |
| policy_loss        | 0.0026152153 |
| serial_timesteps   | 14800        |
| time_elapsed       | 193          |
| time_remaining     | 2.52         |
| total_timesteps    | 444000       |
| true_eprew         | 124          |
| value_loss         | 107.18551    |
-------------------------------------
Current reward shaping 0.556
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7512626647949219 seconds
Total simulation time for 400 steps: 4.45089316368103 	 Other agent action time: 0 	 89.86960263705527 steps/s
Curr learning rate 0.0006262626262626263 	 Curr reward per step 0.561076

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.41it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 171.38it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 169.63it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 142.29it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 170.18it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.79it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 174.45it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 173.82it/s]
-------------------------------------
| approxkl           | 0.0019261163 |
| clipfrac           | 0.17110415   |
| eplenmean          | 400          |
| eprewmean          | 197          |
| explained_variance | 0.307        |
| fps                | 2419         |
| nupdates           | 38           |
| policy_entropy     | 0.5380953    |
| policy_loss        | 0.0027461995 |
| serial_timesteps   | 15200        |
| time_elapsed       | 198          |
| time_remaining     | 2.43         |
| total_timesteps    | 456000       |
| true_eprew         | 130          |
| value_loss         | 94.94019     |
-------------------------------------
Current reward shaping 0.544
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7840921878814697 seconds
Total simulation time for 400 steps: 4.477035999298096 	 Other agent action time: 0 	 89.3448254744236 steps/s
Curr learning rate 0.0006161616161616161 	 Curr reward per step 0.6673386666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.21it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 145.74it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 131.08it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 134.75it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 169.75it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 179.19it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 173.35it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 162.89it/s]
-------------------------------------
| approxkl           | 0.0018956695 |
| clipfrac           | 0.14482287   |
| eplenmean          | 400          |
| eprewmean          | 223          |
| explained_variance | 0.272        |
| fps                | 2391         |
| nupdates           | 39           |
| policy_entropy     | 0.4693966    |
| policy_loss        | 0.0040033534 |
| serial_timesteps   | 15600        |
| time_elapsed       | 203          |
| time_remaining     | 2.34         |
| total_timesteps    | 468000       |
| true_eprew         | 148          |
| value_loss         | 86.82915     |
-------------------------------------
Current reward shaping 0.532
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7565224170684814 seconds
Total simulation time for 400 steps: 4.476876258850098 	 Other agent action time: 0 	 89.3480134076213 steps/s
Curr learning rate 0.0006060606060606061 	 Curr reward per step 0.6524896666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 125.14it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 155.31it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 138.03it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 136.35it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.32it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 169.80it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 168.94it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 166.54it/s]
-------------------------------------
| approxkl           | 0.0010923635 |
| clipfrac           | 0.12043749   |
| eplenmean          | 400          |
| eprewmean          | 243          |
| explained_variance | 0.222        |
| fps                | 2383         |
| nupdates           | 40           |
| policy_entropy     | 0.50622463   |
| policy_loss        | 0.0011937168 |
| serial_timesteps   | 16000        |
| time_elapsed       | 208          |
| time_remaining     | 2.25         |
| total_timesteps    | 480000       |
| true_eprew         | 163          |
| value_loss         | 86.93672     |
-------------------------------------
Current reward shaping 0.52
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7584624290466309 seconds
Total simulation time for 400 steps: 4.495668888092041 	 Other agent action time: 0 	 88.9745241379998 steps/s
Curr learning rate 0.000595959595959596 	 Curr reward per step 0.6333833333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 121.18it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 117.25it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 114.76it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 105.94it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 106.00it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 109.32it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 132.42it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.24it/s]
-------------------------------------
| approxkl           | 0.0018294675 |
| clipfrac           | 0.14976041   |
| eplenmean          | 400          |
| eprewmean          | 256          |
| explained_variance | 0.281        |
| fps                | 2311         |
| nupdates           | 41           |
| policy_entropy     | 0.5070859    |
| policy_loss        | 0.0030782525 |
| serial_timesteps   | 16400        |
| time_elapsed       | 213          |
| time_remaining     | 2.16         |
| total_timesteps    | 492000       |
| true_eprew         | 173          |
| value_loss         | 83.46268     |
-------------------------------------
Current reward shaping 0.508
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.739844560623169 seconds
Total simulation time for 400 steps: 4.430818796157837 	 Other agent action time: 0 	 90.27676788472101 steps/s
Curr learning rate 0.0005858585858585859 	 Curr reward per step 0.6192096666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 149.47it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.24it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.96it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 148.93it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 170.60it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 170.38it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.22it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 169.58it/s]
-------------------------------------
| approxkl           | 0.0012516074 |
| clipfrac           | 0.20262499   |
| eplenmean          | 400          |
| eprewmean          | 256          |
| explained_variance | 0.274        |
| fps                | 2420         |
| nupdates           | 42           |
| policy_entropy     | 0.7598513    |
| policy_loss        | 0.0015460748 |
| serial_timesteps   | 16800        |
| time_elapsed       | 218          |
| time_remaining     | 2.07         |
| total_timesteps    | 504000       |
| true_eprew         | 174          |
| value_loss         | 89.157776    |
-------------------------------------
Current reward shaping 0.496
Current self-play randomization 0.9984
SP envs: 30/30
Other agent actions took 0.7637176513671875 seconds
Total simulation time for 400 steps: 4.486823320388794 	 Other agent action time: 0 	 89.149933357603 steps/s
Curr learning rate 0.0005757575757575758 	 Curr reward per step 0.6466093333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 163.52it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 175.94it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.50it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.56it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.81it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 161.43it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 132.38it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 146.93it/s]
-------------------------------------
| approxkl           | 0.0009859919 |
| clipfrac           | 0.16394791   |
| eplenmean          | 400          |
| eprewmean          | 254          |
| explained_variance | 0.339        |
| fps                | 2388         |
| nupdates           | 43           |
| policy_entropy     | 0.7510033    |
| policy_loss        | 0.0010764644 |
| serial_timesteps   | 17200        |
| time_elapsed       | 223          |
| time_remaining     | 1.99         |
| total_timesteps    | 516000       |
| true_eprew         | 174          |
| value_loss         | 79.00378     |
-------------------------------------
Current reward shaping 0.484
Current self-play randomization 0.9936
SP envs: 30/30
Other agent actions took 0.7915709018707275 seconds
Total simulation time for 400 steps: 4.5628297328948975 	 Other agent action time: 0 	 87.66489731498683 steps/s
Curr learning rate 0.0005656565656565657 	 Curr reward per step 0.5785686666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.85it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 170.49it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.06it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 156.20it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 165.22it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 166.21it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 171.72it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 159.56it/s]
-------------------------------------
| approxkl           | 0.0010863197 |
| clipfrac           | 0.13386458   |
| eplenmean          | 400          |
| eprewmean          | 248          |
| explained_variance | 0.285        |
| fps                | 2364         |
| nupdates           | 44           |
| policy_entropy     | 0.5880052    |
| policy_loss        | 0.0015729747 |
| serial_timesteps   | 17600        |
| time_elapsed       | 228          |
| time_remaining     | 1.9          |
| total_timesteps    | 528000       |
| true_eprew         | 171          |
| value_loss         | 89.19679     |
-------------------------------------
Current reward shaping 0.472
Current self-play randomization 0.9888
SP envs: 30/30
Other agent actions took 0.7470331192016602 seconds
Total simulation time for 400 steps: 4.4203550815582275 	 Other agent action time: 0 	 90.49046798724487 steps/s
Curr learning rate 0.0005555555555555557 	 Curr reward per step 0.6277526666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.12it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.96it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 176.47it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 177.88it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.64it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 173.21it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 170.60it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 172.62it/s]
-------------------------------------
| approxkl           | 0.000987335  |
| clipfrac           | 0.16483334   |
| eplenmean          | 400          |
| eprewmean          | 246          |
| explained_variance | 0.317        |
| fps                | 2446         |
| nupdates           | 45           |
| policy_entropy     | 0.74076504   |
| policy_loss        | 0.0009579235 |
| serial_timesteps   | 18000        |
| time_elapsed       | 233          |
| time_remaining     | 1.81         |
| total_timesteps    | 540000       |
| true_eprew         | 171          |
| value_loss         | 80.53861     |
-------------------------------------
Current reward shaping 0.45999999999999996
Current self-play randomization 0.984
SP envs: 29/30
Other agent actions took 6.098162412643433 seconds
Total simulation time for 400 steps: 9.977941513061523 	 Other agent action time: 0 	 40.08842900876739 steps/s
Curr learning rate 0.0005454545454545455 	 Curr reward per step 0.6113533333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 165.67it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.12it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.11it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 170.30it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.66it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 167.49it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 165.00it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 146.84it/s]
--------------------------------------
| approxkl           | 0.0007897769  |
| clipfrac           | 0.12848958    |
| eplenmean          | 400           |
| eprewmean          | 244           |
| explained_variance | 0.269         |
| fps                | 1143          |
| nupdates           | 46            |
| policy_entropy     | 0.73823273    |
| policy_loss        | 0.00028076698 |
| serial_timesteps   | 18400         |
| time_elapsed       | 243           |
| time_remaining     | 1.76          |
| total_timesteps    | 552000        |
| true_eprew         | 171           |
| value_loss         | 82.08858      |
--------------------------------------
Current reward shaping 0.44799999999999995
Current self-play randomization 0.9792
SP envs: 30/30
Other agent actions took 0.750941276550293 seconds
Total simulation time for 400 steps: 4.444178819656372 	 Other agent action time: 0 	 90.00537922345086 steps/s
Curr learning rate 0.0005353535353535353 	 Curr reward per step 0.6285786666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 150.61it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 154.84it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 169.59it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 163.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.25it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.43it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 119.09it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 117.63it/s]
-------------------------------------
| approxkl           | 0.0012636563 |
| clipfrac           | 0.12973958   |
| eplenmean          | 400          |
| eprewmean          | 245          |
| explained_variance | 0.255        |
| fps                | 2394         |
| nupdates           | 47           |
| policy_entropy     | 0.5245787    |
| policy_loss        | 0.0017985757 |
| serial_timesteps   | 18800        |
| time_elapsed       | 248          |
| time_remaining     | 1.67         |
| total_timesteps    | 564000       |
| true_eprew         | 174          |
| value_loss         | 83.16518     |
-------------------------------------
Current reward shaping 0.43600000000000005
Current self-play randomization 0.9744
SP envs: 30/30
Other agent actions took 0.7902803421020508 seconds
Total simulation time for 400 steps: 4.607463836669922 	 Other agent action time: 0 	 86.81565698171663 steps/s
Curr learning rate 0.0005252525252525252 	 Curr reward per step 0.66342

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 161.95it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 167.90it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.93it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 155.96it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 162.90it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.21it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 171.78it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 171.52it/s]
-------------------------------------
| approxkl           | 0.0059530805 |
| clipfrac           | 0.3651875    |
| eplenmean          | 400          |
| eprewmean          | 253          |
| explained_variance | 0.198        |
| fps                | 2346         |
| nupdates           | 48           |
| policy_entropy     | 0.7389091    |
| policy_loss        | 0.008458116  |
| serial_timesteps   | 19200        |
| time_elapsed       | 253          |
| time_remaining     | 1.58         |
| total_timesteps    | 576000       |
| true_eprew         | 180          |
| value_loss         | 78.459625    |
-------------------------------------
Current reward shaping 0.42400000000000004
Current self-play randomization 0.9696
SP envs: 29/30
Other agent actions took 6.068346738815308 seconds
Total simulation time for 400 steps: 9.918641090393066 	 Other agent action time: 0 	 40.328105065464 steps/s
Curr learning rate 0.0005151515151515151 	 Curr reward per step 0.600996

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.98it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.34it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 167.16it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 176.23it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 178.10it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 148.75it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 161.52it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 156.92it/s]
-------------------------------------
| approxkl           | 0.0022862698 |
| clipfrac           | 0.18433334   |
| eplenmean          | 400          |
| eprewmean          | 252          |
| explained_variance | 0.263        |
| fps                | 1150         |
| nupdates           | 49           |
| policy_entropy     | 0.5503356    |
| policy_loss        | 0.003467715  |
| serial_timesteps   | 19600        |
| time_elapsed       | 264          |
| time_remaining     | 1.53         |
| total_timesteps    | 588000       |
| true_eprew         | 180          |
| value_loss         | 83.601105    |
-------------------------------------
Current reward shaping 0.41200000000000003
Current self-play randomization 0.9648
SP envs: 30/30
Other agent actions took 0.7756078243255615 seconds
Total simulation time for 400 steps: 4.548772811889648 	 Other agent action time: 0 	 87.93580522519704 steps/s
Curr learning rate 0.000505050505050505 	 Curr reward per step 0.5767316666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 165.40it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 165.43it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.86it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 152.19it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 162.57it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 166.70it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 171.26it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 150.00it/s]
--------------------------------------
| approxkl           | 0.0010109047  |
| clipfrac           | 0.16576043    |
| eplenmean          | 400           |
| eprewmean          | 245           |
| explained_variance | 0.3           |
| fps                | 2366          |
| nupdates           | 50            |
| policy_entropy     | 0.7334682     |
| policy_loss        | 0.00038366806 |
| serial_timesteps   | 20000         |
| time_elapsed       | 269           |
| time_remaining     | 1.43          |
| total_timesteps    | 600000        |
| true_eprew         | 177           |
| value_loss         | 80.04892      |
--------------------------------------
Current reward shaping 0.4
Current self-play randomization 0.96
../../thesis_data/dr_ppo/ppo_bc_train_simple/
PPO agent on index 0:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 


Timestep: 2
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑1O 
X   →0  X 
X D X S X 


Timestep: 3
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←1  O 
X ←0    X 
X D X S X 


Timestep: 4
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←1  O 
X   →0  X 
X D X S X 


Timestep: 5
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X ←0    X 
X D X S X 


Timestep: 6
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X ←0    X 
X D X S X 


Timestep: 7
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X ←0    X 
X D X S X 


Timestep: 8
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X ←0    X 
X D X S X 


Timestep: 9
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X   →0  X 
X D X S X 


Timestep: 10
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X ←0    X 
X D X S X 


Timestep: 11
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   →0  X 
X D X S X 


Timestep: 12
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X ←0    X 
X D X S X 


Timestep: 13
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X   →0  X 
X D X S X 


Timestep: 14
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X ←0    X 
X D X S X 


Timestep: 15
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X ←0    X 
X D X S X 


Timestep: 16
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X ↓0    X 
X D X S X 


Timestep: 17
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 18
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑1  O 
X ↓d    X 
X D X S X 


Timestep: 19
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X   →d  X 
X D X S X 


Timestep: 20
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X   →d  X 
X D X S X 


Timestep: 21
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X   →d  X 
X D X S X 


Timestep: 22
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X   →d  X 
X D X S X 


Timestep: 23
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X ←d    X 
X D X S X 


Timestep: 24
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 25
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 26
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑1  O 
X ↓d    X 
X D X S X 


Timestep: 27
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑1  O 
X ↓d    X 
X D X S X 


Timestep: 28
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 29
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 30
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 31
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 32
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
X ↓d    X 
X D X S X 


Timestep: 33
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 34
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 35
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 36
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 37
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø1X X 
O   ↑1  O 
X ↓d    X 
X D X S X 


Timestep: 38
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø2X X 
O   ↑1  O 
X ↓d    X 
X D X S X 


Timestep: 39
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø3X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 40
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø4X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 41
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø5X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 42
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø6X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 43
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø7X X 
O     ↑1O 
X ↓d    X 
X D X S X 


Timestep: 44
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø8X X 
O     ↑1O 
X ↓d    X 
X D X S X 


Timestep: 45
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø9X X 
O     ↑1O 
X ↓d    X 
X D X S X 


Timestep: 46
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø10X X 
O     ↑1O 
X ↓d    X 
X D X S X 


Timestep: 47
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø11X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 48
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø12X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 49
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø13X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 50
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø14X X 
O     →1O 
X ↓d    X 
X D X S X 


Timestep: 51
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø15X X 
O   ←1  O 
X ↓d    X 
X D X S X 


Timestep: 52
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø16X X 
O   ←1  O 
X ↓d    X 
X D X S X 


Timestep: 53
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø17X X 
O ←1    O 
X ↓d    X 
X D X S X 


Timestep: 54
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø18X X 
O ←1    O 
X ↓d    X 
X D X S X 


Timestep: 55
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø19X X 
O ←1    O 
X   →d  X 
X D X S X 


Timestep: 56
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ←1↑d  O 
X       X 
X D X S X 


Timestep: 57
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 5 
X X P X X 
O ←o↑s  O 
X       X 
X D X S X 


Timestep: 58
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑o  →sO 
X       X 
X D X S X 


Timestep: 59
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑o    O 
X     ↓sX 
X D X S X 


Timestep: 60
Joint action taken: ('interact', 'stay') 	 Reward: 20 + shape * 0 
X X P X X 
O ↑o    O 
X     ↓0X 
X D X S X 


Timestep: 61
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑o    O 
X     ↓0X 
X D X S X 


Timestep: 62
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X XoP X X 
O ↑1    O 
X     ↓0X 
X D X S X 


Timestep: 63
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X XoP X X 
O ↑1    O 
X     ↓0X 
X D X S X 


Timestep: 64
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X XoP X X 
O   →1  O 
X     ↓0X 
X D X S X 


Timestep: 65
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X XoP X X 
O   →1  O 
X     →0X 
X D X S X 


Timestep: 66
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X XoP X X 
O ←1    O 
X     →0X 
X D X S X 


Timestep: 67
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X XoP X X 
O ←1    O 
X     →0X 
X D X S X 


Timestep: 68
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X XoP X X 
O ←1    O 
X     →0X 
X D X S X 


Timestep: 69
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X XoP X X 
O ←1    O 
X     →0X 
X D X S X 


Timestep: 70
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X XoP X X 
O ←o    O 
X     →0X 
X D X S X 


Timestep: 71
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X XoP X X 
O ↑o    O 
X     →0X 
X D X S X 


Timestep: 72
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X XoP X X 
O ↑o    O 
X     →0X 
X D X S X 


Timestep: 73
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X XoP X X 
O ↑o    O 
X     →0X 
X D X S X 


Timestep: 74
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X XoP X X 
O ↑o    O 
X     →0X 
X D X S X 


Timestep: 75
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X XoP X X 
O ↑o    O 
X     →0X 
X D X S X 


Timestep: 76
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X XoP X X 
O   →o  O 
X     →0X 
X D X S X 


Timestep: 77
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X XoP X X 
O   →o  O 
X     →0X 
X D X S X 


Timestep: 78
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X XoP X X 
O   ↑o  O 
X   ←0  X 
X D X S X 


Timestep: 79
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X XoP X X 
O   ↑o  O 
X   ←0  X 
X D X S X 


Timestep: 80
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X XoP X X 
O   ↑o  O 
X   ←0  X 
X D X S X 


Timestep: 81
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X XoP X X 
O   ↑o  O 
X   ↑0  X 
X D X S X 


Timestep: 82
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 3 
X Xoø-X X 
O   ↑1  O 
X ←0    X 
X D X S X 


Timestep: 83
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X Xoø-X X 
O     →1O 
X   →0  X 
X D X S X 


Timestep: 84
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X Xoø-X X 
O     →1O 
X   →0  X 
X D X S X 


Timestep: 85
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X Xoø-X X 
O     →1O 
X   →0  X 
X D X S X 


Timestep: 86
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X Xoø-X X 
O     →oO 
X   →0  X 
X D X S X 


Timestep: 87
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X Xoø-X X 
O     →oO 
X ←0    X 
X D X S X 


Timestep: 88
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X Xoø-X X 
O   ←o  O 
X   →0  X 
X D X S X 


Timestep: 89
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X Xoø-X X 
O   ←o  O 
X ←0    X 
X D X S X 


Timestep: 90
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X Xoø-X X 
O   ←o  O 
X ↓0    X 
X D X S X 


Timestep: 91
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 3 
X Xoø-X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 92
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X Xoø-X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 93
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X Xoø-X X 
O     →oO 
X ↓d    X 
X D X S X 


Timestep: 94
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X Xoø-X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 95
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X Xoø-X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 96
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X Xoø-X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 97
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X Xoø-X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 98
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X Xoø=X X 
O   ↑1  O 
X ↓d    X 
X D X S X 


Timestep: 99
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X Xoø=X X 
O     →1O 
X ↓d    X 
X D X S X 


tot rew 60 tot rew shaped 60
PPO agent on index 1:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X ↑0    X 
X D X S X 


Timestep: 2
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X ↑0    X 
X D X S X 


Timestep: 3
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0←o  O 
X       X 
X D X S X 


Timestep: 4
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0↑o  O 
X       X 
X D X S X 


Timestep: 5
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O ↑0↑1  O 
X       X 
X D X S X 


Timestep: 6
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ↑0  →1O 
X       X 
X D X S X 


Timestep: 7
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →0→oO 
X       X 
X D X S X 


Timestep: 8
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →0←oO 
X       X 
X D X S X 


Timestep: 9
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →0←oO 
X       X 
X D X S X 


Timestep: 10
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   →0←oO 
X       X 
X D X S X 


Timestep: 11
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←0←o  O 
X       X 
X D X S X 


Timestep: 12
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←o↑o  O 
X       X 
X D X S X 


Timestep: 13
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O →o↑1  O 
X       X 
X D X S X 


Timestep: 14
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O →o  →1O 
X       X 
X D X S X 


Timestep: 15
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O →o  →oO 
X       X 
X D X S X 


Timestep: 16
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O →o←o  O 
X       X 
X D X S X 


Timestep: 17
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑o↑o  O 
X       X 
X D X S X 


Timestep: 18
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 3 
X X ø1X X 
O ↑o↑1  O 
X       X 
X D X S X 


Timestep: 19
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø2X X 
O ←o↑1  O 
X       X 
X D X S X 


Timestep: 20
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø3X X 
O ←o↑1  O 
X       X 
X D X S X 


Timestep: 21
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø4X X 
O ←o↑1  O 
X       X 
X D X S X 


Timestep: 22
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø5X X 
O ←o←1  O 
X       X 
X D X S X 


Timestep: 23
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø6X X 
O ←o  →1O 
X       X 
X D X S X 


Timestep: 24
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X X ø7X X 
O     →oO 
X ↓o    X 
X D X S X 


Timestep: 25
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø8X X 
O     →oO 
X   →o  X 
X D X S X 


Timestep: 26
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø9X X 
O     →oO 
X   →o  X 
X D X S X 


Timestep: 27
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø10X X 
O     →oO 
X   →o  X 
X D X S X 


Timestep: 28
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø11X X 
O   ←o  O 
X   →o  X 
X D X S X 


Timestep: 29
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X ø12X X 
O   ←o  O 
X   ↑o  X 
X D X S X 


Timestep: 30
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø13X X 
O   ←o  O 
X   ↑o  X 
X D X S X 


Timestep: 31
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X ø14X X 
O   ↓o  O 
X   ↑o  X 
X D X S X 


Timestep: 32
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø15X X 
O     →oO 
X   ↑o  X 
X D X S X 


Timestep: 33
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø16X X 
O     →oO 
X   ↑o  X 
X D X S X 


Timestep: 34
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø17X X 
O     →oO 
X ←o    X 
X D X S X 


Timestep: 35
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø18X X 
O     →oO 
X ←o    X 
X D X S X 


Timestep: 36
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø19X X 
O       O 
X ←o  ↓oX 
X D X S X 


Timestep: 37
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
X ←o  ↓oX 
X D X S X 


Timestep: 38
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O       O 
Xo←0  ↓oX 
X D X S X 


Timestep: 39
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑0    O 
Xo    ↓oX 
X D X S X 


Timestep: 40
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑0    O 
Xo    ↓oX 
X D X S X 


Timestep: 41
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑0    O 
Xo    ↓oX 
X D X S X 


Timestep: 42
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑0    O 
Xo    ↓oX 
X D X S X 


Timestep: 43
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑0    O 
Xo    ↓oX 
X D X S X 


Timestep: 44
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   →0  O 
Xo    ↓oX 
X D X S X 


Timestep: 45
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   →0  O 
Xo  ←o  X 
X D X S X 


Timestep: 46
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   →0  O 
Xo  ←o  X 
X D X S X 


Timestep: 47
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   →0  O 
Xo  ←o  X 
X D X S X 


Timestep: 48
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↓0  O 
Xo  ←o  X 
X D X S X 


Timestep: 49
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↓0  O 
Xo    →oX 
X D X S X 


Timestep: 50
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ←0    O 
Xo  ←o  X 
X D X S X 


Timestep: 51
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ←0↑o  O 
Xo      X 
X D X S X 


Timestep: 52
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ←0↑o  O 
Xo      X 
X D X S X 


Timestep: 53
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O →0↑o  O 
Xo      X 
X D X S X 


Timestep: 54
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O →0↑o  O 
Xo      X 
X D X S X 


Timestep: 55
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O →0↑o  O 
Xo      X 
X D X S X 


Timestep: 56
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X X ø20X X 
O   ↑o  O 
Xo↓0    X 
X D X S X 


Timestep: 57
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø20X X 
O   ↑o  O 
Xo↓d    X 
X D X S X 


Timestep: 58
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
Xo  →d  X 
X D X S X 


Timestep: 59
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
Xo  →d  X 
X D X S X 


Timestep: 60
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
Xo  →d  X 
X D X S X 


Timestep: 61
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O     →oO 
Xo←d    X 
X D X S X 


Timestep: 62
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑d  →oO 
Xo      X 
X D X S X 


Timestep: 63
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑d  →oO 
Xo      X 
X D X S X 


Timestep: 64
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20X X 
O ↑d  →oO 
Xo      X 
X D X S X 


Timestep: 65
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X Xdø20X X 
O ↑0  →oO 
Xo      X 
X D X S X 


Timestep: 66
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X Xdø20X X 
O ↑0    O 
Xo    ↓oX 
X D X S X 


Timestep: 67
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X Xdø20X X 
O   →0  O 
Xo    ↓oX 
X D X S X 


Timestep: 68
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X Xdø20X X 
O   →0  O 
Xo  ←o  X 
X D X S X 


Timestep: 69
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X Xdø20X X 
O     →0O 
Xo  ←o  X 
X D X S X 


Timestep: 70
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X Xdø20X X 
O     →0O 
Xo  ←o  X 
X D X S X 


Timestep: 71
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X Xdø20X X 
O     →0O 
Xo  ←o  X 
X D X S X 


Timestep: 72
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X Xdø20X X 
O     →0O 
Xo  ←o  X 
X D X S X 


Timestep: 73
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X Xdø20X X 
O     →oO 
Xo  ←o  X 
X D X S X 


Timestep: 74
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X Xdø20X X 
O     ↑oO 
Xo  ↓o  X 
X D X S X 


Timestep: 75
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X Xdø20X X 
O     ↑oO 
Xo←o    X 
X D X S X 


Timestep: 76
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X Xdø20X X 
O     ↑oO 
Xo←o    X 
X D X S X 


Timestep: 77
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X Xdø20X X 
O     ↑oO 
Xo  →o  X 
X D X S X 


Timestep: 78
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O     ↑0O 
Xo  →o  X 
X D X S X 


Timestep: 79
Joint action taken: ('↓', 'interact') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O       O 
Xo  →o↓0X 
X D X S X 


Timestep: 80
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O       O 
Xo  →o↓0X 
X D X S X 


Timestep: 81
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O       O 
Xo  →o↓0X 
X D X S X 


Timestep: 82
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O       O 
Xo  →o↓0X 
X D X S X 


Timestep: 83
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O       O 
Xo←o←0  X 
X D X S X 


Timestep: 84
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O       O 
Xo←o←0  X 
X D X S X 


Timestep: 85
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O       O 
Xo←o←0  X 
X D X S X 


Timestep: 86
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O       O 
Xo←o←0  X 
X D X S X 


Timestep: 87
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O ↑o    O 
Xo←0    X 
X D X S X 


Timestep: 88
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O ↑o    O 
Xo←0    X 
X D X S X 


Timestep: 89
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O ↑o    O 
Xo←0    X 
X D X S X 


Timestep: 90
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O ↑o    O 
Xo←0    X 
X D X S X 


Timestep: 91
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O ↑o    O 
Xo  →0  X 
X D X S X 


Timestep: 92
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O ↑o    O 
Xo  →0  X 
X D X S X 


Timestep: 93
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O ←o    O 
Xo  →0  X 
X D X S X 


Timestep: 94
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O ←o    O 
Xo  →0  X 
X D X S X 


Timestep: 95
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O ←o    O 
Xo  ↓0  X 
X D X S X 


Timestep: 96
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O ←o    O 
Xo  ↓0  X 
X D X S X 


Timestep: 97
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O ←o    O 
Xo  ↓0  X 
X D X S X 


Timestep: 98
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O ←o    O 
Xo  ↓0  X 
X D X S X 


Timestep: 99
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X Xdø20XoX 
O ←o    O 
Xo  ↓0  X 
X D X S X 


tot rew 20 tot rew shaped 26
../../thesis_data/dr_ppo/ppo_bc_train_simple/
SP envs: 28/30
Other agent actions took 6.081444263458252 seconds
Total simulation time for 400 steps: 9.897780179977417 	 Other agent action time: 0 	 40.41310200131285 steps/s
Curr learning rate 0.000494949494949495 	 Curr reward per step 0.5363

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 163.34it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 175.08it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 171.97it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.06it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.96it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 173.68it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 164.06it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 162.51it/s]
-------------------------------------
| approxkl           | 0.0011662452 |
| clipfrac           | 0.11893749   |
| eplenmean          | 400          |
| eprewmean          | 232          |
| explained_variance | 0.342        |
| fps                | 1153         |
| nupdates           | 51           |
| policy_entropy     | 0.52521485   |
| policy_loss        | 0.0015897567 |
| serial_timesteps   | 20400        |
| time_elapsed       | 282          |
| time_remaining     | 1.38         |
| total_timesteps    | 612000       |
| true_eprew         | 169          |
| value_loss         | 91.747154    |
-------------------------------------
Current reward shaping 0.388
Current self-play randomization 0.9552
SP envs: 30/30
Other agent actions took 0.790654182434082 seconds
Total simulation time for 400 steps: 4.527001142501831 	 Other agent action time: 0 	 88.35871417053397 steps/s
Curr learning rate 0.0004848484848484849 	 Curr reward per step 0.6143386666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 151.20it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 143.02it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.29it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 153.56it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 168.47it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 169.69it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 170.47it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.69it/s]
-------------------------------------
| approxkl           | 0.0032253265 |
| clipfrac           | 0.27340624   |
| eplenmean          | 400          |
| eprewmean          | 232          |
| explained_variance | 0.231        |
| fps                | 2373         |
| nupdates           | 52           |
| policy_entropy     | 0.7309652    |
| policy_loss        | 0.0043824497 |
| serial_timesteps   | 20800        |
| time_elapsed       | 287          |
| time_remaining     | 1.29         |
| total_timesteps    | 624000       |
| true_eprew         | 170          |
| value_loss         | 77.85927     |
-------------------------------------
Current reward shaping 0.376
Current self-play randomization 0.9504
SP envs: 30/30
Other agent actions took 0.7654581069946289 seconds
Total simulation time for 400 steps: 4.453833818435669 	 Other agent action time: 0 	 89.81026601043975 steps/s
Curr learning rate 0.0004747474747474748 	 Curr reward per step 0.5758880000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.13it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.08it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.43it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 170.16it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 173.88it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 183.05it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 177.84it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.07it/s]
-------------------------------------
| approxkl           | 0.0021572926 |
| clipfrac           | 0.25181252   |
| eplenmean          | 400          |
| eprewmean          | 231          |
| explained_variance | 0.338        |
| fps                | 2428         |
| nupdates           | 53           |
| policy_entropy     | 0.7606061    |
| policy_loss        | 0.0033744692 |
| serial_timesteps   | 21200        |
| time_elapsed       | 292          |
| time_remaining     | 1.19         |
| total_timesteps    | 636000       |
| true_eprew         | 170          |
| value_loss         | 76.13067     |
-------------------------------------
Current reward shaping 0.364
Current self-play randomization 0.9456
SP envs: 28/30
Other agent actions took 6.003124952316284 seconds
Total simulation time for 400 steps: 9.77014446258545 	 Other agent action time: 0 	 40.94105276864545 steps/s
Curr learning rate 0.0004646464646464647 	 Curr reward per step 0.576537

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.86it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 167.37it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 160.25it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 170.90it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.31it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.39it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.65it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 155.36it/s]
-------------------------------------
| approxkl           | 0.0007077413 |
| clipfrac           | 0.10370834   |
| eplenmean          | 400          |
| eprewmean          | 231          |
| explained_variance | 0.235        |
| fps                | 1165         |
| nupdates           | 54           |
| policy_entropy     | 0.54133993   |
| policy_loss        | 0.0004439705 |
| serial_timesteps   | 21600        |
| time_elapsed       | 302          |
| time_remaining     | 1.12         |
| total_timesteps    | 648000       |
| true_eprew         | 172          |
| value_loss         | 83.80353     |
-------------------------------------
Current reward shaping 0.352
Current self-play randomization 0.9408
SP envs: 28/30
Other agent actions took 6.08310341835022 seconds
Total simulation time for 400 steps: 9.872609376907349 	 Other agent action time: 0 	 40.516137601435446 steps/s
Curr learning rate 0.00045454545454545455 	 Curr reward per step 0.5813813333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.22it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 169.98it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 157.15it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 177.48it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.96it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 173.72it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.51it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 173.43it/s]
--------------------------------------
| approxkl           | 0.00064776966 |
| clipfrac           | 0.121322915   |
| eplenmean          | 400           |
| eprewmean          | 233           |
| explained_variance | 0.22          |
| fps                | 1156          |
| nupdates           | 55            |
| policy_entropy     | 0.7042897     |
| policy_loss        | 0.0002014793  |
| serial_timesteps   | 22000         |
| time_elapsed       | 313           |
| time_remaining     | 1.04          |
| total_timesteps    | 660000        |
| true_eprew         | 175           |
| value_loss         | 78.615204     |
--------------------------------------
Current reward shaping 0.33999999999999997
Current self-play randomization 0.9359999999999999
SP envs: 29/30
Other agent actions took 6.099621772766113 seconds
Total simulation time for 400 steps: 9.843095064163208 	 Other agent action time: 0 	 40.63762438466353 steps/s
Curr learning rate 0.0004444444444444444 	 Curr reward per step 0.6036116666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.11it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 170.77it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.43it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 156.14it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 176.39it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 167.14it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.42it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 166.82it/s]
--------------------------------------
| approxkl           | 0.0007322751  |
| clipfrac           | 0.120156266   |
| eplenmean          | 400           |
| eprewmean          | 234           |
| explained_variance | 0.368         |
| fps                | 1159          |
| nupdates           | 56            |
| policy_entropy     | 0.73838395    |
| policy_loss        | 5.1410803e-05 |
| serial_timesteps   | 22400         |
| time_elapsed       | 323           |
| time_remaining     | 0.962         |
| total_timesteps    | 672000        |
| true_eprew         | 177           |
| value_loss         | 71.57502      |
--------------------------------------
Current reward shaping 0.32799999999999996
Current self-play randomization 0.9312
SP envs: 28/30
Other agent actions took 6.025069952011108 seconds
Total simulation time for 400 steps: 9.756311416625977 	 Other agent action time: 0 	 40.99910129132921 steps/s
Curr learning rate 0.00043434343434343433 	 Curr reward per step 0.5770473333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.12it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 175.09it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 177.65it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 157.82it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.93it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 175.14it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 155.45it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 155.28it/s]
-------------------------------------
| approxkl           | 0.0017220365 |
| clipfrac           | 0.20107289   |
| eplenmean          | 400          |
| eprewmean          | 233          |
| explained_variance | 0.281        |
| fps                | 1168         |
| nupdates           | 57           |
| policy_entropy     | 0.7216221    |
| policy_loss        | 0.0023837856 |
| serial_timesteps   | 22800        |
| time_elapsed       | 333          |
| time_remaining     | 0.877        |
| total_timesteps    | 684000       |
| true_eprew         | 178          |
| value_loss         | 75.4366      |
-------------------------------------
Current reward shaping 0.31599999999999995
Current self-play randomization 0.9264
SP envs: 29/30
Other agent actions took 6.03140115737915 seconds
Total simulation time for 400 steps: 9.897863864898682 	 Other agent action time: 0 	 40.41276031473227 steps/s
Curr learning rate 0.00042424242424242425 	 Curr reward per step 0.543229

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.44it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.13it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 149.83it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 134.93it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 157.59it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 176.89it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 181.96it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 159.50it/s]
-------------------------------------
| approxkl           | 0.0032437234 |
| clipfrac           | 0.28407294   |
| eplenmean          | 400          |
| eprewmean          | 232          |
| explained_variance | 0.362        |
| fps                | 1151         |
| nupdates           | 58           |
| policy_entropy     | 0.6967808    |
| policy_loss        | 0.004735212  |
| serial_timesteps   | 23200        |
| time_elapsed       | 344          |
| time_remaining     | 0.79         |
| total_timesteps    | 696000       |
| true_eprew         | 179          |
| value_loss         | 73.342636    |
-------------------------------------
Current reward shaping 0.30400000000000005
Current self-play randomization 0.9216
SP envs: 29/30
Other agent actions took 6.040631294250488 seconds
Total simulation time for 400 steps: 9.779153823852539 	 Other agent action time: 0 	 40.9033345016367 steps/s
Curr learning rate 0.0004141414141414141 	 Curr reward per step 0.5673293333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.66it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 165.36it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 172.94it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 172.76it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 158.89it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 169.13it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 148.60it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.75it/s]
-------------------------------------
| approxkl           | 0.0014844339 |
| clipfrac           | 0.20311458   |
| eplenmean          | 400          |
| eprewmean          | 226          |
| explained_variance | 0.287        |
| fps                | 1165         |
| nupdates           | 59           |
| policy_entropy     | 0.7144249    |
| policy_loss        | 0.0017218981 |
| serial_timesteps   | 23600        |
| time_elapsed       | 354          |
| time_remaining     | 0.7          |
| total_timesteps    | 708000       |
| true_eprew         | 176          |
| value_loss         | 72.36641     |
-------------------------------------
Current reward shaping 0.29200000000000004
Current self-play randomization 0.9168000000000001
SP envs: 27/30
Other agent actions took 5.927759885787964 seconds
Total simulation time for 400 steps: 9.642685174942017 	 Other agent action time: 0 	 41.4822212633739 steps/s
Curr learning rate 0.00040404040404040404 	 Curr reward per step 0.5526026666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 172.27it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.92it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.72it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 169.20it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 167.94it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 160.86it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 169.78it/s]
-------------------------------------
| approxkl           | 0.0012032022 |
| clipfrac           | 0.13698956   |
| eplenmean          | 400          |
| eprewmean          | 222          |
| explained_variance | 0.266        |
| fps                | 1182         |
| nupdates           | 60           |
| policy_entropy     | 0.57702273   |
| policy_loss        | 0.0016993729 |
| serial_timesteps   | 24000        |
| time_elapsed       | 364          |
| time_remaining     | 0.607        |
| total_timesteps    | 720000       |
| true_eprew         | 174          |
| value_loss         | 75.92749     |
-------------------------------------
Current reward shaping 0.28
Current self-play randomization 0.912
SP envs: 27/30
Other agent actions took 6.146661996841431 seconds
Total simulation time for 400 steps: 9.91820740699768 	 Other agent action time: 0 	 40.32986845160996 steps/s
Curr learning rate 0.00039393939393939396 	 Curr reward per step 0.5372066666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 147.08it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 155.11it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.55it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 172.71it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 138.04it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.13it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.54it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 171.30it/s]
-------------------------------------
| approxkl           | 0.001207495  |
| clipfrac           | 0.16508332   |
| eplenmean          | 400          |
| eprewmean          | 221          |
| explained_variance | 0.178        |
| fps                | 1149         |
| nupdates           | 61           |
| policy_entropy     | 0.71819025   |
| policy_loss        | 0.0016365575 |
| serial_timesteps   | 24400        |
| time_elapsed       | 375          |
| time_remaining     | 0.512        |
| total_timesteps    | 732000       |
| true_eprew         | 175          |
| value_loss         | 82.46832     |
-------------------------------------
Current reward shaping 0.268
Current self-play randomization 0.9072
SP envs: 29/30
Other agent actions took 5.944159030914307 seconds
Total simulation time for 400 steps: 9.714126110076904 	 Other agent action time: 0 	 41.177147122381065 steps/s
Curr learning rate 0.0003838383838383839 	 Curr reward per step 0.5448633333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 165.93it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 165.12it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 175.56it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 125.33it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 117.69it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 118.32it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 118.30it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 115.55it/s]
-------------------------------------
| approxkl           | 0.001041557  |
| clipfrac           | 0.13257287   |
| eplenmean          | 400          |
| eprewmean          | 220          |
| explained_variance | 0.311        |
| fps                | 1160         |
| nupdates           | 62           |
| policy_entropy     | 0.57788193   |
| policy_loss        | 0.0009728281 |
| serial_timesteps   | 24800        |
| time_elapsed       | 385          |
| time_remaining     | 0.414        |
| total_timesteps    | 744000       |
| true_eprew         | 176          |
| value_loss         | 77.12248     |
-------------------------------------
Current reward shaping 0.256
Current self-play randomization 0.9024
SP envs: 28/30
Other agent actions took 5.997747898101807 seconds
Total simulation time for 400 steps: 9.856953859329224 	 Other agent action time: 0 	 40.580488222679016 steps/s
Curr learning rate 0.0003737373737373737 	 Curr reward per step 0.5461813333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.78it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 140.94it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 139.58it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 159.19it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 175.48it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 176.46it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.58it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.12it/s]
-------------------------------------
| approxkl           | 0.0010495448 |
| clipfrac           | 0.16203122   |
| eplenmean          | 400          |
| eprewmean          | 216          |
| explained_variance | 0.293        |
| fps                | 1154         |
| nupdates           | 63           |
| policy_entropy     | 0.6948117    |
| policy_loss        | 0.0008636008 |
| serial_timesteps   | 25200        |
| time_elapsed       | 395          |
| time_remaining     | 0.314        |
| total_timesteps    | 756000       |
| true_eprew         | 173          |
| value_loss         | 77.32654     |
-------------------------------------
Current reward shaping 0.244
Current self-play randomization 0.8976
SP envs: 27/30
Other agent actions took 6.0783164501190186 seconds
Total simulation time for 400 steps: 9.829030752182007 	 Other agent action time: 0 	 40.6957725624372 steps/s
Curr learning rate 0.0003636363636363636 	 Curr reward per step 0.5323960000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.87it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.60it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.00it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 175.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 168.23it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 175.24it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.52it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 158.37it/s]
-------------------------------------
| approxkl           | 0.0025240176 |
| clipfrac           | 0.2741042    |
| eplenmean          | 400          |
| eprewmean          | 214          |
| explained_variance | 0.255        |
| fps                | 1161         |
| nupdates           | 64           |
| policy_entropy     | 0.74346125   |
| policy_loss        | 0.004885217  |
| serial_timesteps   | 25600        |
| time_elapsed       | 406          |
| time_remaining     | 0.211        |
| total_timesteps    | 768000       |
| true_eprew         | 173          |
| value_loss         | 78.54729     |
-------------------------------------
Current reward shaping 0.23199999999999998
Current self-play randomization 0.8928
SP envs: 26/30
Other agent actions took 5.973606586456299 seconds
Total simulation time for 400 steps: 9.741347789764404 	 Other agent action time: 0 	 41.06207976890989 steps/s
Curr learning rate 0.00035353535353535354 	 Curr reward per step 0.5294706666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.66it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 174.38it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.88it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 144.78it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 158.18it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 168.48it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.67it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 162.26it/s]
-------------------------------------
| approxkl           | 0.002330257  |
| clipfrac           | 0.21697912   |
| eplenmean          | 400          |
| eprewmean          | 216          |
| explained_variance | 0.372        |
| fps                | 1169         |
| nupdates           | 65           |
| policy_entropy     | 0.6201745    |
| policy_loss        | 0.0030948184 |
| serial_timesteps   | 26000        |
| time_elapsed       | 416          |
| time_remaining     | 0.107        |
| total_timesteps    | 780000       |
| true_eprew         | 176          |
| value_loss         | 75.67146     |
-------------------------------------
Current reward shaping 0.21999999999999997
Current self-play randomization 0.888
SP envs: 27/30
Other agent actions took 6.091748476028442 seconds
Total simulation time for 400 steps: 9.93629765510559 	 Other agent action time: 0 	 40.256442981502985 steps/s
Curr learning rate 0.0003434343434343434 	 Curr reward per step 0.53639

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 163.03it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 146.16it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 159.75it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.35it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 166.42it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.45it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 147.16it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 127.13it/s]
--------------------------------------
| approxkl           | 0.00057977985 |
| clipfrac           | 0.0926875     |
| eplenmean          | 400           |
| eprewmean          | 213           |
| explained_variance | 0.238         |
| fps                | 1144          |
| nupdates           | 66            |
| policy_entropy     | 0.57432044    |
| policy_loss        | 2.7204203e-05 |
| serial_timesteps   | 26400         |
| time_elapsed       | 427           |
| time_remaining     | 0             |
| total_timesteps    | 792000        |
| true_eprew         | 175           |
| value_loss         | 85.68412      |
--------------------------------------
Current reward shaping 0.20799999999999996
Current self-play randomization 0.8832
LOADING BC MODEL FROM: seed0/worker4
Loading a model without an environment, this model cannot be trained until it has a valid environment.
Loaded MediumLevelPlanner from /home/ubuntu/human_aware_rl/overcooked_ai/overcooked_ai_py/data/planners/simple_am.pkl
TOT NUM UPDATES 66
SP envs: 27/30
Other agent actions took 6.031081914901733 seconds
Total simulation time for 400 steps: 9.891888618469238 	 Other agent action time: 0 	 40.437171851405225 steps/s
Curr learning rate 0.001 	 Curr reward per step 0.5219866666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 165.57it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 170.39it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 147.81it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.39it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 177.85it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 169.18it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.24it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 171.19it/s]
------------------------------------
| approxkl           | 0.008671272 |
| clipfrac           | 0.3061146   |
| eplenmean          | 400         |
| eprewmean          | 209         |
| explained_variance | 0.21        |
| fps                | 1153        |
| nupdates           | 1           |
| policy_entropy     | 0.6011686   |
| policy_loss        | 0.013617006 |
| serial_timesteps   | 400         |
| time_elapsed       | 10.4        |
| time_remaining     | 11.3        |
| total_timesteps    | 12000       |
| true_eprew         | 176         |
| value_loss         | 82.68504    |
------------------------------------
Current reward shaping 0.988
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8024587631225586 seconds
Total simulation time for 400 steps: 4.546041011810303 	 Other agent action time: 0 	 87.98864747608467 steps/s
Curr learning rate 0.00098989898989899 	 Curr reward per step 0.9140389999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.83it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.57it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 134.54it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 145.18it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 169.81it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 148.16it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.35it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 158.14it/s]
------------------------------------
| approxkl           | 0.013627358 |
| clipfrac           | 0.44196877  |
| eplenmean          | 400         |
| eprewmean          | 287         |
| explained_variance | 0.333       |
| fps                | 2359        |
| nupdates           | 2           |
| policy_entropy     | 0.7114251   |
| policy_loss        | 0.017491182 |
| serial_timesteps   | 800         |
| time_elapsed       | 15.5        |
| time_remaining     | 8.26        |
| total_timesteps    | 24000       |
| true_eprew         | 185         |
| value_loss         | 132.95467   |
------------------------------------
Current reward shaping 0.976
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.762049674987793 seconds
Total simulation time for 400 steps: 4.42751932144165 	 Other agent action time: 0 	 90.3440439125527 steps/s
Curr learning rate 0.0009797979797979799 	 Curr reward per step 0.9320026666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 156.24it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 170.25it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 138.98it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.85it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 171.38it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.25it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.04it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 171.81it/s]
------------------------------------
| approxkl           | 0.031871296 |
| clipfrac           | 0.58036476  |
| eplenmean          | 400         |
| eprewmean          | 316         |
| explained_variance | 0.225       |
| fps                | 2425        |
| nupdates           | 3           |
| policy_entropy     | 0.80380905  |
| policy_loss        | 0.030602127 |
| serial_timesteps   | 1200        |
| time_elapsed       | 20.4        |
| time_remaining     | 7.15        |
| total_timesteps    | 36000       |
| true_eprew         | 190         |
| value_loss         | 121.31566   |
------------------------------------
Current reward shaping 0.964
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7981112003326416 seconds
Total simulation time for 400 steps: 4.5612077713012695 	 Other agent action time: 0 	 87.69607087771048 steps/s
Curr learning rate 0.0009696969696969698 	 Curr reward per step 0.8515446666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 110.66it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 143.77it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.36it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 169.86it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 168.92it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.51it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 149.95it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.10it/s]
-------------------------------------
| approxkl           | 0.0043892553 |
| clipfrac           | 0.25288543   |
| eplenmean          | 400          |
| eprewmean          | 345          |
| explained_variance | 0.348        |
| fps                | 2345         |
| nupdates           | 4            |
| policy_entropy     | 0.6589358    |
| policy_loss        | 0.0061104866 |
| serial_timesteps   | 1600         |
| time_elapsed       | 25.6         |
| time_remaining     | 6.6          |
| total_timesteps    | 48000        |
| true_eprew         | 191          |
| value_loss         | 106.284645   |
-------------------------------------
Current reward shaping 0.952
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7391502857208252 seconds
Total simulation time for 400 steps: 4.421933174133301 	 Other agent action time: 0 	 90.45817389097022 steps/s
Curr learning rate 0.0009595959595959597 	 Curr reward per step 0.8626980000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.77it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.61it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 169.46it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.19it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 142.91it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 147.65it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.39it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 165.41it/s]
------------------------------------
| approxkl           | 0.009328742 |
| clipfrac           | 0.44156247  |
| eplenmean          | 400         |
| eprewmean          | 351         |
| explained_variance | 0.259       |
| fps                | 2425        |
| nupdates           | 5           |
| policy_entropy     | 0.78820384  |
| policy_loss        | 0.012456214 |
| serial_timesteps   | 2000        |
| time_elapsed       | 30.5        |
| time_remaining     | 6.2         |
| total_timesteps    | 60000       |
| true_eprew         | 189         |
| value_loss         | 111.866196  |
------------------------------------
Current reward shaping 0.94
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7667751312255859 seconds
Total simulation time for 400 steps: 4.464966773986816 	 Other agent action time: 0 	 89.5863329443851 steps/s
Curr learning rate 0.0009494949494949496 	 Curr reward per step 0.8485550000000002

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 153.65it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 147.74it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.48it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 158.37it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.53it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 160.89it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 175.30it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 167.93it/s]
-------------------------------------
| approxkl           | 0.0031163893 |
| clipfrac           | 0.24758334   |
| eplenmean          | 400          |
| eprewmean          | 344          |
| explained_variance | 0.229        |
| fps                | 2396         |
| nupdates           | 6            |
| policy_entropy     | 0.70001805   |
| policy_loss        | 0.0048631113 |
| serial_timesteps   | 2400         |
| time_elapsed       | 35.5         |
| time_remaining     | 5.92         |
| total_timesteps    | 72000        |
| true_eprew         | 186          |
| value_loss         | 104.90944    |
-------------------------------------
Current reward shaping 0.928
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7894783020019531 seconds
Total simulation time for 400 steps: 4.51410698890686 	 Other agent action time: 0 	 88.61110314464752 steps/s
Curr learning rate 0.0009393939393939395 	 Curr reward per step 0.8234426666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.11it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.77it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 143.26it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.04it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 173.03it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.21it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 153.04it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 162.37it/s]
------------------------------------
| approxkl           | 0.005492592 |
| clipfrac           | 0.30756253  |
| eplenmean          | 400         |
| eprewmean          | 338         |
| explained_variance | 0.241       |
| fps                | 2384        |
| nupdates           | 7           |
| policy_entropy     | 0.704795    |
| policy_loss        | 0.008484247 |
| serial_timesteps   | 2800        |
| time_elapsed       | 40.5        |
| time_remaining     | 5.7         |
| total_timesteps    | 84000       |
| true_eprew         | 184         |
| value_loss         | 102.88725   |
------------------------------------
Current reward shaping 0.916
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7722458839416504 seconds
Total simulation time for 400 steps: 4.508498430252075 	 Other agent action time: 0 	 88.72133509374108 steps/s
Curr learning rate 0.0009292929292929292 	 Curr reward per step 0.8267306666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 153.17it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 139.59it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 157.98it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.70it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 155.29it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 137.69it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.15it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.38it/s]
-------------------------------------
| approxkl           | 0.0036505926 |
| clipfrac           | 0.23951039   |
| eplenmean          | 400          |
| eprewmean          | 335          |
| explained_variance | 0.258        |
| fps                | 2376         |
| nupdates           | 8            |
| policy_entropy     | 0.66783226   |
| policy_loss        | 0.0053887265 |
| serial_timesteps   | 3200         |
| time_elapsed       | 45.6         |
| time_remaining     | 5.51         |
| total_timesteps    | 96000        |
| true_eprew         | 183          |
| value_loss         | 102.2025     |
-------------------------------------
Current reward shaping 0.904
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7871863842010498 seconds
Total simulation time for 400 steps: 4.567793130874634 	 Other agent action time: 0 	 87.5696399857339 steps/s
Curr learning rate 0.0009191919191919192 	 Curr reward per step 0.7868066666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.67it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.27it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.57it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 170.92it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.43it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.77it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 163.17it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 158.56it/s]
------------------------------------
| approxkl           | 0.004246938 |
| clipfrac           | 0.29587495  |
| eplenmean          | 400         |
| eprewmean          | 327         |
| explained_variance | 0.247       |
| fps                | 2362        |
| nupdates           | 9           |
| policy_entropy     | 0.66717654  |
| policy_loss        | 0.006469817 |
| serial_timesteps   | 3600        |
| time_elapsed       | 50.7        |
| time_remaining     | 5.35        |
| total_timesteps    | 108000      |
| true_eprew         | 180         |
| value_loss         | 94.634705   |
------------------------------------
Current reward shaping 0.892
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7563169002532959 seconds
Total simulation time for 400 steps: 4.407598257064819 	 Other agent action time: 0 	 90.75237275966585 steps/s
Curr learning rate 0.0009090909090909091 	 Curr reward per step 0.7886546666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 168.86it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.23it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 149.69it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 173.59it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 165.33it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.81it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 143.98it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 147.01it/s]
------------------------------------
| approxkl           | 0.002278752 |
| clipfrac           | 0.253302    |
| eplenmean          | 400         |
| eprewmean          | 321         |
| explained_variance | 0.239       |
| fps                | 2432        |
| nupdates           | 10          |
| policy_entropy     | 0.81001645  |
| policy_loss        | 0.003260817 |
| serial_timesteps   | 4000        |
| time_elapsed       | 55.6        |
| time_remaining     | 5.19        |
| total_timesteps    | 120000      |
| true_eprew         | 178         |
| value_loss         | 93.31326    |
------------------------------------
Current reward shaping 0.88
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7619256973266602 seconds
Total simulation time for 400 steps: 4.464355230331421 	 Other agent action time: 0 	 89.59860480688162 steps/s
Curr learning rate 0.000898989898989899 	 Curr reward per step 0.80136

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 154.59it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.90it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 160.32it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 155.72it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 151.54it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 173.55it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.78it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.23it/s]
------------------------------------
| approxkl           | 0.006770788 |
| clipfrac           | 0.40045834  |
| eplenmean          | 400         |
| eprewmean          | 318         |
| explained_variance | 0.243       |
| fps                | 2405        |
| nupdates           | 11          |
| policy_entropy     | 0.79945457  |
| policy_loss        | 0.009804355 |
| serial_timesteps   | 4400        |
| time_elapsed       | 60.6        |
| time_remaining     | 5.05        |
| total_timesteps    | 132000      |
| true_eprew         | 177         |
| value_loss         | 91.538      |
------------------------------------
Current reward shaping 0.868
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8089244365692139 seconds
Total simulation time for 400 steps: 4.597122669219971 	 Other agent action time: 0 	 87.01094766911476 steps/s
Curr learning rate 0.0008888888888888889 	 Curr reward per step 0.8258016666666665

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 142.02it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.18it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 167.98it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.32it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.83it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 156.42it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 152.08it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 154.18it/s]
-------------------------------------
| approxkl           | 0.0028625024 |
| clipfrac           | 0.24072924   |
| eplenmean          | 400          |
| eprewmean          | 320          |
| explained_variance | 0.214        |
| fps                | 2337         |
| nupdates           | 12           |
| policy_entropy     | 0.6588692    |
| policy_loss        | 0.004768284  |
| serial_timesteps   | 4800         |
| time_elapsed       | 65.7         |
| time_remaining     | 4.93         |
| total_timesteps    | 144000       |
| true_eprew         | 180          |
| value_loss         | 94.54145     |
-------------------------------------
Current reward shaping 0.856
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.753119707107544 seconds
Total simulation time for 400 steps: 4.450182914733887 	 Other agent action time: 0 	 89.88394582066731 steps/s
Curr learning rate 0.0008787878787878789 	 Curr reward per step 0.8015113333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 154.61it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.61it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 135.88it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 153.22it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 163.17it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.49it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 168.36it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 161.80it/s]
------------------------------------
| approxkl           | 0.028593536 |
| clipfrac           | 0.4863751   |
| eplenmean          | 400         |
| eprewmean          | 322         |
| explained_variance | 0.22        |
| fps                | 2406        |
| nupdates           | 13          |
| policy_entropy     | 0.7974765   |
| policy_loss        | 0.023708735 |
| serial_timesteps   | 5200        |
| time_elapsed       | 70.7        |
| time_remaining     | 4.81        |
| total_timesteps    | 156000      |
| true_eprew         | 181         |
| value_loss         | 91.37981    |
------------------------------------
Current reward shaping 0.844
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7924172878265381 seconds
Total simulation time for 400 steps: 4.517135381698608 	 Other agent action time: 0 	 88.55169619680191 steps/s
Curr learning rate 0.0008686868686868688 	 Curr reward per step 0.7302339999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 128.39it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.64it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 135.07it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 158.78it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 157.53it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 161.73it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.38it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 155.99it/s]
------------------------------------
| approxkl           | 0.015084636 |
| clipfrac           | 0.49795827  |
| eplenmean          | 400         |
| eprewmean          | 315         |
| explained_variance | 0.258       |
| fps                | 2365        |
| nupdates           | 14          |
| policy_entropy     | 0.7868101   |
| policy_loss        | 0.019201305 |
| serial_timesteps   | 5600        |
| time_elapsed       | 75.8        |
| time_remaining     | 4.69        |
| total_timesteps    | 168000      |
| true_eprew         | 178         |
| value_loss         | 77.38019    |
------------------------------------
Current reward shaping 0.832
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7441434860229492 seconds
Total simulation time for 400 steps: 4.459266424179077 	 Other agent action time: 0 	 89.70085255079539 steps/s
Curr learning rate 0.0008585858585858587 	 Curr reward per step 0.7173546666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 164.78it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 147.41it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.02it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.48it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 152.18it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 156.80it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.97it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 162.69it/s]
-------------------------------------
| approxkl           | 0.0053840233 |
| clipfrac           | 0.2599062    |
| eplenmean          | 400          |
| eprewmean          | 303          |
| explained_variance | 0.207        |
| fps                | 2406         |
| nupdates           | 15           |
| policy_entropy     | 0.6136521    |
| policy_loss        | 0.0066653527 |
| serial_timesteps   | 6000         |
| time_elapsed       | 80.8         |
| time_remaining     | 4.58         |
| total_timesteps    | 180000       |
| true_eprew         | 172          |
| value_loss         | 91.85434     |
-------------------------------------
Current reward shaping 0.8200000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7524185180664062 seconds
Total simulation time for 400 steps: 4.422534704208374 	 Other agent action time: 0 	 90.4458702425489 steps/s
Curr learning rate 0.0008484848484848486 	 Curr reward per step 0.7229366666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 154.22it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.90it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.50it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 118.35it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 113.24it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 110.38it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 110.60it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 108.27it/s]
-------------------------------------
| approxkl           | 0.0058594523 |
| clipfrac           | 0.29693747   |
| eplenmean          | 400          |
| eprewmean          | 292          |
| explained_variance | 0.213        |
| fps                | 2362         |
| nupdates           | 16           |
| policy_entropy     | 0.6404211    |
| policy_loss        | 0.0077743074 |
| serial_timesteps   | 6400         |
| time_elapsed       | 85.9         |
| time_remaining     | 4.47         |
| total_timesteps    | 192000       |
| true_eprew         | 167          |
| value_loss         | 79.95091     |
-------------------------------------
Current reward shaping 0.808
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7690315246582031 seconds
Total simulation time for 400 steps: 4.483151912689209 	 Other agent action time: 0 	 89.22294131230116 steps/s
Curr learning rate 0.0008383838383838385 	 Curr reward per step 0.7231513333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 164.63it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.67it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 144.96it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 174.05it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 137.37it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 164.58it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 161.74it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.43it/s]
------------------------------------
| approxkl           | 0.011980811 |
| clipfrac           | 0.3492292   |
| eplenmean          | 400         |
| eprewmean          | 290         |
| explained_variance | 0.221       |
| fps                | 2393        |
| nupdates           | 17          |
| policy_entropy     | 0.6281291   |
| policy_loss        | 0.014083875 |
| serial_timesteps   | 6800        |
| time_elapsed       | 90.9        |
| time_remaining     | 4.37        |
| total_timesteps    | 204000      |
| true_eprew         | 167         |
| value_loss         | 85.13229    |
------------------------------------
Current reward shaping 0.796
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7427749633789062 seconds
Total simulation time for 400 steps: 4.460988759994507 	 Other agent action time: 0 	 89.66622009612338 steps/s
Curr learning rate 0.0008282828282828282 	 Curr reward per step 0.6965266666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 165.85it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.58it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 169.86it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 169.97it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 162.14it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 173.84it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 165.09it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 176.27it/s]
-------------------------------------
| approxkl           | 0.0051956484 |
| clipfrac           | 0.34054166   |
| eplenmean          | 400          |
| eprewmean          | 284          |
| explained_variance | 0.167        |
| fps                | 2417         |
| nupdates           | 18           |
| policy_entropy     | 0.76723677   |
| policy_loss        | 0.00766929   |
| serial_timesteps   | 7200         |
| time_elapsed       | 95.8         |
| time_remaining     | 4.26         |
| total_timesteps    | 216000       |
| true_eprew         | 164          |
| value_loss         | 89.4739      |
-------------------------------------
Current reward shaping 0.784
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8127522468566895 seconds
Total simulation time for 400 steps: 4.561945199966431 	 Other agent action time: 0 	 87.68189499578895 steps/s
Curr learning rate 0.0008181818181818183 	 Curr reward per step 0.7374933333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 163.90it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.49it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.65it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 159.76it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 141.04it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 157.33it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.08it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.27it/s]
------------------------------------
| approxkl           | 0.009330042 |
| clipfrac           | 0.4106041   |
| eplenmean          | 400         |
| eprewmean          | 287         |
| explained_variance | 0.273       |
| fps                | 2355        |
| nupdates           | 19          |
| policy_entropy     | 0.808041    |
| policy_loss        | 0.011396538 |
| serial_timesteps   | 7600        |
| time_elapsed       | 101         |
| time_remaining     | 4.16        |
| total_timesteps    | 228000      |
| true_eprew         | 167         |
| value_loss         | 84.2302     |
------------------------------------
Current reward shaping 0.772
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7624661922454834 seconds
Total simulation time for 400 steps: 4.501246929168701 	 Other agent action time: 0 	 88.86426501242241 steps/s
Curr learning rate 0.0008080808080808081 	 Curr reward per step 0.754186

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.83it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 177.64it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 178.68it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 170.06it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 162.20it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 155.80it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 168.71it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 171.11it/s]
-------------------------------------
| approxkl           | 0.0033446706 |
| clipfrac           | 0.323        |
| eplenmean          | 400          |
| eprewmean          | 293          |
| explained_variance | 0.206        |
| fps                | 2397         |
| nupdates           | 20           |
| policy_entropy     | 0.8222334    |
| policy_loss        | 0.004755143  |
| serial_timesteps   | 8000         |
| time_elapsed       | 106          |
| time_remaining     | 4.06         |
| total_timesteps    | 240000       |
| true_eprew         | 172          |
| value_loss         | 88.88415     |
-------------------------------------
Current reward shaping 0.76
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8079195022583008 seconds
Total simulation time for 400 steps: 4.5740742683410645 	 Other agent action time: 0 	 87.44938899845911 steps/s
Curr learning rate 0.000797979797979798 	 Curr reward per step 0.6958199999999998

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.58it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 171.14it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.42it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 149.33it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 172.93it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.30it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 143.79it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 154.39it/s]
-------------------------------------
| approxkl           | 0.005540095  |
| clipfrac           | 0.30320832   |
| eplenmean          | 400          |
| eprewmean          | 291          |
| explained_variance | 0.355        |
| fps                | 2352         |
| nupdates           | 21           |
| policy_entropy     | 0.71159226   |
| policy_loss        | 0.0066598877 |
| serial_timesteps   | 8400         |
| time_elapsed       | 111          |
| time_remaining     | 3.97         |
| total_timesteps    | 252000       |
| true_eprew         | 172          |
| value_loss         | 81.03691     |
-------------------------------------
Current reward shaping 0.748
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.756523609161377 seconds
Total simulation time for 400 steps: 4.427453517913818 	 Other agent action time: 0 	 90.34538666110647 steps/s
Curr learning rate 0.0007878787878787879 	 Curr reward per step 0.7303026666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 163.36it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 157.28it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 163.01it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.33it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.08it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 131.88it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 163.87it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 145.09it/s]
------------------------------------
| approxkl           | 0.006290564 |
| clipfrac           | 0.2968854   |
| eplenmean          | 400         |
| eprewmean          | 290         |
| explained_variance | 0.266       |
| fps                | 2411        |
| nupdates           | 22          |
| policy_entropy     | 0.6881497   |
| policy_loss        | 0.009418501 |
| serial_timesteps   | 8800        |
| time_elapsed       | 116         |
| time_remaining     | 3.87        |
| total_timesteps    | 264000      |
| true_eprew         | 173         |
| value_loss         | 87.14934    |
------------------------------------
Current reward shaping 0.736
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7453672885894775 seconds
Total simulation time for 400 steps: 4.427973508834839 	 Other agent action time: 0 	 90.33477711687001 steps/s
Curr learning rate 0.0007777777777777778 	 Curr reward per step 0.7271519999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 134.38it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 155.33it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 160.53it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 162.35it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 162.78it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 152.67it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.37it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 166.02it/s]
-------------------------------------
| approxkl           | 0.0043432256 |
| clipfrac           | 0.33967704   |
| eplenmean          | 400          |
| eprewmean          | 288          |
| explained_variance | 0.226        |
| fps                | 2414         |
| nupdates           | 23           |
| policy_entropy     | 0.81940395   |
| policy_loss        | 0.006787396  |
| serial_timesteps   | 9200         |
| time_elapsed       | 121          |
| time_remaining     | 3.77         |
| total_timesteps    | 276000       |
| true_eprew         | 172          |
| value_loss         | 86.99717     |
-------------------------------------
Current reward shaping 0.724
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8020238876342773 seconds
Total simulation time for 400 steps: 4.533260822296143 	 Other agent action time: 0 	 88.23670547096295 steps/s
Curr learning rate 0.0007676767676767678 	 Curr reward per step 0.7435593333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 117.80it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 148.76it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 153.48it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 146.53it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.62it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 170.52it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 163.82it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 148.33it/s]
------------------------------------
| approxkl           | 0.022389952 |
| clipfrac           | 0.4751563   |
| eplenmean          | 400         |
| eprewmean          | 294         |
| explained_variance | 0.267       |
| fps                | 2352        |
| nupdates           | 24          |
| policy_entropy     | 0.7075192   |
| policy_loss        | 0.023023974 |
| serial_timesteps   | 9600        |
| time_elapsed       | 126         |
| time_remaining     | 3.68        |
| total_timesteps    | 288000      |
| true_eprew         | 177         |
| value_loss         | 82.14703    |
------------------------------------
Current reward shaping 0.712
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7421755790710449 seconds
Total simulation time for 400 steps: 4.397862672805786 	 Other agent action time: 0 	 90.95327202311312 steps/s
Curr learning rate 0.0007575757575757577 	 Curr reward per step 0.7434193333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.51it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.65it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 148.32it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 160.84it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 169.14it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 169.93it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 162.06it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 166.34it/s]
------------------------------------
| approxkl           | 0.020344233 |
| clipfrac           | 0.5255418   |
| eplenmean          | 400         |
| eprewmean          | 293         |
| explained_variance | 0.229       |
| fps                | 2439        |
| nupdates           | 25          |
| policy_entropy     | 0.8105896   |
| policy_loss        | 0.023398226 |
| serial_timesteps   | 10000       |
| time_elapsed       | 131         |
| time_remaining     | 3.58        |
| total_timesteps    | 300000      |
| true_eprew         | 178         |
| value_loss         | 84.54899    |
------------------------------------
Current reward shaping 0.7
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7893786430358887 seconds
Total simulation time for 400 steps: 4.560473442077637 	 Other agent action time: 0 	 87.71019173346399 steps/s
Curr learning rate 0.0007474747474747475 	 Curr reward per step 0.7233666666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 154.22it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 159.54it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 159.14it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 150.43it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 165.38it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 152.19it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.44it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 161.60it/s]
------------------------------------
| approxkl           | 0.005763388 |
| clipfrac           | 0.36383334  |
| eplenmean          | 400         |
| eprewmean          | 295         |
| explained_variance | 0.304       |
| fps                | 2355        |
| nupdates           | 26          |
| policy_entropy     | 0.808997    |
| policy_loss        | 0.0086896   |
| serial_timesteps   | 10400       |
| time_elapsed       | 136         |
| time_remaining     | 3.49        |
| total_timesteps    | 312000      |
| true_eprew         | 180         |
| value_loss         | 84.53734    |
------------------------------------
Current reward shaping 0.688
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.760413408279419 seconds
Total simulation time for 400 steps: 4.469563245773315 	 Other agent action time: 0 	 89.49420290187498 steps/s
Curr learning rate 0.0007373737373737374 	 Curr reward per step 0.6908666666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 167.01it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 169.45it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 169.03it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 161.14it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 171.55it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.16it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 160.78it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 147.79it/s]
-------------------------------------
| approxkl           | 0.002693689  |
| clipfrac           | 0.2805416    |
| eplenmean          | 400          |
| eprewmean          | 289          |
| explained_variance | 0.228        |
| fps                | 2404         |
| nupdates           | 27           |
| policy_entropy     | 0.8398137    |
| policy_loss        | 0.0042805998 |
| serial_timesteps   | 10800        |
| time_elapsed       | 141          |
| time_remaining     | 3.4          |
| total_timesteps    | 324000       |
| true_eprew         | 178          |
| value_loss         | 88.1446      |
-------------------------------------
Current reward shaping 0.6759999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7647709846496582 seconds
Total simulation time for 400 steps: 4.5478174686431885 	 Other agent action time: 0 	 87.9542775755548 steps/s
Curr learning rate 0.0007272727272727272 	 Curr reward per step 0.6901569999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.75it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.63it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 170.89it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 170.91it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.48it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 158.13it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 161.63it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 154.74it/s]
-------------------------------------
| approxkl           | 0.0075947493 |
| clipfrac           | 0.39934367   |
| eplenmean          | 400          |
| eprewmean          | 282          |
| explained_variance | 0.168        |
| fps                | 2367         |
| nupdates           | 28           |
| policy_entropy     | 0.77161515   |
| policy_loss        | 0.011395922  |
| serial_timesteps   | 11200        |
| time_elapsed       | 146          |
| time_remaining     | 3.31         |
| total_timesteps    | 336000       |
| true_eprew         | 175          |
| value_loss         | 81.76506     |
-------------------------------------
Current reward shaping 0.6639999999999999
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7989640235900879 seconds
Total simulation time for 400 steps: 4.4987688064575195 	 Other agent action time: 0 	 88.91321541703614 steps/s
Curr learning rate 0.0007171717171717171 	 Curr reward per step 0.7178753333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 158.66it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.60it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 167.98it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.18it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 169.57it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 162.97it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.82it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 157.39it/s]
------------------------------------
| approxkl           | 0.004979643 |
| clipfrac           | 0.3328854   |
| eplenmean          | 400         |
| eprewmean          | 282         |
| explained_variance | 0.338       |
| fps                | 2393        |
| nupdates           | 29          |
| policy_entropy     | 0.7804891   |
| policy_loss        | 0.007312628 |
| serial_timesteps   | 11600       |
| time_elapsed       | 151         |
| time_remaining     | 3.21        |
| total_timesteps    | 348000      |
| true_eprew         | 176         |
| value_loss         | 81.51085    |
------------------------------------
Current reward shaping 0.652
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7363827228546143 seconds
Total simulation time for 400 steps: 4.47109317779541 	 Other agent action time: 0 	 89.46357950813955 steps/s
Curr learning rate 0.0007070707070707071 	 Curr reward per step 0.7179733333333335

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 154.98it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.99it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 167.80it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 171.76it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 168.69it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 170.50it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 177.14it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.52it/s]
------------------------------------
| approxkl           | 0.028979395 |
| clipfrac           | 0.52254164  |
| eplenmean          | 400         |
| eprewmean          | 282         |
| explained_variance | 0.319       |
| fps                | 2413        |
| nupdates           | 30          |
| policy_entropy     | 0.8623131   |
| policy_loss        | 0.029784914 |
| serial_timesteps   | 12000       |
| time_elapsed       | 156         |
| time_remaining     | 3.12        |
| total_timesteps    | 360000      |
| true_eprew         | 178         |
| value_loss         | 76.03853    |
------------------------------------
Current reward shaping 0.64
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8076841831207275 seconds
Total simulation time for 400 steps: 4.577630996704102 	 Other agent action time: 0 	 87.38144256013653 steps/s
Curr learning rate 0.000696969696969697 	 Curr reward per step 0.7401466666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.70it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.64it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 164.46it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.30it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.20it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.68it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.42it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 160.07it/s]
-------------------------------------
| approxkl           | 0.0031584005 |
| clipfrac           | 0.2565312    |
| eplenmean          | 400          |
| eprewmean          | 289          |
| explained_variance | 0.259        |
| fps                | 2356         |
| nupdates           | 31           |
| policy_entropy     | 0.7378431    |
| policy_loss        | 0.0046591754 |
| serial_timesteps   | 12400        |
| time_elapsed       | 161          |
| time_remaining     | 3.03         |
| total_timesteps    | 372000       |
| true_eprew         | 183          |
| value_loss         | 83.11183     |
-------------------------------------
Current reward shaping 0.628
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.762340784072876 seconds
Total simulation time for 400 steps: 4.447240829467773 	 Other agent action time: 0 	 89.94340880969791 steps/s
Curr learning rate 0.0006868686868686869 	 Curr reward per step 0.7574473333333336

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 155.57it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 147.68it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.47it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.72it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 152.07it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 145.37it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 154.16it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 156.13it/s]
-------------------------------------
| approxkl           | 0.0074320645 |
| clipfrac           | 0.3443021    |
| eplenmean          | 400          |
| eprewmean          | 295          |
| explained_variance | 0.219        |
| fps                | 2405         |
| nupdates           | 32           |
| policy_entropy     | 0.82412016   |
| policy_loss        | 0.010057236  |
| serial_timesteps   | 12800        |
| time_elapsed       | 166          |
| time_remaining     | 2.94         |
| total_timesteps    | 384000       |
| true_eprew         | 188          |
| value_loss         | 83.13388     |
-------------------------------------
Current reward shaping 0.616
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7588613033294678 seconds
Total simulation time for 400 steps: 4.502745866775513 	 Other agent action time: 0 	 88.83468262143924 steps/s
Curr learning rate 0.0006767676767676768 	 Curr reward per step 0.7374913333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.24it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 156.24it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 171.40it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 155.15it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 168.74it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 170.97it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 142.59it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 148.79it/s]
-------------------------------------
| approxkl           | 0.0034086225 |
| clipfrac           | 0.24883333   |
| eplenmean          | 400          |
| eprewmean          | 297          |
| explained_variance | 0.239        |
| fps                | 2382         |
| nupdates           | 33           |
| policy_entropy     | 0.7090134    |
| policy_loss        | 0.004629413  |
| serial_timesteps   | 13200        |
| time_elapsed       | 171          |
| time_remaining     | 2.85         |
| total_timesteps    | 396000       |
| true_eprew         | 190          |
| value_loss         | 81.83114     |
-------------------------------------
Current reward shaping 0.604
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.8089640140533447 seconds
Total simulation time for 400 steps: 4.64184832572937 	 Other agent action time: 0 	 86.17257004776181 steps/s
Curr learning rate 0.0006666666666666668 	 Curr reward per step 0.7574380000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.57it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 171.10it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.98it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.19it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.04it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 150.87it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 168.45it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 153.89it/s]
-------------------------------------
| approxkl           | 0.004397369  |
| clipfrac           | 0.3121875    |
| eplenmean          | 400          |
| eprewmean          | 300          |
| explained_variance | 0.218        |
| fps                | 2327         |
| nupdates           | 34           |
| policy_entropy     | 0.798171     |
| policy_loss        | 0.0061904443 |
| serial_timesteps   | 13600        |
| time_elapsed       | 176          |
| time_remaining     | 2.77         |
| total_timesteps    | 408000       |
| true_eprew         | 194          |
| value_loss         | 83.44411     |
-------------------------------------
Current reward shaping 0.5920000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7463304996490479 seconds
Total simulation time for 400 steps: 4.439488410949707 	 Other agent action time: 0 	 90.10047171503506 steps/s
Curr learning rate 0.0006565656565656567 	 Curr reward per step 0.7508146666666669

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 146.11it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 157.15it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 169.62it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.12it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.22it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.28it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 152.91it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 171.98it/s]
-------------------------------------
| approxkl           | 0.0012934158 |
| clipfrac           | 0.20618749   |
| eplenmean          | 400          |
| eprewmean          | 299          |
| explained_variance | 0.218        |
| fps                | 2418         |
| nupdates           | 35           |
| policy_entropy     | 0.79819596   |
| policy_loss        | 0.0014301314 |
| serial_timesteps   | 14000        |
| time_elapsed       | 181          |
| time_remaining     | 2.68         |
| total_timesteps    | 420000       |
| true_eprew         | 195          |
| value_loss         | 83.47106     |
-------------------------------------
Current reward shaping 0.5800000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7621088027954102 seconds
Total simulation time for 400 steps: 4.58559513092041 	 Other agent action time: 0 	 87.22968089852122 steps/s
Curr learning rate 0.0006464646464646465 	 Curr reward per step 0.771725

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.23it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.67it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 151.25it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.34it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.60it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 166.13it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 154.75it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 158.24it/s]
-------------------------------------
| approxkl           | 0.0010457712 |
| clipfrac           | 0.15406248   |
| eplenmean          | 400          |
| eprewmean          | 304          |
| explained_variance | 0.179        |
| fps                | 2346         |
| nupdates           | 36           |
| policy_entropy     | 0.6720184    |
| policy_loss        | 0.0012003038 |
| serial_timesteps   | 14400        |
| time_elapsed       | 187          |
| time_remaining     | 2.59         |
| total_timesteps    | 432000       |
| true_eprew         | 199          |
| value_loss         | 85.709526    |
-------------------------------------
Current reward shaping 0.5680000000000001
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7935066223144531 seconds
Total simulation time for 400 steps: 4.6095335483551025 	 Other agent action time: 0 	 86.77667616558269 steps/s
Curr learning rate 0.0006363636363636364 	 Curr reward per step 0.7526553333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 138.77it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 165.83it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 160.15it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 164.34it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 158.04it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 128.05it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.33it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 144.61it/s]
------------------------------------
| approxkl           | 0.006234723 |
| clipfrac           | 0.3231979   |
| eplenmean          | 400         |
| eprewmean          | 304         |
| explained_variance | 0.233       |
| fps                | 2323        |
| nupdates           | 37          |
| policy_entropy     | 0.793076    |
| policy_loss        | 0.00854318  |
| serial_timesteps   | 14800       |
| time_elapsed       | 192         |
| time_remaining     | 2.5         |
| total_timesteps    | 444000      |
| true_eprew         | 201         |
| value_loss         | 85.59287    |
------------------------------------
Current reward shaping 0.556
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7597393989562988 seconds
Total simulation time for 400 steps: 4.490365982055664 	 Other agent action time: 0 	 89.07959876733305 steps/s
Curr learning rate 0.0006262626262626263 	 Curr reward per step 0.7474186666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 158.71it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 160.57it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.24it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 157.52it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 155.73it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 149.37it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 147.45it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 151.87it/s]
-------------------------------------
| approxkl           | 0.0021488331 |
| clipfrac           | 0.24437504   |
| eplenmean          | 400          |
| eprewmean          | 303          |
| explained_variance | 0.245        |
| fps                | 2384         |
| nupdates           | 38           |
| policy_entropy     | 0.79485196   |
| policy_loss        | 0.0027166442 |
| serial_timesteps   | 15200        |
| time_elapsed       | 197          |
| time_remaining     | 2.42         |
| total_timesteps    | 456000       |
| true_eprew         | 201          |
| value_loss         | 82.57384     |
-------------------------------------
Current reward shaping 0.544
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.788013219833374 seconds
Total simulation time for 400 steps: 4.560670614242554 	 Other agent action time: 0 	 87.7063997454315 steps/s
Curr learning rate 0.0006161616161616161 	 Curr reward per step 0.7332266666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.80it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 138.43it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 153.58it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 170.75it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 174.02it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 171.76it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 168.88it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 160.24it/s]
------------------------------------
| approxkl           | 0.004857211 |
| clipfrac           | 0.34177077  |
| eplenmean          | 400         |
| eprewmean          | 299         |
| explained_variance | 0.277       |
| fps                | 2361        |
| nupdates           | 39          |
| policy_entropy     | 0.7656599   |
| policy_loss        | 0.006839449 |
| serial_timesteps   | 15600       |
| time_elapsed       | 202         |
| time_remaining     | 2.33        |
| total_timesteps    | 468000      |
| true_eprew         | 200         |
| value_loss         | 84.29213    |
------------------------------------
Current reward shaping 0.532
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7758290767669678 seconds
Total simulation time for 400 steps: 4.515226125717163 	 Other agent action time: 0 	 88.58914013668964 steps/s
Curr learning rate 0.0006060606060606061 	 Curr reward per step 0.717581

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 142.74it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.89it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 156.98it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 157.76it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 150.28it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 145.96it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 140.59it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 133.20it/s]
-------------------------------------
| approxkl           | 0.0025829517 |
| clipfrac           | 0.2455312    |
| eplenmean          | 400          |
| eprewmean          | 293          |
| explained_variance | 0.3          |
| fps                | 2360         |
| nupdates           | 40           |
| policy_entropy     | 0.7724229    |
| policy_loss        | 0.003354403  |
| serial_timesteps   | 16000        |
| time_elapsed       | 207          |
| time_remaining     | 2.24         |
| total_timesteps    | 480000       |
| true_eprew         | 197          |
| value_loss         | 84.77514     |
-------------------------------------
Current reward shaping 0.52
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7522673606872559 seconds
Total simulation time for 400 steps: 4.49870753288269 	 Other agent action time: 0 	 88.91442643831688 steps/s
Curr learning rate 0.000595959595959596 	 Curr reward per step 0.7387400000000001

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 112.63it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 118.48it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 121.05it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 115.40it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 126.26it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 118.63it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 119.28it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 118.69it/s]
-------------------------------------
| approxkl           | 0.0062191887 |
| clipfrac           | 0.35903132   |
| eplenmean          | 400          |
| eprewmean          | 294          |
| explained_variance | 0.213        |
| fps                | 2307         |
| nupdates           | 41           |
| policy_entropy     | 0.7880079    |
| policy_loss        | 0.00990185   |
| serial_timesteps   | 16400        |
| time_elapsed       | 212          |
| time_remaining     | 2.16         |
| total_timesteps    | 492000       |
| true_eprew         | 199          |
| value_loss         | 86.161575    |
-------------------------------------
Current reward shaping 0.508
Current self-play randomization 1
SP envs: 30/30
Other agent actions took 0.7414157390594482 seconds
Total simulation time for 400 steps: 4.472281455993652 	 Other agent action time: 0 	 89.43980917478459 steps/s
Curr learning rate 0.0005858585858585859 	 Curr reward per step 0.742298666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.32it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 166.49it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 165.21it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 140.75it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 162.02it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 157.03it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 168.75it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.02it/s]
-------------------------------------
| approxkl           | 0.0028806208 |
| clipfrac           | 0.22566669   |
| eplenmean          | 400          |
| eprewmean          | 294          |
| explained_variance | 0.21         |
| fps                | 2398         |
| nupdates           | 42           |
| policy_entropy     | 0.7299045    |
| policy_loss        | 0.0037889252 |
| serial_timesteps   | 16800        |
| time_elapsed       | 217          |
| time_remaining     | 2.07         |
| total_timesteps    | 504000       |
| true_eprew         | 201          |
| value_loss         | 80.61345     |
-------------------------------------
Current reward shaping 0.496
Current self-play randomization 0.9984
SP envs: 30/30
Other agent actions took 0.7524070739746094 seconds
Total simulation time for 400 steps: 4.534507751464844 	 Other agent action time: 0 	 88.21244155350325 steps/s
Curr learning rate 0.0005757575757575758 	 Curr reward per step 0.7387239999999999

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.71it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 153.84it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.81it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 160.75it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 147.02it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 166.49it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 168.31it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 162.16it/s]
-------------------------------------
| approxkl           | 0.0013247409 |
| clipfrac           | 0.17846876   |
| eplenmean          | 400          |
| eprewmean          | 296          |
| explained_variance | 0.249        |
| fps                | 2371         |
| nupdates           | 43           |
| policy_entropy     | 0.7055891    |
| policy_loss        | 0.0013088302 |
| serial_timesteps   | 17200        |
| time_elapsed       | 222          |
| time_remaining     | 1.98         |
| total_timesteps    | 516000       |
| true_eprew         | 204          |
| value_loss         | 76.5963      |
-------------------------------------
Current reward shaping 0.484
Current self-play randomization 0.9936
SP envs: 30/30
Other agent actions took 0.7899274826049805 seconds
Total simulation time for 400 steps: 4.542861223220825 	 Other agent action time: 0 	 88.05023537928055 steps/s
Curr learning rate 0.0005656565656565657 	 Curr reward per step 0.7236873333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 156.16it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.08it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 169.22it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 172.62it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 148.80it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 159.54it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 155.12it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 167.94it/s]
------------------------------------
| approxkl           | 0.009096434 |
| clipfrac           | 0.3487708   |
| eplenmean          | 400         |
| eprewmean          | 294         |
| explained_variance | 0.214       |
| fps                | 2369        |
| nupdates           | 44          |
| policy_entropy     | 0.7171969   |
| policy_loss        | 0.010327072 |
| serial_timesteps   | 17600       |
| time_elapsed       | 227         |
| time_remaining     | 1.89        |
| total_timesteps    | 528000      |
| true_eprew         | 204         |
| value_loss         | 80.106125   |
------------------------------------
Current reward shaping 0.472
Current self-play randomization 0.9888
SP envs: 30/30
Other agent actions took 0.7572171688079834 seconds
Total simulation time for 400 steps: 4.516462326049805 	 Other agent action time: 0 	 88.5648924143354 steps/s
Curr learning rate 0.0005555555555555557 	 Curr reward per step 0.7095433333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 164.01it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 171.66it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.81it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 157.32it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 171.70it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 145.11it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.23it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 169.78it/s]
-------------------------------------
| approxkl           | 0.0027628033 |
| clipfrac           | 0.22299996   |
| eplenmean          | 400          |
| eprewmean          | 290          |
| explained_variance | 0.225        |
| fps                | 2385         |
| nupdates           | 45           |
| policy_entropy     | 0.7410179    |
| policy_loss        | 0.0034123461 |
| serial_timesteps   | 18000        |
| time_elapsed       | 232          |
| time_remaining     | 1.81         |
| total_timesteps    | 540000       |
| true_eprew         | 203          |
| value_loss         | 82.50952     |
-------------------------------------
Current reward shaping 0.45999999999999996
Current self-play randomization 0.984
SP envs: 29/30
Other agent actions took 6.13217568397522 seconds
Total simulation time for 400 steps: 10.011888027191162 	 Other agent action time: 0 	 39.952504354188235 steps/s
Curr learning rate 0.0005454545454545455 	 Curr reward per step 0.69485

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 162.29it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.09it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.75it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 155.52it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 170.73it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 160.39it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 169.76it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 163.33it/s]
-------------------------------------
| approxkl           | 0.0027047782 |
| clipfrac           | 0.22943747   |
| eplenmean          | 400          |
| eprewmean          | 285          |
| explained_variance | 0.168        |
| fps                | 1139         |
| nupdates           | 46           |
| policy_entropy     | 0.7053757    |
| policy_loss        | 0.004152142  |
| serial_timesteps   | 18400        |
| time_elapsed       | 243          |
| time_remaining     | 1.76         |
| total_timesteps    | 552000       |
| true_eprew         | 201          |
| value_loss         | 87.09206     |
-------------------------------------
Current reward shaping 0.44799999999999995
Current self-play randomization 0.9792
SP envs: 29/30
Other agent actions took 5.963573455810547 seconds
Total simulation time for 400 steps: 9.726866960525513 	 Other agent action time: 0 	 41.123210754636375 steps/s
Curr learning rate 0.0005353535353535353 	 Curr reward per step 0.680104

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 157.51it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 149.17it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 144.87it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 147.17it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 159.99it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.21it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 140.98it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 149.11it/s]
------------------------------------
| approxkl           | 0.003917913 |
| clipfrac           | 0.28633338  |
| eplenmean          | 400         |
| eprewmean          | 279         |
| explained_variance | 0.342       |
| fps                | 1167        |
| nupdates           | 47          |
| policy_entropy     | 0.713437    |
| policy_loss        | 0.006193289 |
| serial_timesteps   | 18800       |
| time_elapsed       | 253         |
| time_remaining     | 1.71        |
| total_timesteps    | 564000      |
| true_eprew         | 198         |
| value_loss         | 85.515305   |
------------------------------------
Current reward shaping 0.43600000000000005
Current self-play randomization 0.9744
SP envs: 30/30
Other agent actions took 0.7451591491699219 seconds
Total simulation time for 400 steps: 4.460608959197998 	 Other agent action time: 0 	 89.67385477159571 steps/s
Curr learning rate 0.0005252525252525252 	 Curr reward per step 0.6917673333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 152.42it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 161.64it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 166.85it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 167.02it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 169.72it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 166.22it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 160.45it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.05it/s]
-------------------------------------
| approxkl           | 0.0050675184 |
| clipfrac           | 0.31896865   |
| eplenmean          | 400          |
| eprewmean          | 277          |
| explained_variance | 0.229        |
| fps                | 2409         |
| nupdates           | 48           |
| policy_entropy     | 0.71688664   |
| policy_loss        | 0.008372992  |
| serial_timesteps   | 19200        |
| time_elapsed       | 258          |
| time_remaining     | 1.61         |
| total_timesteps    | 576000       |
| true_eprew         | 198          |
| value_loss         | 77.62928     |
-------------------------------------
Current reward shaping 0.42400000000000004
Current self-play randomization 0.9696
SP envs: 29/30
Other agent actions took 5.9840004444122314 seconds
Total simulation time for 400 steps: 9.789146661758423 	 Other agent action time: 0 	 40.86158005606467 steps/s
Curr learning rate 0.0005151515151515151 	 Curr reward per step 0.669122

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 170.79it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 173.32it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 173.77it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 169.00it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 161.71it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 174.19it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 171.97it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 161.25it/s]
-------------------------------------
| approxkl           | 0.0035226725 |
| clipfrac           | 0.24923953   |
| eplenmean          | 400          |
| eprewmean          | 274          |
| explained_variance | 0.202        |
| fps                | 1166         |
| nupdates           | 49           |
| policy_entropy     | 0.717499     |
| policy_loss        | 0.005627412  |
| serial_timesteps   | 19600        |
| time_elapsed       | 268          |
| time_remaining     | 1.55         |
| total_timesteps    | 588000       |
| true_eprew         | 197          |
| value_loss         | 85.377174    |
-------------------------------------
Current reward shaping 0.41200000000000003
Current self-play randomization 0.9648
SP envs: 26/30
Other agent actions took 5.991552352905273 seconds
Total simulation time for 400 steps: 9.785699605941772 	 Other agent action time: 0 	 40.87597372773678 steps/s
Curr learning rate 0.000505050505050505 	 Curr reward per step 0.6107456666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 147.83it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 148.70it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.56it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 162.45it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 154.31it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 168.39it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 159.03it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 142.77it/s]
------------------------------------
| approxkl           | 0.005039141 |
| clipfrac           | 0.35398954  |
| eplenmean          | 400         |
| eprewmean          | 265         |
| explained_variance | 0.156       |
| fps                | 1161        |
| nupdates           | 50          |
| policy_entropy     | 0.78490114  |
| policy_loss        | 0.010432466 |
| serial_timesteps   | 20000       |
| time_elapsed       | 279         |
| time_remaining     | 1.49        |
| total_timesteps    | 600000      |
| true_eprew         | 192         |
| value_loss         | 103.90239   |
------------------------------------
Current reward shaping 0.4
Current self-play randomization 0.96
../../thesis_data/dr_ppo/ppo_bc_train_simple/
PPO agent on index 0:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 


Timestep: 2
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑1O 
X   →0  X 
X D X S X 


Timestep: 3
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑1O 
X ←0    X 
X D X S X 


Timestep: 4
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←1  O 
X   →0  X 
X D X S X 


Timestep: 5
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←1  O 
X ←0    X 
X D X S X 


Timestep: 6
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←1  O 
X   →0  X 
X D X S X 


Timestep: 7
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←1  O 
X     →0X 
X D X S X 


Timestep: 8
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X     →0X 
X D X S X 


Timestep: 9
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X     ↑0X 
X D X S X 


Timestep: 10
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X   ←0  X 
X D X S X 


Timestep: 11
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑0→oO 
X       X 
X D X S X 


Timestep: 12
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O ←0←o  O 
X       X 
X D X S X 


Timestep: 13
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O ←0←o  O 
X       X 
X D X S X 


Timestep: 14
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X ↓0    X 
X D X S X 


Timestep: 15
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 16
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 17
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 18
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 19
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 20
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O   ↑1  O 
X ↓d    X 
X D X S X 


Timestep: 21
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →1O 
X   →d  X 
X D X S X 


Timestep: 22
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X   →d  X 
X D X S X 


Timestep: 23
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X ←d    X 
X D X S X 


Timestep: 24
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X ↓d    X 
X D X S X 


Timestep: 25
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O     →oO 
X ←d    X 
X D X S X 


Timestep: 26
Joint action taken: ('↓', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 27
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 28
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑1  O 
X ↓d    X 
X D X S X 


Timestep: 29
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X   →d  X 
X D X S X 


Timestep: 30
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X   →d  X 
X D X S X 


Timestep: 31
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
X   →d  X 
X D X S X 


Timestep: 32
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
X ←d    X 
X D X S X 


Timestep: 33
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ←d    X 
X D X S X 


Timestep: 34
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X   →d  X 
X D X S X 


Timestep: 35
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ←d    X 
X D X S X 


Timestep: 36
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 37
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   ↑o  O 
X ↓d    X 
X D X S X 


Timestep: 38
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 3 
X X ø1X X 
O   ↑1  O 
X ↓d    X 
X D X S X 


Timestep: 39
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø2X X 
O   ↑1  O 
X ←d    X 
X D X S X 


Timestep: 40
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø3X X 
O ↑d↑1  O 
X       X 
X D X S X 


Timestep: 41
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø4X X 
O   →d→1O 
X       X 
X D X S X 


Timestep: 42
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X ø5X X 
O   ↑d→1O 
X       X 
X D X S X 


Timestep: 43
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø6X X 
O   ↑d→1O 
X       X 
X D X S X 


Timestep: 44
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø7X X 
O   ↑d→1O 
X       X 
X D X S X 


Timestep: 45
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø8X X 
O   ↑d→oO 
X       X 
X D X S X 


Timestep: 46
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø9X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 47
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø10X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 48
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø11X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 49
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø12X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 50
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø13X X 
O   ↑d←oO 
X       X 
X D X S X 


Timestep: 51
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø14X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 52
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø15X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 53
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø16X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 54
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø17X X 
O   ↑d↑oO 
X       X 
X D X S X 


Timestep: 55
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø18XoX 
O   ↑d↑1O 
X       X 
X D X S X 


Timestep: 56
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 0 
X X ø19XoX 
O   ↑d←1O 
X       X 
X D X S X 


Timestep: 57
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XoX 
O   ↑d←1O 
X       X 
X D X S X 


Timestep: 58
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 5 
X X P XoX 
O   ↑s←1O 
X       X 
X D X S X 


Timestep: 59
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s←1O 
X       X 
X D X S X 


Timestep: 60
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←1O 
X       X 
X D X S X 


Timestep: 61
Joint action taken: ('→', 'interact') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←1O 
X       X 
X D X S X 


Timestep: 62
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s↑1O 
X       X 
X D X S X 


Timestep: 63
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s↑1O 
X       X 
X D X S X 


Timestep: 64
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s↑1O 
X       X 
X D X S X 


Timestep: 65
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O     ↑1O 
X   ↓s  X 
X D X S X 


Timestep: 66
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s→1O 
X       X 
X D X S X 


Timestep: 67
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O     →1O 
X   ↓s  X 
X D X S X 


Timestep: 68
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s→1O 
X       X 
X D X S X 


Timestep: 69
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s→1O 
X       X 
X D X S X 


Timestep: 70
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s→1O 
X       X 
X D X S X 


Timestep: 71
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←1O 
X       X 
X D X S X 


Timestep: 72
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s←1O 
X       X 
X D X S X 


Timestep: 73
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O     ←1O 
X   ↓s  X 
X D X S X 


Timestep: 74
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s←1O 
X       X 
X D X S X 


Timestep: 75
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s←1O 
X       X 
X D X S X 


Timestep: 76
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑1O 
X       X 
X D X S X 


Timestep: 77
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s↑1O 
X       X 
X D X S X 


Timestep: 78
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑1O 
X       X 
X D X S X 


Timestep: 79
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s↑1O 
X       X 
X D X S X 


Timestep: 80
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s←1O 
X       X 
X D X S X 


Timestep: 81
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s←1O 
X       X 
X D X S X 


Timestep: 82
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s←1O 
X       X 
X D X S X 


Timestep: 83
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O     ←1O 
X   ↓s  X 
X D X S X 


Timestep: 84
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X P XoX 
O   ↑s↑1O 
X       X 
X D X S X 


Timestep: 85
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑1O 
X       X 
X D X S X 


Timestep: 86
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑1O 
X       X 
X D X S X 


Timestep: 87
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P XoX 
O   →s↑1O 
X       X 
X D X S X 


Timestep: 88
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s↑oO 
X       X 
X D X S X 


Timestep: 89
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s←oO 
X       X 
X D X S X 


Timestep: 90
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s←oO 
X       X 
X D X S X 


Timestep: 91
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     ←oO 
X   ↓s  X 
X D X S X 


Timestep: 92
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s←oO 
X       X 
X D X S X 


Timestep: 93
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O     ↑oO 
X   ↓s  X 
X D X S X 


Timestep: 94
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s↑oO 
X       X 
X D X S X 


Timestep: 95
Joint action taken: ('→', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s↑oO 
X       X 
X D X S X 


Timestep: 96
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s↑oO 
X       X 
X D X S X 


Timestep: 97
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O   →s←oO 
X       X 
X D X S X 


Timestep: 98
Joint action taken: ('↓', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O     ←oO 
X   ↓s  X 
X D X S X 


Timestep: 99
Joint action taken: ('↑', 'stay') 	 Reward: 0 + shape * 0 
X X P X X 
O   ↑s←oO 
X       X 
X D X S X 


tot rew 20 tot rew shaped 34
PPO agent on index 1:
X X P X X 
O     ↑1O 
X ↑0    X 
X D X S X 

Timestep: 1
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →1O 
X ↑0    X 
X D X S X 


Timestep: 2
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X ↑0    X 
X D X S X 


Timestep: 3
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X ↑0    X 
X D X S X 


Timestep: 4
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P X X 
O     →oO 
X ↑0    X 
X D X S X 


Timestep: 5
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0←o  O 
X       X 
X D X S X 


Timestep: 6
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P X X 
O ↑0↑o  O 
X       X 
X D X S X 


Timestep: 7
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø-X X 
O ↑0↑1  O 
X       X 
X D X S X 


Timestep: 8
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ↑0  →1O 
X       X 
X D X S X 


Timestep: 9
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ↑0  →oO 
X       X 
X D X S X 


Timestep: 10
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø-X X 
O ←0←o  O 
X       X 
X D X S X 


Timestep: 11
Joint action taken: ('↓', '↑') 	 Reward: 0 + shape * 0 
X X ø-X X 
O   ↑o  O 
X ↓0    X 
X D X S X 


Timestep: 12
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ↑1  O 
X ↓0    X 
X D X S X 


Timestep: 13
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →1O 
X ↓0    X 
X D X S X 


Timestep: 14
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O     →oO 
X ↓0    X 
X D X S X 


Timestep: 15
Joint action taken: ('interact', '←') 	 Reward: 0 + shape * 3 
X X ø=X X 
O   ←o  O 
X ↓d    X 
X D X S X 


Timestep: 16
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←o    O 
X   →d  X 
X D X S X 


Timestep: 17
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑o    O 
X   →d  X 
X D X S X 


Timestep: 18
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑o    O 
X   →d  X 
X D X S X 


Timestep: 19
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑o    O 
X   →d  X 
X D X S X 


Timestep: 20
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑o↑d  O 
X       X 
X D X S X 


Timestep: 21
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X Xoø=X X 
O ↑1↑d  O 
X       X 
X D X S X 


Timestep: 22
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑o↑d  O 
X       X 
X D X S X 


Timestep: 23
Joint action taken: ('→', '↑') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ↑o  →dO 
X       X 
X D X S X 


Timestep: 24
Joint action taken: ('↑', '←') 	 Reward: 0 + shape * 0 
X X ø=X X 
O ←o  ↑dO 
X       X 
X D X S X 


Timestep: 25
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   →o↑dO 
X       X 
X D X S X 


Timestep: 26
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   →o↑dO 
X       X 
X D X S X 


Timestep: 27
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø=X X 
O   →o↑dO 
X       X 
X D X S X 


Timestep: 28
Joint action taken: ('interact', '→') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   →o↑0O 
X       X 
X D X S X 


Timestep: 29
Joint action taken: ('→', '→') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   →o→0O 
X       X 
X D X S X 


Timestep: 30
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   →o→oO 
X       X 
X D X S X 


Timestep: 31
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ←o  →oO 
X       X 
X D X S X 


Timestep: 32
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   →o→oO 
X       X 
X D X S X 


Timestep: 33
Joint action taken: ('←', 'stay') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   →o←oO 
X       X 
X D X S X 


Timestep: 34
Joint action taken: ('←', '←') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O ←o←o  O 
X       X 
X D X S X 


Timestep: 35
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   ←o  O 
X ↓o    X 
X D X S X 


Timestep: 36
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   ↑o  O 
X ↓o    X 
X D X S X 


Timestep: 37
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø=XdX 
O   ↑o  O 
X   →o  X 
X D X S X 


Timestep: 38
Joint action taken: ('interact', '↑') 	 Reward: 0 + shape * 3 
X X ø1XdX 
O   ↑0  O 
X   ↑o  X 
X D X S X 


Timestep: 39
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø2XdX 
O   ↑0  O 
X   ↓o  X 
X D X S X 


Timestep: 40
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X ø3XdX 
O     →0O 
X   ↓o  X 
X D X S X 


Timestep: 41
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø4XdX 
O     →0O 
X   ↓o  X 
X D X S X 


Timestep: 42
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø5XdX 
O     →0O 
X   ↓1  X 
X D XoS X 


Timestep: 43
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø6XdX 
O     →0O 
X   ↓1  X 
X D XoS X 


Timestep: 44
Joint action taken: ('←', 'interact') 	 Reward: 0 + shape * 0 
X X ø7XdX 
O   ←0  O 
X   ↓o  X 
X D X S X 


Timestep: 45
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø8XdX 
O   ←0  O 
X     →oX 
X D X S X 


Timestep: 46
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø9XdX 
O   ←0  O 
X     ↓oX 
X D X S X 


Timestep: 47
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø10XdX 
O   ←0  O 
X   ←o  X 
X D X S X 


Timestep: 48
Joint action taken: ('→', '↓') 	 Reward: 0 + shape * 0 
X X ø11XdX 
O     →0O 
X   ↓o  X 
X D X S X 


Timestep: 49
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø12XdX 
O     →0O 
X   ↓o  X 
X D X S X 


Timestep: 50
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø13XdX 
O     →0O 
X   ↓o  X 
X D X S X 


Timestep: 51
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø14XdX 
O     →0O 
X   ↓o  X 
X D X S X 


Timestep: 52
Joint action taken: ('interact', '↓') 	 Reward: 0 + shape * 0 
X X ø15XdX 
O     →oO 
X   ↓o  X 
X D X S X 


Timestep: 53
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø16XdX 
O   ↑o↑oO 
X       X 
X D X S X 


Timestep: 54
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø17XdX 
O     ↑oO 
X   ↓o  X 
X D X S X 


Timestep: 55
Joint action taken: ('←', '↓') 	 Reward: 0 + shape * 0 
X X ø18XdX 
O   ←o  O 
X   ↓o  X 
X D X S X 


Timestep: 56
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø19XdX 
O   ←o  O 
X   ↓o  X 
X D X S X 


Timestep: 57
Joint action taken: ('stay', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ←o  O 
X   ↓1  X 
X D XoS X 


Timestep: 58
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ←o  O 
X ←1    X 
X D XoS X 


Timestep: 59
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O ↑1↑o  O 
X       X 
X D XoS X 


Timestep: 60
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O ↑1↑o  O 
X       X 
X D XoS X 


Timestep: 61
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O ↑1↑o  O 
X       X 
X D XoS X 


Timestep: 62
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑o  O 
X ↓1    X 
X D XoS X 


Timestep: 63
Joint action taken: ('interact', 'interact') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑o  O 
X ↓d    X 
X D XoS X 


Timestep: 64
Joint action taken: ('→', '←') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →oO 
X ←d    X 
X D XoS X 


Timestep: 65
Joint action taken: ('stay', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →oO 
X ←d    X 
X D XoS X 


Timestep: 66
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O     →oO 
X   →d  X 
X D XoS X 


Timestep: 67
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑d→oO 
X       X 
X D XoS X 


Timestep: 68
Joint action taken: ('interact', 'stay') 	 Reward: 0 + shape * 0 
X X ø20XdX 
O   ↑d→oO 
X       X 
X D XoS X 


Timestep: 69
Joint action taken: ('↑', 'interact') 	 Reward: 0 + shape * 5 
X X P XdX 
O   ↑s↑oO 
X       X 
X D XoS X 


Timestep: 70
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑s↑oO 
X       X 
X D XoS X 


Timestep: 71
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X P XdX 
O     ↑oO 
X   ↓s  X 
X D XoS X 


Timestep: 72
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑s↑oO 
X       X 
X D XoS X 


Timestep: 73
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑s←oO 
X       X 
X D XoS X 


Timestep: 74
Joint action taken: ('stay', '←') 	 Reward: 0 + shape * 0 
X X P XdX 
O ←s  ←oO 
X       X 
X D XoS X 


Timestep: 75
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s←oO 
X       X 
X D XoS X 


Timestep: 76
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑s←oO 
X       X 
X D XoS X 


Timestep: 77
Joint action taken: ('↑', '↓') 	 Reward: 0 + shape * 0 
X X P XdX 
O     ↑oO 
X   ↓s  X 
X D XoS X 


Timestep: 78
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑s↑oO 
X       X 
X D XoS X 


Timestep: 79
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s←oO 
X       X 
X D XoS X 


Timestep: 80
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X P XdX 
O     ←oO 
X   ↓s  X 
X D XoS X 


Timestep: 81
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑s←oO 
X       X 
X D XoS X 


Timestep: 82
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X P XdX 
O     ←oO 
X   ↓s  X 
X D XoS X 


Timestep: 83
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O     ←oO 
X   ↑s  X 
X D XoS X 


Timestep: 84
Joint action taken: ('↑', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑s↑oO 
X       X 
X D XoS X 


Timestep: 85
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s↑oO 
X       X 
X D XoS X 


Timestep: 86
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑s↑oO 
X       X 
X D XoS X 


Timestep: 87
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑s↑oO 
X       X 
X D XoS X 


Timestep: 88
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s←oO 
X       X 
X D XoS X 


Timestep: 89
Joint action taken: ('←', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑s←oO 
X       X 
X D XoS X 


Timestep: 90
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X P XdX 
O     ←oO 
X   ↓s  X 
X D XoS X 


Timestep: 91
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑s←oO 
X       X 
X D XoS X 


Timestep: 92
Joint action taken: ('↑', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s↑oO 
X       X 
X D XoS X 


Timestep: 93
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s↑oO 
X       X 
X D XoS X 


Timestep: 94
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s↑oO 
X       X 
X D XoS X 


Timestep: 95
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑s↑oO 
X       X 
X D XoS X 


Timestep: 96
Joint action taken: ('←', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s←oO 
X       X 
X D XoS X 


Timestep: 97
Joint action taken: ('stay', '↓') 	 Reward: 0 + shape * 0 
X X P XdX 
O     ←oO 
X   ↓s  X 
X D XoS X 


Timestep: 98
Joint action taken: ('stay', '↑') 	 Reward: 0 + shape * 0 
X X P XdX 
O   ↑s←oO 
X       X 
X D XoS X 


Timestep: 99
Joint action taken: ('stay', '→') 	 Reward: 0 + shape * 0 
X X P XdX 
O   →s←oO 
X       X 
X D XoS X 


tot rew 20 tot rew shaped 34
../../thesis_data/dr_ppo/ppo_bc_train_simple/
SP envs: 29/30
Other agent actions took 5.9514734745025635 seconds
Total simulation time for 400 steps: 9.694642066955566 	 Other agent action time: 0 	 41.25990389716503 steps/s
Curr learning rate 0.000494949494949495 	 Curr reward per step 0.6609

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 171.07it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.28it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 163.06it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.41it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 143.08it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 172.53it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 175.46it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.76it/s]
------------------------------------
| approxkl           | 0.004079169 |
| clipfrac           | 0.3129479   |
| eplenmean          | 400         |
| eprewmean          | 260         |
| explained_variance | 0.325       |
| fps                | 1176        |
| nupdates           | 51          |
| policy_entropy     | 0.76286113  |
| policy_loss        | 0.006397143 |
| serial_timesteps   | 20400       |
| time_elapsed       | 292         |
| time_remaining     | 1.43        |
| total_timesteps    | 612000      |
| true_eprew         | 190         |
| value_loss         | 83.16289    |
------------------------------------
Current reward shaping 0.388
Current self-play randomization 0.9552
SP envs: 29/30
Other agent actions took 5.957412481307983 seconds
Total simulation time for 400 steps: 9.692139387130737 	 Other agent action time: 0 	 41.27055792564453 steps/s
Curr learning rate 0.0004848484848484849 	 Curr reward per step 0.6715283333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 140.14it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 157.22it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 160.00it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 147.79it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 158.29it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 165.52it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 161.26it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 146.31it/s]
-------------------------------------
| approxkl           | 0.0033173873 |
| clipfrac           | 0.27279168   |
| eplenmean          | 400          |
| eprewmean          | 261          |
| explained_variance | 0.164        |
| fps                | 1171         |
| nupdates           | 52           |
| policy_entropy     | 0.7726754    |
| policy_loss        | 0.0055202646 |
| serial_timesteps   | 20800        |
| time_elapsed       | 302          |
| time_remaining     | 1.36         |
| total_timesteps    | 624000       |
| true_eprew         | 193          |
| value_loss         | 88.61989     |
-------------------------------------
Current reward shaping 0.376
Current self-play randomization 0.9504
SP envs: 30/30
Other agent actions took 0.8154079914093018 seconds
Total simulation time for 400 steps: 4.627737522125244 	 Other agent action time: 0 	 86.43532570453647 steps/s
Curr learning rate 0.0004747474747474748 	 Curr reward per step 0.6849206666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.79it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 155.74it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 134.52it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.71it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 129.02it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 157.55it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 151.30it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 174.72it/s]
-------------------------------------
| approxkl           | 0.0045046564 |
| clipfrac           | 0.3129687    |
| eplenmean          | 400          |
| eprewmean          | 267          |
| explained_variance | 0.345        |
| fps                | 2315         |
| nupdates           | 53           |
| policy_entropy     | 0.7662403    |
| policy_loss        | 0.006597514  |
| serial_timesteps   | 21200        |
| time_elapsed       | 307          |
| time_remaining     | 1.26         |
| total_timesteps    | 636000       |
| true_eprew         | 199          |
| value_loss         | 71.56679     |
-------------------------------------
Current reward shaping 0.364
Current self-play randomization 0.9456
SP envs: 26/30
Other agent actions took 6.057542085647583 seconds
Total simulation time for 400 steps: 9.818243265151978 	 Other agent action time: 0 	 40.74048576691162 steps/s
Curr learning rate 0.0004646464646464647 	 Curr reward per step 0.563015

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 155.60it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 162.84it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 157.55it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 149.87it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 164.97it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 161.52it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 168.38it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 160.10it/s]
-------------------------------------
| approxkl           | 0.0065353615 |
| clipfrac           | 0.32671872   |
| eplenmean          | 400          |
| eprewmean          | 256          |
| explained_variance | 0.402        |
| fps                | 1159         |
| nupdates           | 54           |
| policy_entropy     | 0.7217625    |
| policy_loss        | 0.010769559  |
| serial_timesteps   | 21600        |
| time_elapsed       | 318          |
| time_remaining     | 1.18         |
| total_timesteps    | 648000       |
| true_eprew         | 192          |
| value_loss         | 93.653496    |
-------------------------------------
Current reward shaping 0.352
Current self-play randomization 0.9408
SP envs: 27/30
Other agent actions took 5.990585565567017 seconds
Total simulation time for 400 steps: 9.751641035079956 	 Other agent action time: 0 	 41.01873710907369 steps/s
Curr learning rate 0.00045454545454545455 	 Curr reward per step 0.6145013333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.40it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 163.01it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.20it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 148.99it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 169.34it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 165.23it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 171.70it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 175.07it/s]
-------------------------------------
| approxkl           | 0.0018850649 |
| clipfrac           | 0.20279166   |
| eplenmean          | 400          |
| eprewmean          | 249          |
| explained_variance | 0.159        |
| fps                | 1168         |
| nupdates           | 55           |
| policy_entropy     | 0.7043456    |
| policy_loss        | 0.0030345293 |
| serial_timesteps   | 22000        |
| time_elapsed       | 328          |
| time_remaining     | 1.09         |
| total_timesteps    | 660000       |
| true_eprew         | 188          |
| value_loss         | 89.13134     |
-------------------------------------
Current reward shaping 0.33999999999999997
Current self-play randomization 0.9359999999999999
SP envs: 27/30
Other agent actions took 6.042367458343506 seconds
Total simulation time for 400 steps: 9.794583559036255 	 Other agent action time: 0 	 40.838898110269255 steps/s
Curr learning rate 0.0004444444444444444 	 Curr reward per step 0.6056183333333334

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 118.72it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 114.27it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 117.64it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 115.68it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 115.10it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 117.55it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.47it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 165.60it/s]
--------------------------------------
| approxkl           | 0.00070189696 |
| clipfrac           | 0.11651041    |
| eplenmean          | 400           |
| eprewmean          | 240           |
| explained_variance | 0.293         |
| fps                | 1147          |
| nupdates           | 56            |
| policy_entropy     | 0.7138319     |
| policy_loss        | 2.9542574e-05 |
| serial_timesteps   | 22400         |
| time_elapsed       | 338           |
| time_remaining     | 1.01          |
| total_timesteps    | 672000        |
| true_eprew         | 183           |
| value_loss         | 87.89509      |
--------------------------------------
Current reward shaping 0.32799999999999996
Current self-play randomization 0.9312
SP envs: 29/30
Other agent actions took 5.963756084442139 seconds
Total simulation time for 400 steps: 9.810346364974976 	 Other agent action time: 0 	 40.77328007786607 steps/s
Curr learning rate 0.00043434343434343433 	 Curr reward per step 0.6394293333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.91it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 161.38it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 156.32it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 147.97it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 129.10it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 159.43it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 163.18it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.82it/s]
-------------------------------------
| approxkl           | 0.0014956695 |
| clipfrac           | 0.17104158   |
| eplenmean          | 400          |
| eprewmean          | 242          |
| explained_variance | 0.292        |
| fps                | 1158         |
| nupdates           | 57           |
| policy_entropy     | 0.6879208    |
| policy_loss        | 0.0013967733 |
| serial_timesteps   | 22800        |
| time_elapsed       | 349          |
| time_remaining     | 0.917        |
| total_timesteps    | 684000       |
| true_eprew         | 185          |
| value_loss         | 74.305374    |
-------------------------------------
Current reward shaping 0.31599999999999995
Current self-play randomization 0.9264
SP envs: 30/30
Other agent actions took 0.8015866279602051 seconds
Total simulation time for 400 steps: 4.5398125648498535 	 Other agent action time: 0 	 88.10936449162176 steps/s
Curr learning rate 0.00042424242424242425 	 Curr reward per step 0.6645173333333331

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 156.69it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 151.69it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 162.67it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 143.77it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 159.53it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 160.00it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 172.73it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 180.07it/s]
-------------------------------------
| approxkl           | 0.0009075332 |
| clipfrac           | 0.14313541   |
| eplenmean          | 400          |
| eprewmean          | 254          |
| explained_variance | 0.283        |
| fps                | 2367         |
| nupdates           | 58           |
| policy_entropy     | 0.77284575   |
| policy_loss        | 0.0003600468 |
| serial_timesteps   | 23200        |
| time_elapsed       | 354          |
| time_remaining     | 0.813        |
| total_timesteps    | 696000       |
| true_eprew         | 196          |
| value_loss         | 72.64314     |
-------------------------------------
Current reward shaping 0.30400000000000005
Current self-play randomization 0.9216
SP envs: 27/30
Other agent actions took 6.015262842178345 seconds
Total simulation time for 400 steps: 9.860990285873413 	 Other agent action time: 0 	 40.563877298716044 steps/s
Curr learning rate 0.0004141414141414141 	 Curr reward per step 0.6116013333333332

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 160.59it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.35it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.91it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 173.90it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 148.83it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 155.12it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 155.18it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 156.90it/s]
-------------------------------------
| approxkl           | 0.0026570437 |
| clipfrac           | 0.2038646    |
| eplenmean          | 400          |
| eprewmean          | 254          |
| explained_variance | 0.181        |
| fps                | 1154         |
| nupdates           | 59           |
| policy_entropy     | 0.7077988    |
| policy_loss        | 0.0039084335 |
| serial_timesteps   | 23600        |
| time_elapsed       | 364          |
| time_remaining     | 0.72         |
| total_timesteps    | 708000       |
| true_eprew         | 198          |
| value_loss         | 88.36147     |
-------------------------------------
Current reward shaping 0.29200000000000004
Current self-play randomization 0.9168000000000001
SP envs: 29/30
Other agent actions took 5.96161961555481 seconds
Total simulation time for 400 steps: 9.741913080215454 	 Other agent action time: 0 	 41.05969707452507 steps/s
Curr learning rate 0.00040404040404040404 	 Curr reward per step 0.633322

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 156.84it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 152.59it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 161.76it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 146.96it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 160.89it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 167.58it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 167.02it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 150.29it/s]
---------------------------------------
| approxkl           | 0.00080658414  |
| clipfrac           | 0.1298229      |
| eplenmean          | 400            |
| eprewmean          | 256            |
| explained_variance | 0.207          |
| fps                | 1167           |
| nupdates           | 60             |
| policy_entropy     | 0.76993567     |
| policy_loss        | -4.9293052e-05 |
| serial_timesteps   | 24000          |
| time_elapsed       | 374            |
| time_remaining     | 0.624          |
| total_timesteps    | 720000         |
| true_eprew         | 202            |
| value_loss         | 80.123825      |
---------------------------------------
Current reward shaping 0.28
Current self-play randomization 0.912
SP envs: 24/30
Other agent actions took 6.098912954330444 seconds
Total simulation time for 400 steps: 9.84485387802124 	 Other agent action time: 0 	 40.63036434629112 steps/s
Curr learning rate 0.00039393939393939396 	 Curr reward per step 0.5084566666666668

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 166.55it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 168.10it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 152.15it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 166.34it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.84it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 151.94it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 165.27it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 159.58it/s]
-------------------------------------
| approxkl           | 0.0016876673 |
| clipfrac           | 0.19171874   |
| eplenmean          | 400          |
| eprewmean          | 237          |
| explained_variance | 0.539        |
| fps                | 1157         |
| nupdates           | 61           |
| policy_entropy     | 0.76652545   |
| policy_loss        | 0.0021618234 |
| serial_timesteps   | 24400        |
| time_elapsed       | 385          |
| time_remaining     | 0.526        |
| total_timesteps    | 732000       |
| true_eprew         | 188          |
| value_loss         | 89.468185    |
-------------------------------------
Current reward shaping 0.268
Current self-play randomization 0.9072
SP envs: 28/30
Other agent actions took 6.066741943359375 seconds
Total simulation time for 400 steps: 9.899809122085571 	 Other agent action time: 0 	 40.4048194330976 steps/s
Curr learning rate 0.0003838383838383839 	 Curr reward per step 0.5923813333333333

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 133.73it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 111.66it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 134.58it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 172.40it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 169.83it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 167.62it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 165.31it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.55it/s]
--------------------------------------
| approxkl           | 0.0010406303  |
| clipfrac           | 0.14719789    |
| eplenmean          | 400           |
| eprewmean          | 231           |
| explained_variance | 0.387         |
| fps                | 1146          |
| nupdates           | 62            |
| policy_entropy     | 0.719997      |
| policy_loss        | 0.00072055205 |
| serial_timesteps   | 24800         |
| time_elapsed       | 395           |
| time_remaining     | 0.425         |
| total_timesteps    | 744000        |
| true_eprew         | 185           |
| value_loss         | 75.58904      |
--------------------------------------
Current reward shaping 0.256
Current self-play randomization 0.9024
SP envs: 28/30
Other agent actions took 5.962073564529419 seconds
Total simulation time for 400 steps: 9.690813541412354 	 Other agent action time: 0 	 41.276204344522284 steps/s
Curr learning rate 0.0003737373737373737 	 Curr reward per step 0.60388

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 169.46it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 172.15it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 168.21it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 160.44it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 146.25it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 156.89it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 156.78it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 115.52it/s]
-------------------------------------
| approxkl           | 0.0016902083 |
| clipfrac           | 0.17128126   |
| eplenmean          | 400          |
| eprewmean          | 231          |
| explained_variance | 0.205        |
| fps                | 1171         |
| nupdates           | 63           |
| policy_entropy     | 0.7027523    |
| policy_loss        | 0.002152961  |
| serial_timesteps   | 25200        |
| time_elapsed       | 405          |
| time_remaining     | 0.322        |
| total_timesteps    | 756000       |
| true_eprew         | 187          |
| value_loss         | 82.93412     |
-------------------------------------
Current reward shaping 0.244
Current self-play randomization 0.8976
SP envs: 29/30
Other agent actions took 6.081628084182739 seconds
Total simulation time for 400 steps: 9.844072103500366 	 Other agent action time: 0 	 40.63359103777466 steps/s
Curr learning rate 0.0003636363636363636 	 Curr reward per step 0.6257556666666666

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 152.85it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 164.04it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 144.00it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 151.89it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.16it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 162.21it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 166.21it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 164.27it/s]
-------------------------------------
| approxkl           | 0.0035007067 |
| clipfrac           | 0.2586979    |
| eplenmean          | 400          |
| eprewmean          | 237          |
| explained_variance | 0.215        |
| fps                | 1156         |
| nupdates           | 64           |
| policy_entropy     | 0.70242894   |
| policy_loss        | 0.005845526  |
| serial_timesteps   | 25600        |
| time_elapsed       | 416          |
| time_remaining     | 0.217        |
| total_timesteps    | 768000       |
| true_eprew         | 193          |
| value_loss         | 74.587234    |
-------------------------------------
Current reward shaping 0.23199999999999998
Current self-play randomization 0.8928
SP envs: 30/30
Other agent actions took 0.7823152542114258 seconds
Total simulation time for 400 steps: 4.57372260093689 	 Other agent action time: 0 	 87.45611286483864 steps/s
Curr learning rate 0.00035353535353535354 	 Curr reward per step 0.6415186666666667

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 159.30it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 155.43it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 171.68it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 171.30it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 167.39it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 163.94it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 157.78it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 148.66it/s]
-------------------------------------
| approxkl           | 0.0010135641 |
| clipfrac           | 0.16557293   |
| eplenmean          | 400          |
| eprewmean          | 250          |
| explained_variance | 0.226        |
| fps                | 2354         |
| nupdates           | 65           |
| policy_entropy     | 0.77767104   |
| policy_loss        | 0.0011278427 |
| serial_timesteps   | 26000        |
| time_elapsed       | 421          |
| time_remaining     | 0.108        |
| total_timesteps    | 780000       |
| true_eprew         | 205          |
| value_loss         | 70.639694    |
-------------------------------------
Current reward shaping 0.21999999999999997
Current self-play randomization 0.888
SP envs: 25/30
Other agent actions took 5.959004640579224 seconds
Total simulation time for 400 steps: 9.752063035964966 	 Other agent action time: 0 	 41.01696210584636 steps/s
Curr learning rate 0.0003434343434343434 	 Curr reward per step 0.52221

0/8:   0%|          | 0/10 [00:00<?, ?it/s]
0/8: 100%|██████████| 10/10 [00:00<00:00, 164.02it/s]

1/8:   0%|          | 0/10 [00:00<?, ?it/s]
1/8: 100%|██████████| 10/10 [00:00<00:00, 171.00it/s]

2/8:   0%|          | 0/10 [00:00<?, ?it/s]
2/8: 100%|██████████| 10/10 [00:00<00:00, 140.03it/s]

3/8:   0%|          | 0/10 [00:00<?, ?it/s]
3/8: 100%|██████████| 10/10 [00:00<00:00, 168.59it/s]

4/8:   0%|          | 0/10 [00:00<?, ?it/s]
4/8: 100%|██████████| 10/10 [00:00<00:00, 152.95it/s]

5/8:   0%|          | 0/10 [00:00<?, ?it/s]
5/8: 100%|██████████| 10/10 [00:00<00:00, 169.17it/s]

6/8:   0%|          | 0/10 [00:00<?, ?it/s]
6/8: 100%|██████████| 10/10 [00:00<00:00, 176.61it/s]

7/8:   0%|          | 0/10 [00:00<?, ?it/s]
7/8: 100%|██████████| 10/10 [00:00<00:00, 168.32it/s]
--------------------------------------
| approxkl           | 0.0008984     |
| clipfrac           | 0.15190622    |
| eplenmean          | 400           |
| eprewmean          | 241           |
| explained_variance | 0.423         |
| fps                | 1168          |
| nupdates           | 66            |
| policy_entropy     | 0.7253804     |
| policy_loss        | 0.00033565413 |
| serial_timesteps   | 26400         |
| time_elapsed       | 431           |
| time_remaining     | 0             |
| total_timesteps    | 792000        |
| true_eprew         | 199           |
| value_loss         | 92.6276       |
--------------------------------------
Current reward shaping 0.20799999999999996
Current self-play randomization 0.8832
WARNING:tensorflow:From maml/dr_ppo.py:244: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.
WARNING - tensorflow - From maml/dr_ppo.py:244: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.
WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.
WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.
INFO:tensorflow:Assets added to graph.
INFO - tensorflow - Assets added to graph.
INFO:tensorflow:No assets to write.
INFO - tensorflow - No assets to write.
INFO:tensorflow:SavedModel written to: ../../thesis_data/dr_ppo/ppo_bc_train_simple/seed9456/ppo_agent/saved_model.pb
INFO - tensorflow - SavedModel written to: ../../thesis_data/dr_ppo/ppo_bc_train_simple/seed9456/ppo_agent/saved_model.pb
Saved training info at ../../thesis_data/dr_ppo/ppo_bc_train_simple/seed9456/training_info
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ubuntu/anaconda3/envs/harl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
INFO - PPO - Result: [defaultdict(<class 'list'>, {'eprewmean': [208.79466666666667, 287.20513333333326, 315.7371111111112, 345.30283999999983, 351.46372, 344.23524, 338.40920000000006, 335.3742, 327.4296, 320.50687999999997, 318.2501199999999, 320.49411999999995, 321.99024, 314.76804000000004, 303.42231999999996, 292.34736, 290.34576, 284.26192000000003, 287.20796, 292.9026400000001, 290.75236, 290.14216, 288.10636, 293.50928, 293.40348, 294.83952, 288.59644, 282.05476, 282.00388, 282.3994, 288.95688, 295.3908799999999, 296.55096000000003, 300.04359999999997, 299.37156, 303.98964, 304.45311999999996, 302.72852, 298.50848, 292.79084, 293.60376, 293.57196, 296.14220000000006, 294.2991999999999, 290.43443999999994, 285.16552, 279.20655999999997, 276.62232000000006, 273.62440000000004, 264.61092, 259.88667999999996, 260.86447999999996, 267.49224000000004, 255.53567999999996, 248.78368000000003, 240.44639999999998, 241.78688000000002, 253.52412000000004, 254.08256, 256.18447999999995, 237.16312, 231.12671999999998, 231.2032, 237.38284000000004, 249.96252, 240.91764], 'ep_dense_rew_mean': [157.66666666666666, 165.68333333333334, 169.7, 170.48, 168.5, 165.78, 164.04, 163.31, 160.27, 157.64, 157.93, 159.51, 161.79, 159.26, 154.7, 150.19, 149.98, 148.17, 150.34, 154.3, 153.59, 154.17, 154.22, 157.82, 158.93, 160.74, 157.96, 154.72, 155.68, 157.04, 162.29, 167.25, 169.07, 171.82, 172.49, 176.34, 178.34, 178.34, 177.1, 174.56, 176.29, 178.03, 180.94, 181.18, 179.71, 177.31, 175.13, 174.57, 174.77, 170.57, 168.13, 169.76, 175.08, 168.23, 165.74, 162.15, 165.05, 173.76, 175.53, 178.04, 165.84, 162.53, 164.45, 170.75, 181.89, 176.62], 'ep_sparse_rew_mean': [176.0, 185.0, 189.77777777777777, 191.4, 188.6, 186.0, 183.8, 183.4, 180.2, 177.6, 177.0, 179.8, 181.2, 178.0, 172.4, 167.0, 167.0, 164.2, 167.2, 171.6, 171.8, 172.6, 172.4, 177.0, 178.0, 180.0, 177.6, 175.2, 176.4, 177.8, 182.8, 188.0, 190.0, 193.8, 194.8, 199.2, 200.6, 201.0, 199.6, 197.4, 199.4, 200.6, 203.8, 204.0, 203.0, 201.0, 198.2, 198.0, 197.0, 191.8, 190.2, 192.6, 199.2, 191.8, 188.0, 183.0, 185.4, 196.2, 198.2, 201.6, 188.2, 185.2, 186.8, 193.4, 205.2, 199.4], 'eplenmean': [400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0, 400.0], 'explained_variance': [0.21047061681747437, 0.33306562900543213, 0.22520583868026733, 0.3480910062789917, 0.2590806484222412, 0.2294040322303772, 0.2405967116355896, 0.25769317150115967, 0.2469351887702942, 0.2387404441833496, 0.24334651231765747, 0.21360433101654053, 0.2203984260559082, 0.25817906856536865, 0.20693188905715942, 0.2129475474357605, 0.22076767683029175, 0.16686326265335083, 0.2728796601295471, 0.20589929819107056, 0.3553743362426758, 0.26640456914901733, 0.2257136106491089, 0.2670021653175354, 0.2285255789756775, 0.30414533615112305, 0.22844535112380981, 0.16767370700836182, 0.3383524417877197, 0.31936246156692505, 0.2594151496887207, 0.21889269351959229, 0.2394932508468628, 0.21831738948822021, 0.2181665301322937, 0.1790769100189209, 0.2326907515525818, 0.2446644902229309, 0.27734196186065674, 0.3002737760543823, 0.21283483505249023, 0.21013176441192627, 0.24854904413223267, 0.21420729160308838, 0.22528040409088135, 0.1679544448852539, 0.34150147438049316, 0.22922354936599731, 0.20229625701904297, 0.1564730405807495, 0.3250693082809448, 0.16428852081298828, 0.3445426821708679, 0.40233784914016724, 0.1594003438949585, 0.29268062114715576, 0.29187995195388794, 0.28347522020339966, 0.18084508180618286, 0.20680582523345947, 0.5385421216487885, 0.3865118622779846, 0.20468837022781372, 0.21501696109771729, 0.22576534748077393, 0.423093318939209], 'policy_loss': [0.013617006, 0.017491182, 0.030602127, 0.0061104866, 0.012456214, 0.0048631113, 0.008484247, 0.0053887265, 0.006469817, 0.003260817, 0.009804355, 0.004768284, 0.023708735, 0.019201305, 0.0066653527, 0.0077743074, 0.014083875, 0.00766929, 0.011396538, 0.004755143, 0.0066598877, 0.009418501, 0.006787396, 0.023023974, 0.023398226, 0.0086896, 0.0042805998, 0.011395922, 0.007312628, 0.029784914, 0.0046591754, 0.010057236, 0.004629413, 0.0061904443, 0.0014301314, 0.0012003038, 0.00854318, 0.0027166442, 0.006839449, 0.003354403, 0.00990185, 0.0037889252, 0.0013088302, 0.010327072, 0.0034123461, 0.004152142, 0.006193289, 0.008372992, 0.005627412, 0.010432466, 0.006397143, 0.0055202646, 0.006597514, 0.010769559, 0.0030345293, 2.9542574e-05, 0.0013967733, 0.0003600468, 0.0039084335, -4.9293052e-05, 0.0021618234, 0.00072055205, 0.002152961, 0.005845526, 0.0011278427, 0.00033565413], 'value_loss': [82.68504, 132.95467, 121.31566, 106.284645, 111.866196, 104.90944, 102.88725, 102.2025, 94.634705, 93.31326, 91.538, 94.54145, 91.37981, 77.38019, 91.85434, 79.95091, 85.13229, 89.4739, 84.2302, 88.88415, 81.03691, 87.14934, 86.99717, 82.14703, 84.54899, 84.53734, 88.1446, 81.76506, 81.51085, 76.03853, 83.11183, 83.13388, 81.83114, 83.44411, 83.47106, 85.709526, 85.59287, 82.57384, 84.29213, 84.77514, 86.161575, 80.61345, 76.5963, 80.106125, 82.50952, 87.09206, 85.515305, 77.62928, 85.377174, 103.90239, 83.16289, 88.61989, 71.56679, 93.653496, 89.13134, 87.89509, 74.305374, 72.64314, 88.36147, 80.123825, 89.468185, 75.58904, 82.93412, 74.587234, 70.639694, 92.6276], 'policy_entropy': [0.6011686, 0.7114251, 0.80380905, 0.6589358, 0.78820384, 0.70001805, 0.704795, 0.66783226, 0.66717654, 0.81001645, 0.79945457, 0.6588692, 0.7974765, 0.7868101, 0.6136521, 0.6404211, 0.6281291, 0.76723677, 0.808041, 0.8222334, 0.71159226, 0.6881497, 0.81940395, 0.7075192, 0.8105896, 0.808997, 0.8398137, 0.77161515, 0.7804891, 0.8623131, 0.7378431, 0.82412016, 0.7090134, 0.798171, 0.79819596, 0.6720184, 0.793076, 0.79485196, 0.7656599, 0.7724229, 0.7880079, 0.7299045, 0.7055891, 0.7171969, 0.7410179, 0.7053757, 0.713437, 0.71688664, 0.717499, 0.78490114, 0.76286113, 0.7726754, 0.7662403, 0.7217625, 0.7043456, 0.7138319, 0.6879208, 0.77284575, 0.7077988, 0.76993567, 0.76652545, 0.719997, 0.7027523, 0.70242894, 0.77767104, 0.7253804], 'approxkl': [0.008671272, 0.013627358, 0.031871296, 0.0043892553, 0.009328742, 0.0031163893, 0.005492592, 0.0036505926, 0.004246938, 0.002278752, 0.006770788, 0.0028625024, 0.028593536, 0.015084636, 0.0053840233, 0.0058594523, 0.011980811, 0.0051956484, 0.009330042, 0.0033446706, 0.005540095, 0.006290564, 0.0043432256, 0.022389952, 0.020344233, 0.005763388, 0.002693689, 0.0075947493, 0.004979643, 0.028979395, 0.0031584005, 0.0074320645, 0.0034086225, 0.004397369, 0.0012934158, 0.0010457712, 0.006234723, 0.0021488331, 0.004857211, 0.0025829517, 0.0062191887, 0.0028806208, 0.0013247409, 0.009096434, 0.0027628033, 0.0027047782, 0.003917913, 0.0050675184, 0.0035226725, 0.005039141, 0.004079169, 0.0033173873, 0.0045046564, 0.0065353615, 0.0018850649, 0.00070189696, 0.0014956695, 0.0009075332, 0.0026570437, 0.00080658414, 0.0016876673, 0.0010406303, 0.0016902083, 0.0035007067, 0.0010135641, 0.0008984], 'clipfrac': [0.3061146, 0.44196877, 0.58036476, 0.25288543, 0.44156247, 0.24758334, 0.30756253, 0.23951039, 0.29587495, 0.253302, 0.40045834, 0.24072924, 0.4863751, 0.49795827, 0.2599062, 0.29693747, 0.3492292, 0.34054166, 0.4106041, 0.323, 0.30320832, 0.2968854, 0.33967704, 0.4751563, 0.5255418, 0.36383334, 0.2805416, 0.39934367, 0.3328854, 0.52254164, 0.2565312, 0.3443021, 0.24883333, 0.3121875, 0.20618749, 0.15406248, 0.3231979, 0.24437504, 0.34177077, 0.2455312, 0.35903132, 0.22566669, 0.17846876, 0.3487708, 0.22299996, 0.22943747, 0.28633338, 0.31896865, 0.24923953, 0.35398954, 0.3129479, 0.27279168, 0.3129687, 0.32671872, 0.20279166, 0.11651041, 0.17104158, 0.14313541, 0.2038646, 0.1298229, 0.19171874, 0.14719789, 0.17128126, 0.2586979, 0.16557293, 0.15190622]})]
INFO - PPO - Completed after 1:08:42
